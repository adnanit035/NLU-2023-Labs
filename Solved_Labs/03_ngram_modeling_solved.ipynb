{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Statistical Language Modeling with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "\n",
    "- Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 3: N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Ngrams and Ngram Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of *n* items from a given sequence of text or speech. An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character n-grams: mice\n",
    "- word n-grams: the answer is 42\n",
    "\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['m', 'i', 'c', 'e']` | `['mi', 'ic', 'ce']` | `['mic', 'ice']` | \n",
    "| *Word N-grams*      | `['the', 'answer', 'is' , '42']` | `['the answer', 'answer is', ...]` | `['the answer is', ...]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Counting Ngrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count. Ngram count only differs in a unit of counting sequence of $n$ of tokens. \n",
    "\n",
    "We can compute the count by taking sequences of 2 items (bigrams), 3 items (trigrams), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.1. Preparing Data\n",
    "The required data format for ngram counting is a _list-of-lists_ (lists of sentences consisting of lists of words): \n",
    "\n",
    "```\n",
    "[\n",
    "     ['the', 'answer', 'is', '42'], \n",
    "     ['the', 'mice', 'said']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.2. Sentence beginning & end tags\n",
    "\n",
    ">Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence (BOS) tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence (EOS) tag, on the other hand, makes the bigram model a true probability distribution (Jurafsky and Martin). \"Without it,  the sentence probabilities for all sentences of *a given length* would sum to one only. This model would define an infinite set of probability distributions, with one distribution per sentence length\"\n",
    "\n",
    ">For larger ngrams, we’ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "`['<s>', 'the', 'answer', 'is', '42', '</s>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.3. NLTK Utility Functions\n",
    "NLTK provides utility functions for padding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent = ['the', 'answer', 'is', '42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['<s>', 'the', 'answer', 'is', '42', '</s>']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "list(pad_sequence(\n",
    "        sent,  # input sequence\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        pad_right=True,\n",
    "        right_pad_symbol=\"</s>\",\n",
    "        n=2  # padding for bigrams\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another NLTK function wraps this utility function with default arguments to provide a more convenient interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['<s>', 'the', 'answer', 'is', '42', '</s>']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "list(pad_both_ends(sent, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.4. Extracting Ngrams\n",
    "NLTK provides a function to extact ngrams from a sequence, which also performs padding, if required arguments are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 'answer'), ('answer', 'is'), ('is', '42')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(sent, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('<s>', 'the'),\n ('the', 'answer'),\n ('answer', 'is'),\n ('is', '42'),\n ('42', '</s>')]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ngrams(\n",
    "    sent,\n",
    "    2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\",\n",
    ")\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Additionally, NLTK provides wrapper functions to extract bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'answer'), ('answer', 'is'), ('is', '42')]\n",
      "[('the', 'answer', 'is'), ('answer', 'is', '42')]\n",
      "[('<s>', 'the'), ('the', 'answer'), ('answer', 'is'), ('is', '42'), ('42', '</s>')]\n",
      "[('<s>', 'the'), ('the', 'answer'), ('answer', 'is'), ('is', '42'), ('42', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "print(list(bigrams(sent)))\n",
    "print(list(trigrams(sent)))\n",
    "\n",
    "print(list(bigrams(pad_both_ends(sent, n=2))))\n",
    "print(list(bigrams(sent, \n",
    "                   pad_left=True, \n",
    "                   left_pad_symbol=\"<s>\", \n",
    "                   pad_right=True, \n",
    "                   right_pad_symbol=\"</s>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.5. Extracting \"Everygrams\"\n",
    "To make an ngram model more robust, it is usually also trained on lower-order ngrams (i.e. unigrams in case of bigrams). \n",
    "NLTK also provides utility function to extract these together.\n",
    "\n",
    "Note the `max_len` argument that defines the maximum length of an ngram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('the', 'answer'), ('the', 'answer', 'is'), ('answer',), ('answer', 'is'), ('answer', 'is', '42'), ('is',), ('is', '42'), ('42',)]\n",
      "[('<s>',), ('<s>', 'the'), ('the',), ('the', 'answer'), ('answer',), ('answer', 'is'), ('is',), ('is', '42'), ('42',), ('42', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "\n",
    "print(list(everygrams(sent, max_len=3)))\n",
    "print(list(everygrams(pad_both_ends(sent, n=2), max_len=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('<s>',),\n ('<s>', 'the'),\n ('the',),\n ('the', 'answer'),\n ('answer',),\n ('answer', 'is'),\n ('is',),\n ('is', '42'),\n ('42',),\n ('42', '</s>'),\n ('</s>',)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ngrams = everygrams(\n",
    "    sent,\n",
    "    min_len=1,\n",
    "    max_len=2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\"\n",
    ")\n",
    "list(all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.6. \"Flattening\" the Data\n",
    "For language model training and evaluation (as well as vocabulary extraction) NLTK expects the data to be a flat list. \n",
    "In python, this is accomplished either by using `itertools.chain.from_iterable` or python list comprehension as\n",
    "\n",
    "`[element for sublist in superlist for element in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "data = [['which', 'is', 'the', 'sense', 'of', 'life', '?'], ['the', 'answer', 'is', '42']]\n",
    "\n",
    "list(chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprehension list\n",
    "[token for sent in data for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.7. Combining the Tasks\n",
    "NLTK wraps both tasks:\n",
    "\n",
    "- padded ngram (everygram) extraction from each sentence\n",
    "- flat list creation\n",
    "\n",
    "into a convenient utility function `padded_everygram_pipeline` that returns lazy iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', 'which'), ('which',), ('which', 'is'), ('is',), ('is', 'the'), ('the',), ('the', 'sense'), ('sense',), ('sense', 'of'), ('of',), ('of', 'life'), ('life',), ('life', '?'), ('?',), ('?', '</s>'), ('</s>',)]\n",
      "[('<s>',), ('<s>', 'the'), ('the',), ('the', 'answer'), ('answer',), ('answer', 'is'), ('is',), ('is', '42'), ('42',), ('42', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "for sent_ngrams in padded_ngrams:\n",
    "    print(list(sent_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['<s>',\n 'which',\n 'is',\n 'the',\n 'sense',\n 'of',\n 'life',\n '?',\n '</s>',\n '<s>',\n 'the',\n 'answer',\n 'is',\n '42',\n '</s>']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** lazy iterators (aka generators) are for one-use only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x000002D9E97CE660>\n",
      "[[('<s>',), ('<s>', 'winter'), ('winter',), ('winter', 'is'), ('is',), ('is', 'coming'), ('coming',), ('coming', '</s>'), ('</s>',)]]\n",
      "[]\n",
      "[[('<s>',), ('<s>', 'winter'), ('winter',), ('winter', 'is'), ('is',), ('is', 'coming'), ('coming',), ('coming', '</s>'), ('</s>',)]]\n"
     ]
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, ['winter is coming'.split()])\n",
    "print(padded_ngrams)\n",
    "print([list(x) for x in padded_ngrams]) # We use the generator\n",
    "print([list(x) for x in padded_ngrams]) # We lost it \n",
    "# If you want to reuse the generator you have to recompute it\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, ['winter is coming'.split()])\n",
    "print([list(x) for x in padded_ngrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Ngram Counter\n",
    "NLTK provides NgramCounter class to do the ngram counting. The class can be initialized without any agrument and then updated with ngrams (via `update()` method); or directly initialized with ngrams.\n",
    "\n",
    "`N()` method returns total number of ngrams stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "\n",
    "counter = NgramCounter()\n",
    "counter.N()  # return total number of stored ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter.update(padded_ngrams)  # update counter with ngrams\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter = NgramCounter(padded_ngrams)\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.2.1. Accessing Ngram Counts\n",
    "\n",
    "Ngram counts can be accessed using standard python dictionary notation. \n",
    "- Ngram order counts can be accessed using integer keys (order)\n",
    "    - returns `Frequency Distribution` or `Conditional Frequency Distribution` objects. [link](https://www.nltk.org/api/nltk.probability.html)\n",
    "- Unigram counts can be accessed using string keys.\n",
    "- Bigram counts can be accessed using list keys & string keys (see example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 11 samples and 15 outcomes>\n",
      "<ConditionalFreqDist with 10 conditions>\n"
     ]
    }
   ],
   "source": [
    "# ngram order counts\n",
    "print(counter[1])  # Frequency Distribution\n",
    "print(counter[2])  # Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 11 samples and 15 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# print counts\n",
    "print(counter.unigrams)  # unigram count for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigram counts\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'full' bigram counts (full bigrams)\n",
    "counter[['the']]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "FreqDist({'sense': 1, 'answer': 1})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a frequency distribution over all continuations, you can use list or tuple. \n",
    "# counter[['the']] or counter[('the',)]\n",
    "counter[['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "FreqDist({'sense': 1, 'answer': 1})"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Conditional Frequency Distributions, only tuples are accepted.\n",
    "# counter[2][['the']] with throw an error\n",
    "counter[2][('the',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Counting Ngrams in Shakespeare's Hamlet\n",
    "\n",
    "- Load Shakespeare's Hamlet from Gutenberg corpus\n",
    "    - lowercase it\n",
    "\n",
    "- Extract padded unigrams and bigrams\n",
    "\n",
    "- Using NgramCounter\n",
    "    - get total number of ngrams\n",
    "    - get count of unigram `the`\n",
    "    - get count of bigram `of the`\n",
    "    \n",
    "|                     | Count  | \n",
    "|---------------------|---------|\n",
    "| Ngrams      | 84038     | \n",
    "| Unigram *the* | 993|\n",
    "| Bigram *of the*     |59 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamlet:  ['Actus', 'Primus', '.']\n",
      "hamlet_lowercase:  ['actus', 'primus', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# It returns a list of lists\n",
    "hamlet = gutenberg.sents('shakespeare-hamlet.txt')\n",
    "\n",
    "# lowercasing use .lower()\n",
    "hamlet_lowercase = [[word.lower() for word in sent] for sent in hamlet]\n",
    "\n",
    "# double check\n",
    "print(\"hamlet: \", hamlet[1])\n",
    "print(\"hamlet_lowercase: \", hamlet_lowercase[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text_ = padded_everygram_pipeline(2, hamlet_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "counter = NgramCounter(padded_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 84038 ngrams>\n",
      "993\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "print(counter) # Total number of Ngrams \n",
    "print(counter[\"the\"]) # the\n",
    "print(counter[[\"of\"]][\"the\"]) # of the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Vocabulary and Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[`Vocabulary`](https://www.nltk.org/api/nltk.lm.vocabulary.html) class of NLTK satisfies two common language modeling requirements for a vocabulary:\n",
    "- When checking membership and calculating its size, filters items by comparing their counts to a cutoff value.\n",
    "- Adds a special \"unknown\" token which unseen words are mapped to.\n",
    "\n",
    "Tokens with counts greater than or equal to the cut-off value will be considered part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary(counts=None, unk_cutoff=1, unk_label='<UNK>)` create a new `Vocabulary`.\n",
    "\n",
    "- `counts` (optional) iterable or collections. \n",
    "    - Counter instance to pre-seed the `Vocabulary`. \n",
    "    - In case it is iterable, counts are calculated.\n",
    "\n",
    "- `unk_cutoff` (int) - Words that occur less frequently than this value are not considered part of the vocabulary.\n",
    "\n",
    "- `unk_label` - Label for marking words not part of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "hamlet_words = gutenberg.words('shakespeare-hamlet.txt')\n",
    "\n",
    "# lowercase\n",
    "hamlet_words = [w.lower() for w in hamlet_words]\n",
    "\n",
    "# initialize vocabulary with cut-off\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary` methods:\n",
    "\n",
    "- `lookup()` looks up words in a vocabulary. \n",
    "    - \"Unseen\" words (with counts less than cutoff) are looked up as the unknown label. \n",
    "    - If given one word (a string) as an input, this method will return a string.\n",
    "    - If given a sequence, it will return an tuple of the looked up words.\n",
    "    \n",
    "- `update()` update counts from a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Vocabulary` properties:\n",
    "\n",
    "- `cutoff` - same as `unk_cutoff`\n",
    "\n",
    "- `unk_label` and `counts` can also be accessed as properties\n",
    "    - `vocab.unk_label`\n",
    "    - `vocab.counts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Counts, Vocabulary Membership, and Lookup\n",
    "\n",
    "Tokens with frequency counts less than the `cutoff` value will be considered not part of the vocabular, even though their entries in the count dictionary are preserved. \n",
    "\n",
    "This is useful for changing cut-off without recomputing counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's define a function to illustrate the relation between counts, membership, and lookup\n",
    "def test_word(word, vocab):\n",
    "    # let's lowercase it\n",
    "    if word != vocab.unk_label:\n",
    "        word = word.lower() \n",
    "    # membership test\n",
    "    word_membership = word in vocab\n",
    "    # accessing count\n",
    "    word_count = vocab[word]\n",
    "    # lookup\n",
    "    word_lookup = vocab.lookup(word)\n",
    "    \n",
    "    print(\"Word: '{}', Count: {}, In Vocab: {}, Mapped to: '{}'\".format(\n",
    "        word, word_count, word_membership, word_lookup))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: '<UNK>', Count: 2, In Vocab: True, Mapped to: '<UNK>'\n",
      "Word: 'the', Count: 993, In Vocab: True, Mapped to: 'the'\n",
      "Word: '1599', Count: 1, In Vocab: False, Mapped to: '<UNK>'\n",
      "Word: 'trento', Count: 0, In Vocab: False, Mapped to: '<UNK>'\n"
     ]
    }
   ],
   "source": [
    "for w in [\"<UNK>\", \"the\", \"1599\", \"Trento\"]:\n",
    "    test_word(w, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Cut-Off and Vocabulary Size\n",
    "The cut-off value influences not only membership checking but also the result of getting the size of the vocabulary using the built-in `len`. Note that while the number of keys in the vocabulary's counter stays the same, the items in the vocabulary differ depending on the cut-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define another vocabulary with cut-off 1\n",
    "# Notice that we are passing counts from the original vocabulary\n",
    "vocab1 = Vocabulary(vocab.counts, unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutOff 2: 1867\n",
      "CutOff 1: 4717\n"
     ]
    }
   ],
   "source": [
    "print(\"CutOff 2:\", len(vocab))\n",
    "print(\"CutOff 1:\", len(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutOff 2 Counts: 4716\n",
      "CutOff 1 Counts: 4716\n"
     ]
    }
   ],
   "source": [
    "print(\"CutOff 2 Counts:\", len(vocab.counts))\n",
    "print(\"CutOff 1 Counts:\", len(vocab1.counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "- lookup in vocabulary\n",
    "    - \"trento is the capital city of trentino\"\n",
    "- update vocabulary with \"trento is the capital city of trentino\"\n",
    "    - do the lookup again to see the effect\n",
    "- experiment with changing the cut-off value from `1` to `10`\n",
    "    - do the lookup again to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-off 0\n",
      "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')\n",
      "('trento', 'is', 'the', 'capital', 'city', 'of', 'trentino')\n"
     ]
    }
   ],
   "source": [
    "sentence = \"trento is the capital city of trentino\".split()\n",
    "\n",
    "# Cut-off 0 \n",
    "print('Cut-off 0')\n",
    "vocab = Vocabulary(hamlet_words)\n",
    "# Print lookup before vocab update using vocab.lookup\n",
    "print(vocab.lookup(sentence))\n",
    "# Update vocab using vocab.update\n",
    "vocab.update(sentence)\n",
    "# Print lookup after vocab update using vocab.lookup\n",
    "print(vocab.lookup(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-off 1\n",
      "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')\n",
      "('trento', 'is', 'the', 'capital', 'city', 'of', 'trentino')\n",
      "\n",
      "Cut-off 2\n",
      "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 3\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 4\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 5\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 6\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 7\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 8\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 9\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "\n",
      "Cut-off 10\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n",
      "('<UNK>', 'is', 'the', '<UNK>', '<UNK>', 'of', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "print('Cut-off 1')\n",
    "# Cut-off 1\n",
    "# Reinitialize vocab with unk_cutoff= 1\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=1)\n",
    "# Print lookup before vocab update\n",
    "print(vocab.lookup(sentence))\n",
    "# Update vocab\n",
    "vocab.update(sentence)\n",
    "# Print lookup after vocab update\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 2\n",
    "print('\\nCut-off 2')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 3\n",
    "print('\\nCut-off 3')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=3)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 4\n",
    "print('\\nCut-off 4')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=4)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 5\n",
    "print('\\nCut-off 5')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=5)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 6\n",
    "print('\\nCut-off 6')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=6)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 7\n",
    "print('\\nCut-off 7')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=7)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 8\n",
    "print('\\nCut-off 8')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=8)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# cut-off 9\n",
    "print('\\nCut-off 9')\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=9)\n",
    "print(vocab.lookup(sentence))\n",
    "vocab.update(sentence)\n",
    "print(vocab.lookup(sentence))\n",
    "\n",
    "# Cut-off 10\n",
    "print('\\nCut-off 10')\n",
    "# Reinitialize vocab with unk_cutoff= 10\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=10)\n",
    "# Print lookup before vocab update\n",
    "print(vocab.lookup(sentence))\n",
    "# Update vocab\n",
    "vocab.update(sentence)\n",
    "# Print lookup after vocab update\n",
    "print(vocab.lookup(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Training Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the data prepared as we have seen above, the `NLTK` ngram model boils down to counting ngrams, as presented above, by specifying the highest ngram size order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's prepare data on hamlet\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "data = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's train a `Maximum Likelihood Estimator (MLE)\n",
    "from nltk.lm import MLE\n",
    "\n",
    "mle_lm = MLE(2) # Where 2 is the highest N-gram size order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Initialization of the Language Model creates:\n",
    "- empty vocabulary\n",
    "- empty counts\n",
    "\n",
    "These are populated once we `fit` the model with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 0 items>\n",
      "<NgramCounter with 1 ngram orders and 0 ngrams>\n"
     ]
    }
   ],
   "source": [
    "# .vocab refers to the Vocabulary class that we have seen before\n",
    "print(mle_lm.vocab)\n",
    "# .vocab refers to the NgramCounter class that we have seen before\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mle_lm.fit(padded_ngrams, flat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4719 items>\n",
      "<NgramCounter with 2 ngram orders and 84038 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.vocab)\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $P(w_{1},\\ldots ,w_{n})$ ($P(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "<!-- It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Probability of a Sequence\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities ([Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))). \n",
    "\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$P(w_i|w_{1}^{i-1}) \\approx P(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\n",
    "N-gram   | Equation                   |\n",
    ":--------|:---------------------------|\n",
    "unigram  | $$P(w_i)$$                 |\n",
    "bigram   | $$P(w_i|w_{i-1})$$         |\n",
    "trigram  | $$P(w_i|w_{i-2},w_{i-1})$$ |\n",
    "\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$P(w_{1}^{n}) = \\prod_{i=1}^{n}{P(w_i|w_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ \n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ \n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus (x could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.1. Scoring Methods of NLTK LMs\n",
    "- `score(word, context=None)` Masks out of vocab (OOV) words (i.e. maps them to `<UNK>`) and computes their model score.\n",
    "    - scores are **model-specific** (later on this)\n",
    "    - Parameters:\n",
    "        - `word (str)` - Word for which we want the score\n",
    "        - `context (tuple(str))` - Context the word is in. If None, computes unigram score.\n",
    "        \n",
    "- `logscore(word, context=None)` - Evaluate the log score of this word in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.02278986505095015"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little illustration of the methods\n",
    "mle_lm.score(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.09672131147540984"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.score(\"the\", [\"of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "-3.3700223830884077"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "mle_lm.logscore(\"the\", [\"of\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "Implement a function to compute score of a sequence (i.e. Chain Rule)\n",
    "\n",
    "- arguments:\n",
    "    - Language Model\n",
    "    - List of Tokens\n",
    "\n",
    "- functionality\n",
    "    - extracts ngrams w.r.t. LM order (`lm.order`)\n",
    "    - scores each ngram w.r.t. LM (`lm.score` or `lm.logscore`)\n",
    "        - note: `score` already takes care of OOV by conterting to `<UNK>` \n",
    "    - computes the overal score using chain rule\n",
    "        - note: the difference between `score` and `logscore`\n",
    "\n",
    "- compute the scores of the `test_sents`\n",
    "    - compute the scores of padded and unpadded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_sents = [\"the king is dead\", \"the tzar is dead\", 'the tragedie of hamlet is good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the king is dead -17.417538571961195\n",
      "the tzar is dead -inf\n",
      "the tragedie of hamlet is good -29.887795439484073\n"
     ]
    }
   ],
   "source": [
    "def chain_rule(lm, sentence, log=True, pad=True):\n",
    "    highest_ngram = lm.order\n",
    "    tokens = sentence.split()\n",
    "    if pad:\n",
    "        # use padded everygram pipeline with the ngram is the highest ngram\n",
    "        ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [tokens])\n",
    "        ngrams = chain.from_iterable(ngrams) # Flat the sequence\n",
    "    else:\n",
    "        # use everygrams with max len= highest ngrams\n",
    "        ngrams = everygrams(tokens, max_len=highest_ngram)\n",
    "        flat_text = tokens\n",
    "    \n",
    "    if log:\n",
    "        total_score = 0\n",
    "    else:\n",
    "        total_score = 1\n",
    "        \n",
    "\n",
    "    for x in ngrams:\n",
    "        # x: (w_t-N, w_t-(N-1), w_t-(N-2), ... ,w_t)\n",
    "        if len(x) == highest_ngram:\n",
    "            if log:\n",
    "                w_t = x[-1]\n",
    "                context = x[:-1]\n",
    "                score = lm.logscore(w_t, context)\n",
    "                # Add or multiply score to total_score ?\n",
    "                total_score += score\n",
    "            else:\n",
    "                w_t = x[-1]\n",
    "                context = x[:-1]\n",
    "                score = lm.score(w_t, context)\n",
    "                # Add or multiply score to total_score ?\n",
    "                total_score *= score\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "for sent in test_sents:\n",
    "    print(sent, chain_rule(mle_lm, sent, log=True, pad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.2. OOV in MLE\n",
    "In MLE LM we did not take care of OOV. Consequently, those have `0` counts and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.score(\"<UNK>\"))\n",
    "print(mle_lm.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(mle_lm.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# same as above: getting lower-caseed sentences and words\n",
    "hamlet_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "hamlet_words = flatten(hamlet_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867\n"
     ]
    }
   ],
   "source": [
    "# computing vocabulary with cutoff\n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "print(len(lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'hamlet', 'by', '<UNK>', '<UNK>', '<UNK>', ']']\n"
     ]
    }
   ],
   "source": [
    "# replacing words with counts below cutoff with '<UNK>'\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "print(hamlet_oov_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# extracting ngrams & words again\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06540897824290828\n",
      "0.06842105263157895\n",
      "0.24874118831822759\n"
     ]
    }
   ],
   "source": [
    "print(lm_oov.score(\"<UNK>\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(lm_oov.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.3. Data Sparsity: Missing Ngrams\n",
    "However, even though we have counted ngrams on the data set with `<UNK>`, the counts still do not account for all possible ngrams. Thus, some possible sequences will have zero probability.\n",
    "\n",
    "__We will address this later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018360414945377767\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(lm_oov.score(\"queen\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"queen\"]))\n",
    "print(lm_oov.score(\"queen\", [\"<UNK>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = $ `<s>`;\n",
    "- *while* $w_i \\neq $ `</s>`\n",
    "\n",
    "    - stochastically get new word w.r.t. $P(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.1. Generation Method of NLTK LM\n",
    "- `generate(num_words=1, text_seed=None, random_seed=None)` generates words from the model.\n",
    "    - Parameters\n",
    "        - `num_words (int)` - How many words to generate. By default 1.\n",
    "        - `text_seed` - preceding context the generation should be conditioned on.\n",
    "            - ngram model is restricted in how much preceding context it can take into account based on max ngram size\n",
    "        - `random_seed` - A random seed. If provided, makes the random sampling part of generation reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['.', '</s>', 'her', 'selfe', 'scapes']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['now', '?', '</s>', 'earth', 'a']"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.generate(5, text_seed=['hamlet', \"pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3. Language Model Evaluation\n",
    "\n",
    "Language Models are evaluated using [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\n",
    "$$ PP(W) = \\sqrt[N]{\\frac{1}{P(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_i|w_{i-N+1})}}$$\n",
    "\n",
    "Where $N$ is the number of words in test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.3.1. Evaluation Methods of NLTK LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`NLTK` provides both **perplexity** and **cross-entropy** as evaluation methods\n",
    "\n",
    "- `entropy(text_ngrams)` calculates cross-entropy of model for given evaluation text.\n",
    "    - Parameters\n",
    "        - `text_ngrams (Iterable(tuple(str)))` A sequence of ngram tuples.\n",
    "\n",
    "- `perplexity(text_ngrams)` calculates the perplexity of the given text.\n",
    "    - This is _2 ** cross-entropy_ for the text, so the arguments are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Perplexity** is related to [**Cross-Entropy**](https://en.wikipedia.org/wiki/Cross_entropy) as:\n",
    "\n",
    "$$PP(p) = 2^{H(p)}$$ (abbreviated as PPL also)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "Compute entropy and perplexity of the `MLE` models  on the bigrams of the test sentences below, treating them as a test set.\n",
    "\n",
    "- experiment with the two test sets\n",
    "- experiment with OOVs (with vs without)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_sents1 = [\"the king is dead\", \"the emperor is dead\", \"may the force be with you\"]\n",
    "test_sents2 = [\"the king is dead\", \"welcome to you\", \"how are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "hamlet_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "hamlet_words = flatten(hamlet_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "data": {
      "text/plain": "['[',\n 'the',\n 'tragedie',\n 'of',\n 'hamlet',\n 'by',\n 'william',\n 'shakespeare',\n '1599',\n ']',\n 'actus',\n 'primus',\n '.',\n 'scoena',\n 'prima',\n '.',\n 'enter',\n 'barnardo',\n 'and',\n 'francisco',\n 'two',\n 'centinels',\n '.',\n 'barnardo',\n '.',\n 'who',\n \"'\",\n 's',\n 'there',\n '?',\n 'fran',\n '.',\n 'nay',\n 'answer',\n 'me',\n ':',\n 'stand',\n '&',\n 'vnfold',\n 'your',\n 'selfe',\n 'bar',\n '.',\n 'long',\n 'liue',\n 'the',\n 'king',\n 'fran',\n '.',\n 'barnardo',\n '?',\n 'bar',\n '.',\n 'he',\n 'fran',\n '.',\n 'you',\n 'come',\n 'most',\n 'carefully',\n 'vpon',\n 'your',\n 'houre',\n 'bar',\n '.',\n \"'\",\n 'tis',\n 'now',\n 'strook',\n 'twelue',\n ',',\n 'get',\n 'thee',\n 'to',\n 'bed',\n 'francisco',\n 'fran',\n '.',\n 'for',\n 'this',\n 'releefe',\n 'much',\n 'thankes',\n ':',\n \"'\",\n 'tis',\n 'bitter',\n 'cold',\n ',',\n 'and',\n 'i',\n 'am',\n 'sicke',\n 'at',\n 'heart',\n 'barn',\n '.',\n 'haue',\n 'you',\n 'had',\n 'quiet',\n 'guard',\n '?',\n 'fran',\n '.',\n 'not',\n 'a',\n 'mouse',\n 'stirring',\n 'barn',\n '.',\n 'well',\n ',',\n 'goodnight',\n '.',\n 'if',\n 'you',\n 'do',\n 'meet',\n 'horatio',\n 'and',\n 'marcellus',\n ',',\n 'the',\n 'riuals',\n 'of',\n 'my',\n 'watch',\n ',',\n 'bid',\n 'them',\n 'make',\n 'hast',\n '.',\n 'enter',\n 'horatio',\n 'and',\n 'marcellus',\n '.',\n 'fran',\n '.',\n 'i',\n 'thinke',\n 'i',\n 'heare',\n 'them',\n '.',\n 'stand',\n ':',\n 'who',\n \"'\",\n 's',\n 'there',\n '?',\n 'hor',\n '.',\n 'friends',\n 'to',\n 'this',\n 'ground',\n 'mar',\n '.',\n 'and',\n 'leige',\n '-',\n 'men',\n 'to',\n 'the',\n 'dane',\n 'fran',\n '.',\n 'giue',\n 'you',\n 'good',\n 'night',\n 'mar',\n '.',\n 'o',\n 'farwel',\n 'honest',\n 'soldier',\n ',',\n 'who',\n 'hath',\n 'relieu',\n \"'\",\n 'd',\n 'you',\n '?',\n 'fra',\n '.',\n 'barnardo',\n 'ha',\n \"'\",\n 's',\n 'my',\n 'place',\n ':',\n 'giue',\n 'you',\n 'goodnight',\n '.',\n 'exit',\n 'fran',\n '.',\n 'mar',\n '.',\n 'holla',\n 'barnardo',\n 'bar',\n '.',\n 'say',\n ',',\n 'what',\n 'is',\n 'horatio',\n 'there',\n '?',\n 'hor',\n '.',\n 'a',\n 'peece',\n 'of',\n 'him',\n 'bar',\n '.',\n 'welcome',\n 'horatio',\n ',',\n 'welcome',\n 'good',\n 'marcellus',\n 'mar',\n '.',\n 'what',\n ',',\n 'ha',\n \"'\",\n 's',\n 'this',\n 'thing',\n 'appear',\n \"'\",\n 'd',\n 'againe',\n 'to',\n 'night',\n 'bar',\n '.',\n 'i',\n 'haue',\n 'seene',\n 'nothing',\n 'mar',\n '.',\n 'horatio',\n 'saies',\n ',',\n \"'\",\n 'tis',\n 'but',\n 'our',\n 'fantasie',\n ',',\n 'and',\n 'will',\n 'not',\n 'let',\n 'beleefe',\n 'take',\n 'hold',\n 'of',\n 'him',\n 'touching',\n 'this',\n 'dreaded',\n 'sight',\n ',',\n 'twice',\n 'seene',\n 'of',\n 'vs',\n ',',\n 'therefore',\n 'i',\n 'haue',\n 'intreated',\n 'him',\n 'along',\n 'with',\n 'vs',\n ',',\n 'to',\n 'watch',\n 'the',\n 'minutes',\n 'of',\n 'this',\n 'night',\n ',',\n 'that',\n 'if',\n 'againe',\n 'this',\n 'apparition',\n 'come',\n ',',\n 'he',\n 'may',\n 'approue',\n 'our',\n 'eyes',\n ',',\n 'and',\n 'speake',\n 'to',\n 'it',\n 'hor',\n '.',\n 'tush',\n ',',\n 'tush',\n ',',\n \"'\",\n 'twill',\n 'not',\n 'appeare',\n 'bar',\n '.',\n 'sit',\n 'downe',\n 'a',\n '-',\n 'while',\n ',',\n 'and',\n 'let',\n 'vs',\n 'once',\n 'againe',\n 'assaile',\n 'your',\n 'eares',\n ',',\n 'that',\n 'are',\n 'so',\n 'fortified',\n 'against',\n 'our',\n 'story',\n ',',\n 'what',\n 'we',\n 'two',\n 'nights',\n 'haue',\n 'seene',\n 'hor',\n '.',\n 'well',\n ',',\n 'sit',\n 'we',\n 'downe',\n ',',\n 'and',\n 'let',\n 'vs',\n 'heare',\n 'barnardo',\n 'speake',\n 'of',\n 'this',\n 'barn',\n '.',\n 'last',\n 'night',\n 'of',\n 'all',\n ',',\n 'when',\n 'yond',\n 'same',\n 'starre',\n 'that',\n \"'\",\n 's',\n 'westward',\n 'from',\n 'the',\n 'pole',\n 'had',\n 'made',\n 'his',\n 'course',\n 't',\n \"'\",\n 'illume',\n 'that',\n 'part',\n 'of',\n 'heauen',\n 'where',\n 'now',\n 'it',\n 'burnes',\n ',',\n 'marcellus',\n 'and',\n 'my',\n 'selfe',\n ',',\n 'the',\n 'bell',\n 'then',\n 'beating',\n 'one',\n 'mar',\n '.',\n 'peace',\n ',',\n 'breake',\n 'thee',\n 'of',\n ':',\n 'enter',\n 'the',\n 'ghost',\n '.',\n 'looke',\n 'where',\n 'it',\n 'comes',\n 'againe',\n 'barn',\n '.',\n 'in',\n 'the',\n 'same',\n 'figure',\n ',',\n 'like',\n 'the',\n 'king',\n 'that',\n \"'\",\n 's',\n 'dead',\n 'mar',\n '.',\n 'thou',\n 'art',\n 'a',\n 'scholler',\n ';',\n 'speake',\n 'to',\n 'it',\n 'horatio',\n 'barn',\n '.',\n 'lookes',\n 'it',\n 'not',\n 'like',\n 'the',\n 'king',\n '?',\n 'marke',\n 'it',\n 'horatio',\n 'hora',\n '.',\n 'most',\n 'like',\n ':',\n 'it',\n 'harrowes',\n 'me',\n 'with',\n 'fear',\n '&',\n 'wonder',\n 'barn',\n '.',\n 'it',\n 'would',\n 'be',\n 'spoke',\n 'too',\n 'mar',\n '.',\n 'question',\n 'it',\n 'horatio',\n 'hor',\n '.',\n 'what',\n 'art',\n 'thou',\n 'that',\n 'vsurp',\n \"'\",\n 'st',\n 'this',\n 'time',\n 'of',\n 'night',\n ',',\n 'together',\n 'with',\n 'that',\n 'faire',\n 'and',\n 'warlike',\n 'forme',\n 'in',\n 'which',\n 'the',\n 'maiesty',\n 'of',\n 'buried',\n 'denmarke',\n 'did',\n 'sometimes',\n 'march',\n ':',\n 'by',\n 'heauen',\n 'i',\n 'charge',\n 'thee',\n 'speake',\n 'mar',\n '.',\n 'it',\n 'is',\n 'offended',\n 'barn',\n '.',\n 'see',\n ',',\n 'it',\n 'stalkes',\n 'away',\n 'hor',\n '.',\n 'stay',\n ':',\n 'speake',\n ';',\n 'speake',\n ':',\n 'i',\n 'charge',\n 'thee',\n ',',\n 'speake',\n '.',\n 'exit',\n 'the',\n 'ghost',\n '.',\n 'mar',\n '.',\n \"'\",\n 'tis',\n 'gone',\n ',',\n 'and',\n 'will',\n 'not',\n 'answer',\n 'barn',\n '.',\n 'how',\n 'now',\n 'horatio',\n '?',\n 'you',\n 'tremble',\n '&',\n 'look',\n 'pale',\n ':',\n 'is',\n 'not',\n 'this',\n 'something',\n 'more',\n 'then',\n 'fantasie',\n '?',\n 'what',\n 'thinke',\n 'you',\n 'on',\n \"'\",\n 't',\n '?',\n 'hor',\n '.',\n 'before',\n 'my',\n 'god',\n ',',\n 'i',\n 'might',\n 'not',\n 'this',\n 'beleeue',\n 'without',\n 'the',\n 'sensible',\n 'and',\n 'true',\n 'auouch',\n 'of',\n 'mine',\n 'owne',\n 'eyes',\n 'mar',\n '.',\n 'is',\n 'it',\n 'not',\n 'like',\n 'the',\n 'king',\n '?',\n 'hor',\n '.',\n 'as',\n 'thou',\n 'art',\n 'to',\n 'thy',\n 'selfe',\n ',',\n 'such',\n 'was',\n 'the',\n 'very',\n 'armour',\n 'he',\n 'had',\n 'on',\n ',',\n 'when',\n 'th',\n \"'\",\n 'ambitious',\n 'norwey',\n 'combatted',\n ':',\n 'so',\n 'frown',\n \"'\",\n 'd',\n 'he',\n 'once',\n ',',\n 'when',\n 'in',\n 'an',\n 'angry',\n 'parle',\n 'he',\n 'smot',\n 'the',\n 'sledded',\n 'pollax',\n 'on',\n 'the',\n 'ice',\n '.',\n \"'\",\n 'tis',\n 'strange',\n 'mar',\n '.',\n 'thus',\n 'twice',\n 'before',\n ',',\n 'and',\n 'iust',\n 'at',\n 'this',\n 'dead',\n 'houre',\n ',',\n 'with',\n 'martiall',\n 'stalke',\n ',',\n 'hath',\n 'he',\n 'gone',\n 'by',\n 'our',\n 'watch',\n 'hor',\n '.',\n 'in',\n 'what',\n 'particular',\n 'thought',\n 'to',\n 'work',\n ',',\n 'i',\n 'know',\n 'not',\n ':',\n 'but',\n 'in',\n 'the',\n 'grosse',\n 'and',\n 'scope',\n 'of',\n 'my',\n 'opinion',\n ',',\n 'this',\n 'boades',\n 'some',\n 'strange',\n 'erruption',\n 'to',\n 'our',\n 'state',\n 'mar',\n '.',\n 'good',\n 'now',\n 'sit',\n 'downe',\n ',',\n '&',\n 'tell',\n 'me',\n 'he',\n 'that',\n 'knowes',\n 'why',\n 'this',\n 'same',\n 'strict',\n 'and',\n 'most',\n 'obseruant',\n 'watch',\n ',',\n 'so',\n 'nightly',\n 'toyles',\n 'the',\n 'subiect',\n 'of',\n 'the',\n 'land',\n ',',\n 'and',\n 'why',\n 'such',\n 'dayly',\n 'cast',\n 'of',\n 'brazon',\n 'cannon',\n 'and',\n 'forraigne',\n 'mart',\n 'for',\n 'implements',\n 'of',\n 'warre',\n ':',\n 'why',\n 'such',\n 'impresse',\n 'of',\n 'ship',\n '-',\n 'wrights',\n ',',\n 'whose',\n 'sore',\n 'taske',\n 'do',\n \"'\",\n 's',\n 'not',\n 'diuide',\n 'the',\n 'sunday',\n 'from',\n 'the',\n 'weeke',\n ',',\n 'what',\n 'might',\n 'be',\n 'toward',\n ',',\n 'that',\n 'this',\n 'sweaty',\n 'hast',\n 'doth',\n 'make',\n 'the',\n 'night',\n 'ioynt',\n '-',\n 'labourer',\n 'with',\n 'the',\n 'day',\n ':',\n 'who',\n 'is',\n \"'\",\n 't',\n 'that',\n 'can',\n 'informe',\n 'me',\n '?',\n 'hor',\n '.',\n 'that',\n 'can',\n 'i',\n ',',\n 'at',\n 'least',\n 'the',\n 'whisper',\n 'goes',\n 'so',\n ':',\n 'our',\n 'last',\n 'king',\n ',',\n 'whose',\n 'image',\n 'euen',\n 'but',\n 'now',\n 'appear',\n \"'\",\n 'd',\n 'to',\n 'vs',\n ',',\n 'was',\n '(',\n 'as',\n 'you',\n 'know',\n ')',\n 'by',\n 'fortinbras',\n 'of',\n 'norway',\n ',',\n '(',\n 'thereto',\n 'prick',\n \"'\",\n 'd',\n 'on',\n 'by',\n 'a',\n 'most',\n 'emulate',\n 'pride',\n ')',\n 'dar',\n \"'\",\n 'd',\n 'to',\n 'the',\n 'combate',\n '.',\n 'in',\n 'which',\n ',',\n 'our',\n 'valiant',\n 'hamlet',\n ',',\n '(',\n 'for',\n 'so',\n 'this',\n 'side',\n 'of',\n 'our',\n 'knowne',\n 'world',\n 'esteem',\n \"'\",\n 'd',\n 'him',\n ')',\n 'did',\n 'slay',\n 'this',\n 'fortinbras',\n ':',\n 'who',\n 'by',\n 'a',\n 'seal',\n \"'\",\n 'd',\n 'compact',\n ',',\n 'well',\n 'ratified',\n 'by',\n 'law',\n ',',\n 'and',\n 'heraldrie',\n ',',\n 'did',\n 'forfeite',\n '(',\n 'with',\n 'his',\n 'life',\n ')',\n 'all',\n 'those',\n 'his',\n 'lands',\n 'which',\n 'he',\n 'stood',\n 'seiz',\n \"'\",\n 'd',\n 'on',\n ',',\n 'to',\n 'the',\n 'conqueror',\n ':',\n 'against',\n 'the',\n 'which',\n ',',\n 'a',\n 'moity',\n 'competent',\n 'was',\n 'gaged',\n 'by',\n 'our',\n 'king',\n ':',\n 'which',\n 'had',\n 'return',\n \"'\",\n 'd',\n 'to',\n 'the',\n 'inheritance',\n 'of',\n 'fortinbras',\n ',',\n 'had',\n 'he',\n 'bin',\n 'vanquisher',\n ',',\n 'as',\n 'by',\n 'the',\n 'same',\n 'cou',\n \"'\",\n 'nant',\n 'and',\n 'carriage',\n 'of',\n 'the',\n 'article',\n 'designe',\n ',',\n 'his',\n 'fell',\n ...]"
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hamlet_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Compute vocab\n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "print(len(lex))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106\n"
     ]
    }
   ],
   "source": [
    "# Handeling OOV\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "print(len(hamlet_oov_sents))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [],
   "source": [
    "# Extracting ngrams & words\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL and entropy with OOV on test 1:\n",
      "PPL: 1.6406492301829485\n",
      "Cross Entropy : 0.7142668244892213\n",
      "\t PPL: 1.6406492301829485\n"
     ]
    }
   ],
   "source": [
    "# Compute PPL and entropy with OOV on test 1\n",
    "print(\"PPL and entropy with OOV on test 1:\")\n",
    "test_set = test_sents1\n",
    "\n",
    "# Compute PPL and entropy with OOV on test 1\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "ngrams = chain.from_iterable(ngrams)\n",
    "ppl =  lm_oov.perplexity([x for x in ngrams if len(x) == lm_oov.order])\n",
    "print('PPL:', ppl)\n",
    "\n",
    "# Generators are one-use only!\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "ngrams = chain.from_iterable(ngrams)\n",
    "cross_entropy = lm_oov.entropy([x for x in ngrams  if len(x) == lm_oov.order])\n",
    "print('Cross Entropy :', cross_entropy)\n",
    "print('\\t PPL:', pow(2, cross_entropy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL and entropy with OOV on test 2:\n",
      "PPL: 1.860269175143374\n",
      "Cross Entropy : 0.8955113899072307\n",
      "\t PPL: 1.860269175143374\n"
     ]
    }
   ],
   "source": [
    "# Compute PPL and entropy with OOV on test 2\n",
    "print(\"PPL and entropy with OOV on test 2:\")\n",
    "test_set = test_sents2\n",
    "\n",
    "# Compute PPL and entropy with OOV on test 1\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "ngrams = chain.from_iterable(ngrams)\n",
    "ppl =  lm_oov.perplexity([x for x in ngrams if len(x) == lm_oov.order])\n",
    "print('PPL:', ppl)\n",
    "\n",
    "# Generators are one-use only!\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "ngrams = chain.from_iterable(ngrams)\n",
    "cross_entropy = lm_oov.entropy([x for x in ngrams  if len(x) == lm_oov.order])\n",
    "print('Cross Entropy :', cross_entropy)\n",
    "print('\\t PPL:', pow(2, cross_entropy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPL: how it works inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "49.091034440830974"
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_ppl_(model, data):\n",
    "    highest_ngram = model.order\n",
    "    scores = [] \n",
    "\n",
    "    for sentence in data:\n",
    "        # if sentence is type of list object\n",
    "        if isinstance(sentence, list):\n",
    "            ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence])\n",
    "        else:\n",
    "            ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence.split()])\n",
    "\n",
    "        scores.extend([-1 * model.logscore(w[-1], w[0:-1]) for gen in ngrams for w in gen if len(w) == highest_ngram])\n",
    "\n",
    "    return math.pow(2.0, np.asarray(scores).mean())\n",
    "\n",
    "compute_ppl_(mle_lm, test_sents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.4. Handling Data Sparseness: Smoothing in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[`Smoothing(vocabulary, counter, **kwargs)`](https://www.nltk.org/api/nltk.lm.smoothing.html) Class is initialized with the following parameters: \n",
    "\n",
    "- `vocabulary (nltk.lm.vocab.Vocabulary)` - The Ngram vocabulary object.\n",
    "- `counter (nltk.lm.counter.NgramCounter)` - The counts of the vocabulary items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Ngram Smoothing Interface` implements [Chen & Goodman 1995](https://aclanthology.org/P96-1041.pdf)'s idea that all smoothing algorithms have certain features in common. Consequently, each Smoothing subclass implements two methods:\n",
    "\n",
    "- `unigram_score(word)` return unigram score\n",
    "- `alpha_gamma(word, context)` returns alpha and gamma values (varies w.r.t. method)\n",
    "    - `gamma` is the value added to missing ngram count\n",
    "    - `alpha` is the value used to scale the lower order probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following smoothing methods are implemented:\n",
    "- `WittenBell(vocabulary, counter, **kwargs)` - Witten-Bell smoothing\n",
    "\n",
    "- `AbsoluteDiscounting(vocabulary, counter, discount=0.75, **kwargs)` - Smoothing with absolute discount\n",
    "    - takes `discount=0.75` parameter (default 0.75)\n",
    "    \n",
    "- `KneserNey(vocabulary, counter, order, discount=0.1, **kwargs)` - Kneser-Ney Smoothing (an extension of smoothing with a discount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5. NLTK Language Models\n",
    "\n",
    "All the language models in NLTK share the same interface and share methods for\n",
    "- evaluation\n",
    "- generation\n",
    "- scoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Scoring** in a language model does the following:\n",
    "- takes care of OOV words (see above)\n",
    "- computes `unmasked_score`\n",
    "\n",
    "The way `unmasked_score` is computed from counts is the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some langauge models use **smoothing** to account for missing ngrams, some do not (e.g. `MLE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Available LM Implementations\n",
    "\n",
    "- `MLE` - providing MLE ngram model scores.\n",
    "\n",
    "- Additive Smoothing LMs\n",
    "    - `Laplace` - Laplace (add one) smoothing ($\\gamma = 1$)\n",
    "    - `Lidstone` - same as `Laplace`, but adds $\\gamma$\n",
    "\n",
    "- Back-Off Language Models\n",
    "    - `StupidBackoff` - uses `alpha` to scale the lower order probabilities for computing missing ngram scores\n",
    "\n",
    "- Interpolated Language Models\n",
    "    - `WittenBellInterpolated` - Interpolated version of Witten-Bell smoothing\n",
    "    - `KneserNeyInterpolated` - Interpolated version of Kneser-Ney smoothing.\n",
    "    - `AbsoluteDiscountingInterpolated` - Interpolated version of smoothing with absolute discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab Exercise: Language Model Evaluation\n",
    "Write your own implementation of the Stupid backoff algorithm. Train it and compare the perplexity with the one provided by NLKT. The dataset that you have to use is the *Shakespeare Macbeth*. \n",
    "\n",
    "Stupid Backoff algorithm (use ⍺=0.4):\n",
    "https://aclanthology.org/D07-1090.pdf \n",
    "\n",
    "NLTK (StupidBackoff):\n",
    "https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "**Suggestion**: adapt the `compute_ppl` function to compute the perplexity of your model. The PPL has to be computed on the whole corpus and not at sentence level."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:41.072797Z",
     "start_time": "2023-06-05T08:38:41.034764Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load data\n",
    "macbeth_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-Macbeth.txt')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:42.267618100Z",
     "start_time": "2023-06-05T08:38:42.191617700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['[',\n 'the',\n 'tragedie',\n 'of',\n 'macbeth',\n 'by',\n 'william',\n 'shakespeare',\n '1603',\n ']']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_words = [w.lower() for w in gutenberg.words('shakespeare-Macbeth.txt')]\n",
    "macbeth_words[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:42.572616Z",
     "start_time": "2023-06-05T08:38:42.497617Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Vocabulary and ngrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# n-gram order\n",
    "n = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:44.051616300Z",
     "start_time": "2023-06-05T08:38:44.034615600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383\n"
     ]
    }
   ],
   "source": [
    "# computing vocabulary with cutoff\n",
    "vocab = Vocabulary(macbeth_words, unk_cutoff=2)\n",
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:44.662614500Z",
     "start_time": "2023-06-05T08:38:44.630615200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# extracting ngrams\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(n, macbeth_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:45.410389100Z",
     "start_time": "2023-06-05T08:38:45.370381200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 NLTK's StupidBackoff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# NLTK StupidBackoff\n",
    "from nltk.lm import StupidBackoff"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:47.836356500Z",
     "start_time": "2023-06-05T08:38:47.818357700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Train the Stupid Backoff model\n",
    "alpha = 0.4\n",
    "model_sb = StupidBackoff(alpha=alpha, order=n, vocabulary=vocab)\n",
    "model_sb.fit(padded_ngrams_oov, flat_text_oov)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:48.432357Z",
     "start_time": "2023-06-05T08:38:48.167358100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "test_sents1 = [\"the king is dead\", \"the emperor is dead\", \"may the force be with you\"]\n",
    "test_sents2 = [\"the king is dead\", \"welcome to you\", \"how are you\"]\n",
    "# test_sents3 = [['the', 'king', 'is', 'dead'], ['the', 'queen', 'is', 'alive']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:50.375360400Z",
     "start_time": "2023-06-05T08:38:50.349357900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "69.30257651989308"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute perplexity of the model\n",
    "def compute_ppl_(model, data):\n",
    "    highest_ngram = model.order\n",
    "    scores = []\n",
    "\n",
    "    for sentence in data:\n",
    "        # if sentence is type of list object\n",
    "        if isinstance(sentence, list):\n",
    "            ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence])\n",
    "        else:\n",
    "            ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence.split()])\n",
    "\n",
    "        scores.extend([-1 * model.logscore(w[-1], w[0:-1]) for gen in ngrams for w in gen if len(w) == highest_ngram])\n",
    "\n",
    "    return math.pow(2.0, np.asarray(scores).mean())\n",
    "\n",
    "compute_ppl_(model_sb, test_sents2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:51.199356700Z",
     "start_time": "2023-06-05T08:38:51.158358400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['the', 'king', 'is', 'dead']\n",
      "\n",
      "Model: StupidBackoff\n",
      "\"king\" given \"('the',)\" has probability 0.0200\n",
      "\"is\" given \"('king',)\" has probability 0.0028\n",
      "\"dead\" given \"('is',)\" has probability 0.0213\n",
      "\n",
      "Sentence: ['the', 'queen', 'is', 'alive']\n",
      "\n",
      "Model: StupidBackoff\n",
      "\"queen\" given \"('the',)\" is an OOV word\n",
      "\"is\" given \"('queen',)\" has probability 0.0050\n",
      "\"alive\" given \"('is',)\" is an OOV word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the models with some example sentences\n",
    "test_sentences = [['the', 'king', 'is', 'dead'], ['the', 'queen', 'is', 'alive']]\n",
    "for sentence in test_sentences:\n",
    "    print(f'Sentence: {sentence}\\n')\n",
    "    for model in [model_sb]:\n",
    "        print(f'Model: {model.__class__.__name__}')\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            context = tuple(sentence[i:i+n-1])\n",
    "            word = sentence[i+n-1]\n",
    "            if word in vocab:\n",
    "                print(f'\"{word}\" given \"{context}\" has probability {model.score(word, context):.4f}')\n",
    "            else:\n",
    "                print(f'\"{word}\" given \"{context}\" is an OOV word')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:38:52.770389400Z",
     "start_time": "2023-06-05T08:38:52.712357700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Own StupidBackoff Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class MyStupidBackoff:\n",
    "    def __init__(self, corpus, n, alpha=0.4):\n",
    "        \"\"\"\n",
    "        Initialize the MyStupidBackoff language model.\n",
    "\n",
    "        Args:\n",
    "            corpus (list): The training corpus as a list of words.\n",
    "            n (int): The order of the n-gram model.\n",
    "            alpha (float): The backoff factor, defaults to 0.4.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.ngrams = defaultdict(lambda: defaultdict(Counter))  # Dictionary to store n-gram counts\n",
    "        self.N = len(corpus)  # Size of the training corpus\n",
    "        self.build_ngrams(corpus, n)  # Build the n-gram model\n",
    "\n",
    "    def build_ngrams(self, corpus, n):\n",
    "        \"\"\"\n",
    "        Build the n-gram model by counting n-grams and their frequencies.\n",
    "\n",
    "        Args:\n",
    "            corpus (list): The training corpus as a list of words.\n",
    "            n (int): The order of the n-gram model.\n",
    "        \"\"\"\n",
    "        for i in range(n-1, len(corpus)):\n",
    "            for j in range(i-n+1, i):\n",
    "                prefix = tuple(corpus[j:i])  # Context prefix of length n-1\n",
    "                token = corpus[i]  # Current word (target token)\n",
    "                self.ngrams[len(prefix)][prefix][token] += 1  # Increment n-gram count\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        Compute the Stupid Backoff score for a word given a context.\n",
    "\n",
    "        Args:\n",
    "            word (str): The target word.\n",
    "            context (list): The context as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            float: The Stupid Backoff score.\n",
    "        \"\"\"\n",
    "        context = tuple(context)\n",
    "        if context in self.ngrams[len(context)] and word in self.ngrams[len(context)][context]:\n",
    "            return self.ngrams[len(context)][context][word] / sum(self.ngrams[len(context)][context].values())\n",
    "        elif len(context) > 0:\n",
    "            return self.alpha * self.score(word, context[1:])\n",
    "        else:\n",
    "            return 1e-2  # Return a small probability for unseen words\n",
    "\n",
    "    def logscore(self, word, context):\n",
    "        \"\"\"\n",
    "        Compute the logarithm of the Stupid Backoff score for a word given a context.\n",
    "\n",
    "        Args:\n",
    "            word (str): The target word.\n",
    "            context (list): The context as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            float: The logarithm of the Stupid Backoff score.\n",
    "        \"\"\"\n",
    "        return math.log(self.score(word, context))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:40:43.779470200Z",
     "start_time": "2023-06-05T08:40:43.767469800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Train the Stupid Backoff model\n",
    "my_model = MyStupidBackoff(macbeth_words, n=2, alpha=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:40:45.344899900Z",
     "start_time": "2023-06-05T08:40:45.278858200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['the', 'king', 'is', 'dead']\n",
      "\n",
      "Model: MyStupidBackoff\n",
      "\"king\" given \"('the',)\" has probability 0.0200\n",
      "\"is\" given \"('king',)\" has probability 0.0040\n",
      "\"dead\" given \"('is',)\" has probability 0.0213\n",
      "\n",
      "Sentence: ['the', 'queen', 'is', 'alive']\n",
      "\n",
      "Model: MyStupidBackoff\n",
      "\"queen\" given \"('the',)\" is an OOV word\n",
      "\"is\" given \"('queen',)\" has probability 0.0040\n",
      "\"alive\" given \"('is',)\" is an OOV word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the models with some example sentences\n",
    "test_sentences = [['the', 'king', 'is', 'dead'], ['the', 'queen', 'is', 'alive']]\n",
    "for sentence in test_sentences:\n",
    "    print(f'Sentence: {sentence}\\n')\n",
    "    for model in [my_model]:\n",
    "        print(f'Model: {model.__class__.__name__}')\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            context = tuple(sentence[i:i+n-1])\n",
    "            word = sentence[i+n-1]\n",
    "            if word in vocab:\n",
    "                print(f'\"{word}\" given \"{context}\" has probability {model.score(word, context):.4f}')\n",
    "            else:\n",
    "                print(f'\"{word}\" given \"{context}\" is an OOV word')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:41:56.783861700Z",
     "start_time": "2023-06-05T08:41:56.720607500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of My Stupid Backoff model: 24.3385\n"
     ]
    }
   ],
   "source": [
    "# Compute the perplexity of the My Stupid Backoff model\n",
    "def compute_ppl_my_sb(model, data):\n",
    "    highest_ngram = max(model.ngrams.keys())\n",
    "    scores = []\n",
    "\n",
    "    for sentence in data:\n",
    "        # if sentence is type of list object\n",
    "        if isinstance(sentence, list):\n",
    "            ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence])\n",
    "        else:\n",
    "            ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence.split()])\n",
    "\n",
    "        scores.extend([-1 * model.logscore(w[-1], w[0:-1]) for gen in ngrams for w in gen if len(w) == highest_ngram])\n",
    "\n",
    "    return math.pow(2.0, np.asarray(scores).mean())\n",
    "\n",
    "# Compute perplexity of the My Stupid Backoff model\n",
    "my_sb_perplexity = compute_ppl_my_sb(my_model, test_sentences)\n",
    "\n",
    "print(f\"Perplexity of My Stupid Backoff model: {my_sb_perplexity:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:42:01.275341200Z",
     "start_time": "2023-06-05T08:42:01.178593400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final Comparisons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['the', 'king', 'is', 'dead']\n",
      "\n",
      "Model: StupidBackoff\n",
      "\"king\" given \"('the',)\" has probability 0.0200\n",
      "\"is\" given \"('king',)\" has probability 0.0028\n",
      "\"dead\" given \"('is',)\" has probability 0.0213\n",
      "\n",
      "Model: MyStupidBackoff\n",
      "\"king\" given \"('the',)\" has probability 0.0200\n",
      "\"is\" given \"('king',)\" has probability 0.0040\n",
      "\"dead\" given \"('is',)\" has probability 0.0213\n",
      "\n",
      "Sentence: ['the', 'queen', 'is', 'alive']\n",
      "\n",
      "Model: StupidBackoff\n",
      "\"queen\" given \"('the',)\" is an OOV word\n",
      "\"is\" given \"('queen',)\" has probability 0.0050\n",
      "\"alive\" given \"('is',)\" is an OOV word\n",
      "\n",
      "Model: MyStupidBackoff\n",
      "\"queen\" given \"('the',)\" is an OOV word\n",
      "\"is\" given \"('queen',)\" has probability 0.0040\n",
      "\"alive\" given \"('is',)\" is an OOV word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the models with some example sentences\n",
    "test_sentences = [['the', 'king', 'is', 'dead'], ['the', 'queen', 'is', 'alive']]\n",
    "for sentence in test_sentences:\n",
    "    print(f'Sentence: {sentence}\\n')\n",
    "    for model in [model_sb, my_model]:\n",
    "        print(f'Model: {model.__class__.__name__}')\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            context = tuple(sentence[i:i+n-1])\n",
    "            word = sentence[i+n-1]\n",
    "            if word in vocab:\n",
    "                print(f'\"{word}\" given \"{context}\" has probability {model.score(word, context):.4f}')\n",
    "            else:\n",
    "                print(f'\"{word}\" given \"{context}\" is an OOV word')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:44:11.697592700Z",
     "start_time": "2023-06-05T08:44:11.619534400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of NLTK's Stupid Backoff model: 69.3026\n",
      "Perplexity of My Stupid Backoff model: 24.3385\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity of NLTK's Stupid Backoff model: {compute_ppl_(model_sb, test_sents2):.4f}\")\n",
    "print(f\"Perplexity of My Stupid Backoff model: {my_sb_perplexity:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:45:31.496171700Z",
     "start_time": "2023-06-05T08:45:31.425157700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
