{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca7d9e5",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- pytorch \n",
    "    - Pytorch install: https://pytorch.org/get-started/locally/ \n",
    "- tqdm\n",
    "- sklearn\n",
    "- Huggingface Transformer: \n",
    "    - pip install transformers \n",
    "- **DATASET**:\n",
    "    - https://github.com/BrownFortress/IntentSlotDatasets\n",
    "    - We will use **ATIS** only\n",
    "    \n",
    "    \n",
    "\n",
    "# Outline\n",
    "\n",
    "#### Introduction\n",
    "- sequence labelling (Slot filling)\n",
    "- text classification (Intent classification)\n",
    "\n",
    "#### Preparing text for NN\n",
    "- word2id\n",
    "- special tokens \n",
    "- Customize Dataset class\n",
    "\n",
    "#### Split data in batches\n",
    "- Usage of Dataloader class\n",
    "- Padding sequences\n",
    "\n",
    "#### Neural Networks in Pytorch\n",
    "- Word emdbeddings\n",
    "- Implementation of an LSTM\n",
    "- Regularization techniques\n",
    "\n",
    "#### Train and Test a Neural Network\n",
    "- Optimizer\n",
    "- Loss function\n",
    "- Iteration over batches\n",
    "\n",
    "#### Hugging face library\n",
    "- Introduction and Usage\n",
    " \n",
    " \n",
    "# References\n",
    "- RNN: https://d2l.ai/chapter_recurrent-neural-networks/index.html \n",
    "- LSTM: https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "- GRU: https://d2l.ai/chapter_recurrent-modern/gru.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a0554",
   "metadata": {},
   "source": [
    "# Sequence Labelling and Text classification tasks\n",
    "\n",
    "### Sequence Labelling\n",
    "The sequence labelling task is defined as:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- defining a sequence of labels as $l = {l_1, l_2, ..., l_n}$\n",
    "- compute the sequence $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "In this lab session, we are going to see a particular case of sequence labelling task, which is named as Slot Filling (or Concept tagging). In this notebook, the **segmentation** (with IOB tags) and the **labelling** are done at the same time. \\\n",
    "\\\n",
    "An example is the following: \n",
    "\n",
    "| Slot Filling |  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output sequence: | O  | B-depart_date.month_name | B-depart_date.day_number | O | O    | O | O      |\n",
    "\n",
    "### Text classification\n",
    "The text classification problem is defined *(similar to Sequence Labelling)* as follows:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- And a set of labels $L$ where $l \\in L$\n",
    "- estimate the label $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "The text classification task that we are going to see in this laboratory is named as Intent Classification. The Intent is an additional component of the *semantic frame*. \\\n",
    "\\\n",
    "An example is the following:\n",
    "\n",
    "| Intent Classification|  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output label: | flight     |\n",
    "\n",
    "\n",
    "# Dataset\n",
    "The dataset that we are going to use is ATIS (Airline Travel Information Systems). It is composed of trascriptions of humans asking about flight information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3289e",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "I have prepared a custom data structure for this dataset. The srtucture is the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "    \"utterance\": \"on april first i need a flight going from phoenix to san diego\", \n",
    "    \"slots\": \"O B-depart_date.month_name B-depart_date.day_number O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name\", \n",
    "    \"intent\": \"flight\"\n",
    "    },\n",
    "    \"...\"\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80808524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:41:55.477475800Z",
     "start_time": "2023-06-14T23:41:55.441480800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "import os\n",
    "device = 'cuda:0' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1ea7f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:41:55.501477Z",
     "start_time": "2023-06-14T23:41:55.458476900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "{'intent': 'flight',\n",
      " 'slots': 'O O O O O B-fromloc.city_name O B-depart_time.time '\n",
      "          'I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O '\n",
      "          'B-arrive_time.period_of_day',\n",
      " 'utterance': 'i want to fly from boston at 838 am and arrive in denver at '\n",
      "              '1110 in the morning'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('dataset','ATIS','train.json'))\n",
    "test_raw = load_data(os.path.join('dataset','ATIS','test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "pprint(tmp_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b3f37",
   "metadata": {},
   "source": [
    "### Create a dev set\n",
    "In the original split the developement set (dev set) is missing. To train and find the best hyperparameter of our network the dev set is fundamental. Thus, we have to create it starting from the **traning** set. The dev set is usually the 10% of the dataset. \\\n",
    "Possible sampling strategies:\n",
    "* Take the last n elements of the training set.\n",
    "* Do a random sampling from the training set.\n",
    "* Do a stratified sampling from the training set using one or more criteria. (The best way)\n",
    "    * For further details look [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd6da44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:42:08.991082500Z",
     "start_time": "2023-06-14T23:42:05.427084200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.3,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.7000000000000002,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.3,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.3,\n",
      " 'distance': 0.3,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.5,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.3,\n",
      " 'ground_service': 5.2,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4381\n",
      "DEV size: 597\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Firt we get the 10% of dataset, then we compute the percentage of these examples \n",
    "# on the training set which is around 11% \n",
    "portion = round(((len(tmp_train_raw) + len(test_raw)) * 0.10)/(len(tmp_train_raw)),2)\n",
    "\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "mini_Train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occure once only, we put them in training\n",
    "        X.append(tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, Y, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y)\n",
    "X_train.extend(mini_Train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbdbe",
   "metadata": {},
   "source": [
    "### Corvert words to numbers (word2id)\n",
    "Neural Netwoks in Pytorch, as in other libraries, work with numbers and vectors. (the value type can be both integers or floats)<br><br>\n",
    "**Exercise** *(5 minutes)*\n",
    "* Create a dictionary that maps the words in the training set to numbers.\n",
    "* Define the Dataset class to easily handle the train, dev and test sets.\n",
    "\n",
    "We will see later how to conver these indexes into vectors (aka embeddings).\n",
    "\n",
    "*Add special tokens \"pad\" and \"unk\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea03b2aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:42:24.274784200Z",
     "start_time": "2023-06-14T23:42:24.241781400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab: 940\n",
      "# Slots: 129\n",
      "# Intent: 26\n"
     ]
    }
   ],
   "source": [
    "w2id = {'pad':PAD_TOKEN}\n",
    "slot2id = {'pad':PAD_TOKEN}\n",
    "intent2id = {}\n",
    "# Map the words only from the train set\n",
    "# Map slot and intent labels of train, dev and test set. 'unk' is not needed.\n",
    "words = [x['utterance'].split() for x in train_raw]\n",
    "slots = [x['slots'].split() for x in train_raw]\n",
    "intents = [x['intent'] for x in train_raw]\n",
    "\n",
    "# Create dictionaries\n",
    "for example in train_raw:\n",
    "    words = example['utterance'].split()\n",
    "    slots = example['slots'].split()\n",
    "    for word in words:\n",
    "        if word not in w2id:\n",
    "            w2id[word] = len(w2id)\n",
    "    for slot in slots:\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    if example['intent'] not in intent2id:\n",
    "        intent2id[example['intent']] = len(intent2id)\n",
    "\n",
    "for example in dev_raw:\n",
    "    words = example['utterance'].split()\n",
    "    slots = example['slots'].split()\n",
    "    for word in words:\n",
    "        if word not in w2id:\n",
    "            w2id[word] = len(w2id)\n",
    "    for slot in slots:\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    if example['intent'] not in intent2id:\n",
    "        intent2id[example['intent']] = len(intent2id)\n",
    "\n",
    "for example in test_raw:\n",
    "    words = example['utterance'].split()\n",
    "    slots = example['slots'].split()\n",
    "    for word in words:\n",
    "        if word not in w2id:\n",
    "            w2id[word] = len(w2id)\n",
    "    for slot in slots:\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    if example['intent'] not in intent2id:\n",
    "        intent2id[example['intent']] = len(intent2id)\n",
    "\n",
    "# Print size of dictionaries\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pad': 0, 'what': 1, 'type': 2, 'of': 3, 'aircraft': 4, 'does': 5, 'eastern': 6, 'fly': 7, 'from': 8, 'atlanta': 9, 'to': 10, 'denver': 11, 'before': 12, '6': 13, 'pm': 14, 'show': 15, 'me': 16, 'the': 17, 'fares': 18, 'dallas': 19, 'san': 20, 'francisco': 21, 'and': 22, 'flights': 23, 'pittsburgh': 24, 'philadelphia': 25, 'flight': 26, 'will': 27, 'originate': 28, 'boston': 29, 'are': 30, 'available': 31, 'on': 32, 'monday': 33, 'is': 34, 'first': 35, 'after': 36, '8': 37, 'am': 38, 'that': 39, 'arrive': 40, 'in': 41, 'afternoon': 42, 'give': 43, 'a': 44, 'list': 45, 'between': 46, 'oakland': 47, 'i': 48, 'need': 49, 'leaves': 50, 'late': 51, 'wednesday': 52, 'salt': 53, 'lake': 54, 'city': 55, 'there': 56, 'newark': 57, 'seattle': 58, 'saturday': 59, \"'d\": 60, 'like': 61, 'see': 62, 'all': 63, 'economy': 64, 'baltimore': 65, 'times': 66, 'for': 67, 'phoenix': 68, 'sfo': 69, 'let': 70, \"'s\": 71, 'how': 72, 'much': 73, 'would': 74, 'direct': 75, 'be': 76, 'may': 77, 'seventh': 78, 'do': 79, 'you': 80, 'have': 81, 'leaving': 82, 'arriving': 83, 'night': 84, 'my': 85, 'destination': 86, 'live': 87, 'august': 88, 'thirtieth': 89, 'morning': 90, 'any': 91, 'airline': 92, 'get': 93, 'overnight': 94, 'washington': 95, 'dc': 96, 'their': 97, 'schedule': 98, 'airlines': 99, 'going': 100, 'next': 101, 'cheapest': 102, 'one': 103, 'way': 104, '4': 105, 'houston': 106, 'los': 107, 'angeles': 108, 'departing': 109, '1026': 110, 'toronto': 111, 'besides': 112, 'continental': 113, 'flies': 114, 'can': 115, 'via': 116, 'at': 117, 'level': 118, 'find': 119, 'travel': 120, 'arrangements': 121, 'round': 122, 'trip': 123, '10': 124, 'leave': 125, 'nashville': 126, 'us': 127, 'air': 128, 'ground': 129, 'transportation': 130, 'look': 131, 'prices': 132, 'class': 133, 'okay': 134, 'tell': 135, 'earliest': 136, 'want': 137, 'milwaukee': 138, 'orlando': 139, 'either': 140, 'evening': 141, 'or': 142, 'thursday': 143, 'st.': 144, 'petersburg': 145, 'charlotte': 146, 'chicago': 147, 'also': 148, 'miami': 149, 'types': 150, 'airport': 151, 'please': 152, 'new': 153, 'york': 154, 'information': 155, 'only': 156, 'local': 157, 'time': 158, 'twenty': 159, 'fare': 160, 'least': 161, 'expensive': 162, '630': 163, 'your': 164, 'last': 165, 'tomorrow': 166, 'abbreviations': 167, 'go': 168, '5': 169, 'price': 170, 'limousine': 171, 'service': 172, 'kind': 173, 'used': 174, 'american': 175, 'with': 176, 'stopover': 177, 'day': 178, 'las': 179, 'vegas': 180, 'memphis': 181, 'sunday': 182, 'right': 183, 'indianapolis': 184, 'fort': 185, 'worth': 186, 'july': 187, 'third': 188, 'which': 189, \"o'clock\": 190, 'thirty': 191, 'nonstop': 192, 'far': 193, '838': 194, '1110': 195, 'bwi': 196, 'serves': 197, 'dinner': 198, '1024': 199, 'downtown': 200, 'stop': 201, 'noon': 202, 'flying': 203, 'fifth': 204, 'know': 205, 'about': 206, 'car': 207, 'rental': 208, 'thrift': 209, 'takeoff': 210, 'general': 211, 'mitchell': 212, 'international': 213, 'display': 214, 'codes': 215, 'long': 216, 'beach': 217, 'tampa': 218, 'diego': 219, 'northwest': 220, 'united': 221, 'stopovers': 222, 'lowest': 223, 'code': 224, 'qw': 225, 'latest': 226, 'cleveland': 227, 'transport': 228, 'ontario': 229, 'texas': 230, 'now': 231, 'tuesday': 232, 'no': 233, 'into': 234, '12': 235, '3': 236, '1991': 237, 'florida': 238, 'requesting': 239, 'mco': 240, 'f': 241, 'returning': 242, 'make': 243, 'starting': 244, 'back': 245, 'love': 246, 'field': 247, 'other': 248, 'airports': 249, 'train': 250, 'california': 251, 'depart': 252, 'burbank': 253, 'by': 254, 'delta': 255, 'kansas': 256, '845': 257, '2153': 258, 'plane': 259, 'jose': 260, '7': 261, 'tacoma': 262, 'symbols': 263, 'stand': 264, 'daily': 265, 'using': 266, 'could': 267, 'listings': 268, 'montreal': 269, 'canada': 270, 'trips': 271, 'ff': 272, 'twa': 273, 'paul': 274, '530': 275, 'around': 276, 'minneapolis': 277, 'y': 278, 'mean': 279, 'many': 280, 'december': 281, 'airfare': 282, 'louis': 283, 'arrives': 284, 'along': 285, 'less': 286, 'than': 287, '466': 288, 'dollars': 289, 'business': 290, 'cost': 291, 'detroit': 292, 'month': 293, 'h': 294, 'pennsylvania': 295, 'possible': 296, 'classes': 297, 'weekday': 298, '1115': 299, '1245': 300, 'friday': 301, 'saturdays': 302, 'number': 303, 'people': 304, 'carried': 305, 'each': 306, 'june': 307, 'eleventh': 308, 'sa': 309, 'ticket': 310, 'dl': 311, '296': 312, 'm': 313, 'columbus': 314, 'second': 315, 'take': 316, 'ten': 317, '11': 318, 'great': 319, 'too': 320, 'we': 321, \"'re\": 322, 'april': 323, 'nighttime': 324, 'departure': 325, 'it': 326, 'rent': 327, 'serve': 328, 'planes': 329, \"'m\": 330, 'interested': 331, 'early': 332, 'hello': 333, 'plan': 334, 'coach': 335, 'approximately': 336, '1': 337, 'layover': 338, 'eighteenth': 339, 'cincinnati': 340, 'should': 341, 'october': 342, 'fn': 343, 'explain': 344, 'restriction': 345, 'ap': 346, '1020': 347, 'carries': 348, 'smallest': 349, 'passengers': 350, 'november': 351, 'through': 352, '934': 353, 'september': 354, 'this': 355, 'prefer': 356, 'fourth': 357, 'equal': 358, 'goes': 359, 'difference': 360, 'q': 361, 'cities': 362, 'served': 363, 'canadian': 364, 'yes': 365, 'breakfast': 366, 'today': 367, 'town': 368, 'schedules': 369, 'an': 370, 'fifteenth': 371, '747': 372, '1222': 373, 'later': 374, '2': 375, 'use': 376, 'meal': 377, 's': 378, 'march': 379, 'meaning': 380, 'ap80': 381, 'serving': 382, '2134': 383, 'hp': 384, '80': 385, 'la': 386, 'as': 387, 'connecting': 388, 'stopping': 389, 'oh': 390, 'during': 391, 'week': 392, 'days': 393, 'offer': 394, 'dc10': 395, 'midwest': 396, 'express': 397, 'sixteenth': 398, 'jfk': 399, 'guardia': 400, 'arrange': 401, 'two': 402, 'friends': 403, 'coming': 404, 'eighth': 405, '1000': 406, '1145': 407, '430': 408, 'ninth': 409, 'another': 410, 'out': 411, 'soon': 412, 'thereafter': 413, 'where': 414, 'ea': 415, 'book': 416, 'reaches': 417, 'colorado': 418, '210': 419, 'options': 420, 'noontime': 421, 'originating': 422, 'order': 423, 'twelfth': 424, 'names': 425, 'itinerary': 426, 'departs': 427, 'sixth': 428, '1765': 429, 'anything': 430, 'non': 431, 'meals': 432, 'eight': 433, 'sixteen': 434, 'qx': 435, 'listing': 436, 'lunch': 437, 'nineteenth': 438, 'seating': 439, 'capacity': 440, 'various': 441, 'airplanes': 442, 'uses': 443, 'nw': 444, '1220': 445, 'return': 446, 'jersey': 447, 'ohio': 448, 'weekdays': 449, 'wednesdays': 450, '4400': 451, '1083': 452, 'fridays': 453, '9': 454, 'eleven': 455, 'quebec': 456, 'philly': 457, 'say': 458, 'mealtime': 459, 'then': 460, 'thank': 461, 'has': 462, '852': 463, 'when': 464, 'under': 465, 'following': 466, 'westchester': 467, 'county': 468, 'ewr': 469, 'north': 470, 'carolina': 471, '415': 472, 'again': 473, 'reverse': 474, 'dfw': 475, '1133': 476, '43': 477, 'tenth': 478, 'most': 479, 'anywhere': 480, 'stops': 481, 'twentieth': 482, 'doesn': 483, \"'t\": 484, 'provided': 485, '445': 486, '515': 487, 'same': 488, 'trying': 489, '650': 490, 'cheap': 491, 'southwest': 492, 'tuesdays': 493, 'provide': 494, '269': 495, '428': 496, 'arrival': 497, 'looking': 498, 'choices': 499, 'if': 500, '813': 501, 'enroute': 502, 'connect': 503, 'mia': 504, 'takes': 505, 'amount': 506, 'january': 507, 'dinnertime': 508, 'oak': 509, 'atl': 510, 'makes': 511, 'sd': 512, 'd': 513, 'stapleton': 514, 'define': 515, 'ua': 516, 'some': 517, 'making': 518, 'reservation': 519, 'seats': 520, '100': 521, 'includes': 522, 'hi': 523, 'sometime': 524, '201': 525, 'help': 526, 'repeat': 527, 'departures': 528, 'rates': 529, 'limo': 530, 'distance': 531, 'fourteenth': 532, 'total': 533, '270': 534, 'sounds': 535, 'sorry': 536, 'wanted': 537, 'ap57': 538, 'seventeenth': 539, 'route': 540, '1992': 541, 'landings': 542, 'tickets': 543, 'arrivals': 544, 'logan': 545, '106': 546, 'boeing': 547, '737': 548, 'shortest': 549, 'numbers': 550, 'minnesota': 551, 'fit': 552, '72s': 553, 'airplane': 554, 'qo': 555, 'capacities': 556, 'turboprop': 557, 'takeoffs': 558, '402': 559, 'tennessee': 560, 'inform': 561, 'cars': 562, '825': 563, '555': 564, '300': 565, 'wish': 566, 'discount': 567, '323': 568, '229': 569, 'dulles': 570, 'arizona': 571, 'within': 572, 'final': 573, 'yn': 574, 'sundays': 575, 'cp': 576, 'traveling': 577, 'they': 578, 'these': 579, '230': 580, 'more': 581, 'advertises': 582, 'having': 583, 'fine': 584, '55': 585, 'very': 586, '57': 587, 'both': 588, 'ac': 589, 'trans': 590, 'world': 591, '1030': 592, '1130': 593, 'georgia': 594, 'four': 595, '257': 596, '500': 597, 'airfares': 598, 'run': 599, '212': 600, 'everywhere': 601, 'connection': 602, 'those': 603, 'ls': 604, 'buy': 605, 'including': 606, 'connections': 607, '767': 608, 'gets': 609, 'nationair': 610, 'mornings': 611, '1100': 612, 'stands': 613, 'lufthansa': 614, 'listed': 615, '1055': 616, '405': 617, 'greatest': 618, 'maximum': 619, 'well': 620, \"'ll\": 621, 'try': 622, 'michigan': 623, 'different': 624, '400': 625, '539': 626, 'still': 627, 'being': 628, 'serviced': 629, 'rentals': 630, '345': 631, '720': 632, 'f28': 633, 'name': 634, '1600': 635, 'hours': 636, 'come': 637, 'located': 638, 'date': 639, 'start': 640, '645': 641, 'abbreviation': 642, '2100': 643, 'iah': 644, 'preferably': 645, 'february': 646, 'area': 647, 'but': 648, 'taxi': 649, 'reservations': 650, 'over': 651, '1993': 652, 'kinds': 653, 'midway': 654, 'indiana': 655, 'three': 656, 'j31': 657, 'midnight': 658, '21': 659, '757': 660, 'co': 661, '281': 662, 'spend': 663, 'lastest': 664, 'land': 665, '217': 666, '815': 667, 'currently': 668, 'sort': 669, 'so': 670, 'priced': 671, '718': 672, '1300': 673, '1700': 674, 'proper': 675, 'tower': 676, 'b': 677, 'economic': 678, 'longest': 679, 'offers': 680, 'd10': 681, 'describe': 682, 'seven': 683, '271': 684, 'america': 685, 'west': 686, 'charges': 687, 'bay': 688, '98': 689, '1940': 690, '723': 691, 'o': 692, \"'hare\": 693, 'zone': 694, '82': 695, '139': 696, 'thanks': 697, 'd9s': 698, 'bound': 699, 'costs': 700, 'limousines': 701, 'services': 702, 'bur': 703, 'include': 704, '1205': 705, 'hold': 706, '3357': 707, 'runs': 708, '329': 709, 'ord': 710, 'regarding': 711, 'booking': 712, '343': 713, 'm80': 714, 'repeating': 715, 'closest': 716, 'companies': 717, '71': 718, 'visit': 719, 'here': 720, 'them': 721, 'lives': 722, 'continuing': 723, 'near': 724, '324': 725, 'just': 726, 'highest': 727, 'six': 728, '1039': 729, 'nonstops': 730, 'hou': 731, 'close': 732, 'travels': 733, '746': 734, 'afternoons': 735, 'laying': 736, '1291': 737, 'lester': 738, 'pearson': 739, '1500': 740, '1059': 741, '3724': 742, 'aa': 743, '459': 744, '819': 745, 'represented': 746, 'database': 747, '150': 748, 'afterwards': 749, 'while': 750, '73s': 751, 'sure': 752, 'planning': 753, '305': 754, 'designate': 755, 'jet': 756, 'heading': 757, 'restrictions': 758, '416': 759, 'actually': 760, 'once': 761, 'snack': 762, 'seat': 763, 'question': 764, '1045': 765, 'bring': 766, 'up': 767, 'tonight': 768, 'twelve': 769, 'belong': 770, 'seventeen': 771, 'working': 772, 'scenario': 773, '727': 774, '297': 775, 'rate': 776, 'thursdays': 777, 'c': 778, 'hopefully': 779, 'thirteenth': 780, 'grounds': 781, 'such': 782, 'connects': 783, '352': 784, '124': 785, '730': 786, '19': 787, 'thing': 788, '110': 789, '315': 790, 'alaska': 791, 'staying': 792, 'straight': 793, 'without': 794, '615': 795, 'operation': 796, 'provides': 797, 'usa': 798, 'year': 799, 'lax': 800, 'begins': 801, 'lands': 802, '225': 803, '1158': 804, 'equipment': 805, 'minimum': 806, 'intercontinental': 807, 'transcontinental': 808, \"'ve\": 809, 'got': 810, 'somebody': 811, 'else': 812, 'who': 813, 'wants': 814, '505': 815, '163': 816, 'cover': 817, 'kindly': 818, 'missouri': 819, 'utah': 820, 'taking': 821, '311': 822, 'must': 823, '932': 824, '1505': 825, 'home': 826, 'directly': 827, '130': 828, 'nights': 829, '771': 830, '420': 831, 'qualify': 832, '823': 833, 'reaching': 834, 'landing': 835, 'able': 836, 'put': 837, '755': 838, '0900': 839, 'earlier': 840, '1209': 841, 'red': 842, 'eye': 843, 'calling': 844, '734': 845, 'supper': 846, '1850': 847, 'toward': 848, 'single': 849, 'vicinity': 850, '497766': 851, 'nevada': 852, 'off': 853, '733': 854, 'largest': 855, 'beginning': 856, 'across': 857, 'continent': 858, 'concerning': 859, 'scheduled': 860, 'summer': 861, '1017': 862, 'inexpensive': 863, 'yyz': 864, 'bna': 865, 'don': 866, '1230': 867, 'operating': 868, 'catch': 869, '810': 870, 'whether': 871, 'offered': 872, '705': 873, '417': 874, '200': 875, '928': 876, 'k': 877, '279': 878, '137338': 879, 'mondays': 880, 'ap68': 881, 'locate': 882, 'hartfield': 883, '1288': 884, 'fifteen': 885, 'instead': 886, 'determine': 887, 'place': 888, '1200': 889, '811': 890, '665': 891, '673': 892, 'comes': 893, '608': 894, '1207': 895, '639': 896, '320': 897, 'dh8': 898, 'sb': 899, '468': 900, 'al': 901, '950': 902, '486': 903, 'india': 904, 'included': 905, 'miles': 906, 'l10': 907, 'be1': 908, 'bn': 909, 'wn': 910, '1994': 911, '382': 912, '20': 913, 'phl': 914, 'mci': 915, 'not': 916, 'l1011': 917, 'dc9': 918, 'dca': 919, '1800': 920, 'good': 921, 'dtw': 922, 'prior': 923, 'tpa': 924, 'kw': 925, 'bh': 926, 'yx': 927, 'ap58': 928, 'sam': 929, 'basis': 930, 'combination': 931, '1201': 932, 'kennedy': 933, 'ever': 934, 'snacks': 935, 'called': 936, '842': 937, 'cvg': 938, 'lga': 939, '419': 940, 'exceeding': 941}\n"
     ]
    }
   ],
   "source": [
    "print(w2id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T23:42:27.531222200Z",
     "start_time": "2023-06-14T23:42:27.446223700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "c9be408b",
   "metadata": {},
   "source": [
    "### Lang class\n",
    "Later we will need to convert those numbers in the original form, so we need to invert those dictionaries. We create a calss named as Lang just for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c04f608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:42:52.891457700Z",
     "start_time": "2023-06-14T23:42:52.859308800Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77bc3bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:43:18.448397700Z",
     "start_time": "2023-06-14T23:43:14.913394400Z"
    }
   },
   "outputs": [],
   "source": [
    "words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw # We do not wat unk labels, however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "lang = Lang(words, intents, slots, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35602d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ac746e",
   "metadata": {},
   "source": [
    "### Customize the Dataset class\n",
    "In Pytorch the Dataset class helps you in handleing the dataset. The mandatory methods are ```__init__, __len__ and __getitem__```. <br>\n",
    "You can find more details here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b4823f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:43:45.655770100Z",
     "start_time": "2023-06-14T23:43:29.082161600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class IntentsAndSlots(data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845ab541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:44:02.235925100Z",
     "start_time": "2023-06-14T23:44:02.164919400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create our datasets\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T23:45:31.155649300Z",
     "start_time": "2023-06-14T23:45:31.133650400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'utterance': tensor([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.]),\n 'slots': tensor([ 25.,  25.,  25.,  25.,  25.,  94.,  25.,  25.,  20.,  25.,  72.,  49.,\n         104.,  90.]),\n 'intent': 22}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T23:45:32.375366300Z",
     "start_time": "2023-06-14T23:45:32.290368100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d297a312",
   "metadata": {},
   "source": [
    "## Batches\n",
    "Batches are used to handle large datasets in the memory. Since the whole dataset cannot fit in GPU memories, we rashuffle the dataset and we split it in small batches that will be processed one at a time.\n",
    "### Padding\n",
    "Padding is a strategy to fit sequences of different lengths into a matrix. For instance:\n",
    "\n",
    "| Right padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| book | me | a | flight | [pad] | [pad] | [pad] | \n",
    "\n",
    "| Left padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| [pad] | [pad] | [pad] | book | me | a | flight | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eb4c4",
   "metadata": {},
   "source": [
    "**Exercise** *(10 minuts)* <br> \n",
    "Write a function that adds padding on the right. (No need to convert the sentences to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bb2fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T00:59:38.603534200Z",
     "start_time": "2023-06-12T00:59:38.593531100Z"
    }
   },
   "outputs": [],
   "source": [
    "# split them by white space\n",
    "sequences = ['I saw a man with a telescope', \n",
    "             'book me a flight', \n",
    "             'I want to see the flights from Milan to Ibiza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# add padding\n",
    "def right_padding(sequences):\n",
    "    padded_sequences = []\n",
    "\n",
    "    max_len = max([len(seq.split()) for seq in sequences])\n",
    "\n",
    "    for seq in sequences:\n",
    "        # add right padding to each sequence to make them all the same length\n",
    "        seq = seq.split() + ['[pad]'] * (max_len - len(seq.split()))\n",
    "\n",
    "        padded_sequences.append(seq)\n",
    "\n",
    "    # Your code here\n",
    "    return padded_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T01:01:43.009927900Z",
     "start_time": "2023-06-12T01:01:42.994891600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[['I', 'saw', 'a', 'man', 'with', 'a', 'telescope', '[pad]', '[pad]', '[pad]'],\n ['book',\n  'me',\n  'a',\n  'flight',\n  '[pad]',\n  '[pad]',\n  '[pad]',\n  '[pad]',\n  '[pad]',\n  '[pad]'],\n ['I', 'want', 'to', 'see', 'the', 'flights', 'from', 'Milan', 'to', 'Ibiza']]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_padding(sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T01:02:28.581748400Z",
     "start_time": "2023-06-12T01:02:28.521749900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "e24b71d2",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "To split the dataset into batches and add padding we will use the DataLoader class. \n",
    "```python\n",
    "DataLoader(Dataset, batch_size=N, collate_fn={custom function}, shuffle=True)\n",
    "```\n",
    "*collate_fn* is used to shape the batch in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7105180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T23:45:55.515589300Z",
     "start_time": "2023-06-14T23:45:55.472548700Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences), max_len).fill_(PAD_TOKEN)\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "\n",
    "        return padded_seqs, lengths\n",
    "\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True)\n",
    "\n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our selected device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "\n",
    "    return new_item\n",
    "\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019e479",
   "metadata": {},
   "source": [
    "## Define a neural network in Pytorch\n",
    "In Pythorch the difinition of a neural network is quite flexible. In ```__init__``` the layer that is going to be used are instantiated. In ```forward```, the achitecture of the neural network is defined. Here you can find all the layers provided by Pytorch https://pytorch.org/docs/stable/nn.html while here you can find the recurrent layers https://pytorch.org/docs/stable/nn.html#recurrent-layers. \n",
    "\n",
    "<br><br>\n",
    "**pack_padded_sequence** and **pad_packed_sequences** are to compress and uncompress sequences in order to remove from the computation the padding embeddings. This reduces the computational cost which means speeding up training and therefore reducing CO2 emission.\n",
    " ![](https://i.stack.imgur.com/LPHAs.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'utterance': tensor([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.]),\n 'slots': tensor([ 25.,  25.,  25.,  25.,  25.,  94.,  25.,  25.,  20.,  25.,  72.,  49.,\n         104.,  90.]),\n 'intent': 22}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T23:46:08.864571700Z",
     "start_time": "2023-06-14T23:46:08.819574500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93adc878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:56.684312600Z",
     "start_time": "2023-06-12T01:59:56.677323Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False)    \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        # Dropout layer How do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: seq_len, batch size, calsses \n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992a22c",
   "metadata": {},
   "source": [
    "### Function to randomly initialize the weights\n",
    "This is a generic function that randomly initialize the parameters of RNN networks and linear layers. To dig deep in to this I would suggest you to look at here: https://pytorch.org/docs/master/nn.init.html \\\n",
    "\\\n",
    "*Note: In Pytorch every parameter of the network has a proper name like weight_ih, weight_hh etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f47fe3fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:00:00.184584500Z",
     "start_time": "2023-06-12T02:00:00.173584500Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a362fc",
   "metadata": {},
   "source": [
    "## Training set up\n",
    "Here we initialize the model and we select the hyperparamters of the neural network. Futhermore, we initialize the optimizer and we select the loss function.\\\n",
    "- You can find further optimization algorithms here: https://pytorch.org/docs/stable/optim.html\n",
    "- and further loss functions here: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5edf6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:00:03.052591800Z",
     "start_time": "2023-06-12T02:00:02.401526800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a18f4",
   "metadata": {},
   "source": [
    "### Train Loop and Evaluation Loop\n",
    "We define two functions one for training our model and the other for evaluating it. To compute the performances on the slot filling task we will use the **conll script**, while for the intent classification task we are going to use the **classification_report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bf6dfca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:00:05.708519100Z",
     "start_time": "2023-06-12T02:00:05.687522100Z"
    }
   },
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_loop(data, optimizer, criterion_slots, critenrion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2eaee",
   "metadata": {},
   "source": [
    "## Train a neural network\n",
    "We train a neural network iterating several times over the training set. \n",
    "* **epochs**: number of times in which the whole training set is seen by the network\n",
    "* **early stopping**: keeps controlled the performance of the model on the dev set and interrupts the training when the performance is getting worse\n",
    "    * **patience**: wait for a number of step before interrupting the training, even though the performance is getting worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d07777d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:04:39.277136800Z",
     "start_time": "2023-06-12T02:00:08.091166600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [04:30<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1:  0.9235635213494993\n",
      "Intent Accuracy: 0.9372900335946248\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model)\n",
    "    if x % 5 == 0:\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        f1 = results_dev['total']['f']\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1466a",
   "metadata": {},
   "source": [
    "### Plot of the train and valid losses\n",
    "One of the techniques for debugging a neural network is to check the plot of the loss. If the loss goes smoothly down then the network works corretly, otherwise a deeper analysis is needed. Furthermore, this plot can be useful for deciding the learning rate and the optimizer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1211aab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:07:24.707756Z",
     "start_time": "2023-06-12T02:07:23.122668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
    "plt.title('Train and Dev Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(sampled_epochs, losses_train, label='Train loss')\n",
    "plt.plot(sampled_epochs, losses_dev, label='Dev loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df73c",
   "metadata": {},
   "source": [
    "### Multiple runs\n",
    "To have reliable results on small corpora we have to train and test the model from scratch for several times. At the end, we average the results and we compute the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f08d0445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:29:01.629597Z",
     "start_time": "2023-06-12T02:07:33.460222100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [21:28<00:00, 257.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1 0.923 +- 0.002\n",
      "Intent Acc 0.936 +- 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "    n_epochs = 200\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4565d",
   "metadata": {},
   "source": [
    " ![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\n",
    "# Hugging Face\n",
    "Hugging Face is a library that allows you to used large pretrained models in an easy way. This means that you do not need to implement an architeture and train it from scratch. Hugging Face is also a community where people share trained models and code.\n",
    "<br/><br/>\n",
    "In Hugging Face there are many different models (https://huggingface.co/models) that you can import and each of them has its own input and output shapes. However, Transformer-based models are usually composed of two parts: \n",
    "- **Tokenizer**\n",
    "- **Architecture/Pretrained model**\n",
    "\n",
    "The **tokenizers** used by Transformer-based models are different from those we saw in the lab. While for instance Spacy's tokenizer is rule-based and splits the text looking at the punctuation, the goal of Transformer tokenizers is to reduce the vocabulary length by spliting words into subwords. To do this, several algorithms have been proposed. If you are interested in this topic you can find a thoroughly explanation here: https://huggingface.co/docs/transformers/tokenizer_summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b01cfd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T02:39:02.444325800Z",
     "start_time": "2023-06-12T02:30:36.362346100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af832f450e984ff8a68a0ae848447282"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\adnan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db01cc4a26d349f1b4fe45a2aa911999"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbdd23d9a89a41cfaf96417073a3bec5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c8bccd68154a968a54c20845cd42e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# BERT model script from: huggingface.co\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"I saw a man with a telescope\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2387,  1037,  2158,  2007,  1037, 12772,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T02:45:29.243542900Z",
     "start_time": "2023-06-12T02:45:29.200539300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8e2fad3c",
   "metadata": {},
   "source": [
    "# Exercise (2 points)\n",
    "Modify the baseline architecture Model IAS in an addition way:\n",
    "- Add bidirectionality\n",
    "- Add dropout layer\n",
    "\n",
    "***Dataset to use: ATIS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce570a7f-ce03-4940-9c46-195fe6dd5b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T04:04:14.696285100Z",
     "start_time": "2023-06-12T03:58:35.388208400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.3,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.7000000000000002,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.3,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.3,\n",
      " 'distance': 0.3,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.5,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.3,\n",
      " 'ground_service': 5.2,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4381\n",
      "DEV size: 597\n",
      "TEST size: 893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 154/199 [05:37<01:38,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1:  0.9350372736954207\n",
      "Intent Accuracy: 0.9540873460246361\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Global variables\n",
    "device = 'cuda:0'  # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "\n",
    "# Lang class to create vocabularies for words, intents and slots and convert them to ids (Vectors)\n",
    "class Lang:\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v: k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v: k for k, v in self.intent2id.items()}\n",
    "\n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "            vocab[elem] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "\n",
    "# Customize the Dataset class to load the data and convert it to ids using the Lang class\n",
    "class IntentsAndSlots(data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "\n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "\n",
    "    # Auxiliary methods\n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "\n",
    "    def mapping_seq(self, data, mapper):  # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "\n",
    "\n",
    "# Model Class: Defines a neural network in Pytorch\n",
    "class ModelIAS(nn.Module):\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "\n",
    "        # LSTM layer with bidirectionality\n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=True)\n",
    "        # Output Linear layers for slot filling and intent classification\n",
    "        # hid_size * 2 because of bidirectionality (concatenation of the forward and backward hidden states)\n",
    "        self.slot_out = nn.Linear(hid_size * 2, out_slot)  # bidirectional = True -> hid_size * 2\n",
    "        self.intent_out = nn.Linear(hid_size * 2, out_int)  # bidirectional = True -> hid_size * 2\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance)  # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1, 0, 2)  # we need seq len first -> seq_len X batch_size X emb_size\n",
    "\n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "\n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input)\n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "\n",
    "        # Apply dropout to the output of the LSTM layer\n",
    "        # Since LSTM layer is bidirectional, we need to apply dropout to both directions (forward and backward)\n",
    "        # For this reason we need to split the tensor in two parts and apply dropout to each part\n",
    "        #  OR we need to apply dropout to the concatenation of the two parts: forward and backward hidden states\n",
    "        # Concatenate the final forward (last_hidden[-2,:,:]) and backward (last_hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        last_hidden = self.dropout(torch.cat((last_hidden[-2, :, :], last_hidden[-1, :, :]), dim=1))\n",
    "        utt_encoded = self.dropout(utt_encoded)\n",
    "\n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        # Slot size: seq_len, batch size, calsses\n",
    "        slots = slots.permute(1, 2, 0)  # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent\n",
    "\n",
    "\n",
    "# Function to load the data from the json file\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "        input: path/to/data\n",
    "        output: json\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Collate_fn function which is passed to the DataLoader class to create batches\n",
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len\n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths) == 0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences), max_len).fill_(PAD_TOKEN)\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq  # We copy each sequence into the matrix\n",
    "\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "\n",
    "        return padded_seqs, lengths\n",
    "\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True)\n",
    "\n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "\n",
    "    src_utt = src_utt.to(device)  # We load the Tensor on our selected device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "\n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "\n",
    "    return new_item\n",
    "\n",
    "\n",
    "# Function to randomly initialize the weights of a neural network in Pytorch based on the type of layer\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0] // 4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx * mul:(idx + 1) * mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0] // 4\n",
    "                        torch.nn.init.orthogonal_(param[idx * mul:(idx + 1) * mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# Train Loop and Evaluation Loop Functions\n",
    "# We define two functions one for training our model and the other for evaluating it.To compute the performances on the\n",
    "# slot filling task we will use the ** conll script **, while for the intent classification task we are going to use\n",
    "# the ** classification_report **.\n",
    "def train_loop(data, optimizer, criterion_slots, critenrion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad()  # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot  # In joint training we sum the losses.\n",
    "        # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward()  # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()  # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "\n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "\n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad():  # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot\n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x]\n",
    "                           for x in torch.argmax(intents, dim=1).tolist()]\n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "\n",
    "            # Slot inference\n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "\n",
    "    report_intent = classification_report(ref_intents, hyp_intents,\n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n",
    "\n",
    "\n",
    "# 1. Load data, split into train and test set\n",
    "tmp_train_raw = load_data(os.path.join('dataset', 'ATIS', 'train.json'))\n",
    "test_raw = load_data(os.path.join('dataset', 'ATIS', 'test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "# 2. Create a dev set\n",
    "# First we get the 10% of dataset, then compute the percentage of these examples on the training set which is around 11%\n",
    "portion = round(((len(tmp_train_raw) + len(test_raw)) * 0.10) / (len(tmp_train_raw)), 2)\n",
    "intents = [x['intent'] for x in tmp_train_raw]  # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "# 3. Split the training set into train and dev set with stratification (same distribution of intents)\n",
    "Y = []\n",
    "X = []\n",
    "mini_Train = []\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1:  # If some intents occur once only, we put them in training\n",
    "        X.append(tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, Y, test_size=portion,\n",
    "                                                  random_state=42,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=Y)\n",
    "X_train.extend(mini_Train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k: round(v / len(y_train), 3) * 100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'),\n",
    "pprint({k: round(v / len(y_dev), 3) * 100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:')\n",
    "pprint({k: round(v / len(y_test), 3) * 100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('=' * 89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))\n",
    "\n",
    "# 4. Convert words to numbers (word2id)\n",
    "words = sum([x['utterance'].split() for x in train_raw], [])  # No set() since we want to compute the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw  # We do not want unk labels, # however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus], []))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "# 5. Create the lang object that will be used to convert words to numbers\n",
    "lang = Lang(words, intents, slots, cutoff=0)\n",
    "\n",
    "\n",
    "# 6. Create the datasets and dataloaders for train, dev and test set\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "# 7. Training set up\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001  # learning rate\n",
    "clip = 5  # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss()  # Because we do not have the pad token\n",
    "\n",
    "# 8. Train a neural network model for intent detection and slot filling (with early stopping)\n",
    "n_epochs = 200\n",
    "patience = 3  # Early stopping patience\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "for x in tqdm(range(1, n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, criterion_intents, model)\n",
    "    if x % 5 == 0:\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        f1 = results_dev['total']['f']\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0:  # Early stopping with patience\n",
    "            break  # Not nice but it keeps the code clean\n",
    "\n",
    "\n",
    "# 9. Evaluate the model on the test set\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)\n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])\n",
    "\n",
    "# 10. Plot of the train and valid losses during training (one plot for the train and one for the dev)\n",
    "plt.figure(num=3, figsize=(8, 5)).patch.set_facecolor('white')\n",
    "plt.title('Train and Dev Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(sampled_epochs, losses_train, label='Train loss')\n",
    "plt.plot(sampled_epochs, losses_dev, label='Dev loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb2ce-9952-4127-b258-3d372f452c2b",
   "metadata": {},
   "source": [
    "# Exercise (4 points)\n",
    "Train a BERT model using a multi-task learning setting on intent classification and slot filling. \n",
    "<br>\n",
    "You can refer to this paper to have a better understanding of such model: https://arxiv.org/abs/1902.10909\n",
    "\n",
    "***Dataset to use: ATIS***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.3,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.7000000000000002,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.3,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.3,\n",
      " 'distance': 0.3,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.5,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.3,\n",
      " 'ground_service': 5.2,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4381\n",
      "DEV size: 597\n",
      "TEST size: 893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForIntentClassificationAndSlotFilling: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForIntentClassificationAndSlotFilling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForIntentClassificationAndSlotFilling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForIntentClassificationAndSlotFilling were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['intent_classifier.bias', 'slot_classifier.bias', 'slot_classifier.weight', 'intent_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/137 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|          | 1/137 [00:00<00:57,  2.38it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 2/137 [00:00<00:49,  2.71it/s]\u001B[A\n",
      "Iteration:   2%|▏         | 3/137 [00:01<00:56,  2.39it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 4/137 [00:01<00:52,  2.56it/s]\u001B[A\n",
      "Iteration:   4%|▎         | 5/137 [00:01<00:49,  2.66it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 6/137 [00:02<00:52,  2.49it/s]\u001B[A\n",
      "Iteration:   5%|▌         | 7/137 [00:02<00:48,  2.67it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 8/137 [00:03<00:46,  2.77it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 9/137 [00:03<00:49,  2.59it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 10/137 [00:03<00:46,  2.72it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 11/137 [00:04<00:44,  2.80it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 12/137 [00:04<00:48,  2.60it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 13/137 [00:04<00:45,  2.74it/s]\u001B[A\n",
      "Iteration:  10%|█         | 14/137 [00:05<00:43,  2.82it/s]\u001B[A\n",
      "Iteration:  11%|█         | 15/137 [00:05<00:46,  2.62it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 16/137 [00:06<00:44,  2.71it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 17/137 [00:06<00:43,  2.78it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 18/137 [00:06<00:46,  2.58it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 19/137 [00:07<00:43,  2.71it/s]\u001B[A\n",
      "Iteration:  15%|█▍        | 20/137 [00:07<00:42,  2.78it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 21/137 [00:07<00:44,  2.61it/s]\u001B[A\n",
      "Iteration:  16%|█▌        | 22/137 [00:08<00:41,  2.74it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 23/137 [00:08<00:40,  2.81it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 24/137 [00:09<00:43,  2.62it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 25/137 [00:09<00:40,  2.73it/s]\u001B[A\n",
      "Iteration:  19%|█▉        | 26/137 [00:09<00:39,  2.80it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 27/137 [00:10<00:41,  2.63it/s]\u001B[A\n",
      "Iteration:  20%|██        | 28/137 [00:10<00:39,  2.75it/s]\u001B[A\n",
      "Iteration:  21%|██        | 29/137 [00:10<00:38,  2.82it/s]\u001B[A\n",
      "Iteration:  22%|██▏       | 30/137 [00:11<00:41,  2.60it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 31/137 [00:11<00:38,  2.73it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 32/137 [00:11<00:37,  2.80it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 33/137 [00:12<00:40,  2.60it/s]\u001B[A\n",
      "Iteration:  25%|██▍       | 34/137 [00:12<00:37,  2.74it/s]\u001B[A\n",
      "Iteration:  26%|██▌       | 35/137 [00:12<00:36,  2.82it/s]\u001B[A\n",
      "Iteration:  26%|██▋       | 36/137 [00:13<00:38,  2.63it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 37/137 [00:13<00:36,  2.76it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 38/137 [00:14<00:35,  2.82it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 39/137 [00:14<00:37,  2.61it/s]\u001B[A\n",
      "Iteration:  29%|██▉       | 40/137 [00:14<00:35,  2.75it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 41/137 [00:15<00:33,  2.83it/s]\u001B[A\n",
      "Iteration:  31%|███       | 42/137 [00:15<00:35,  2.64it/s]\u001B[A\n",
      "Iteration:  31%|███▏      | 43/137 [00:15<00:34,  2.76it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 44/137 [00:16<00:32,  2.82it/s]\u001B[A\n",
      "Iteration:  33%|███▎      | 45/137 [00:16<00:34,  2.63it/s]\u001B[A\n",
      "Iteration:  34%|███▎      | 46/137 [00:17<00:32,  2.76it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 47/137 [00:17<00:31,  2.82it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 48/137 [00:17<00:33,  2.64it/s]\u001B[A\n",
      "Iteration:  36%|███▌      | 49/137 [00:18<00:31,  2.77it/s]\u001B[A\n",
      "Iteration:  36%|███▋      | 50/137 [00:18<00:30,  2.82it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 51/137 [00:18<00:32,  2.63it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 52/137 [00:19<00:30,  2.75it/s]\u001B[A\n",
      "Iteration:  39%|███▊      | 53/137 [00:19<00:29,  2.81it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 54/137 [00:19<00:31,  2.63it/s]\u001B[A\n",
      "Iteration:  40%|████      | 55/137 [00:20<00:29,  2.76it/s]\u001B[A\n",
      "Iteration:  41%|████      | 56/137 [00:20<00:28,  2.82it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 57/137 [00:21<00:30,  2.63it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 58/137 [00:21<00:28,  2.74it/s]\u001B[A\n",
      "Iteration:  43%|████▎     | 59/137 [00:21<00:27,  2.81it/s]\u001B[A\n",
      "Iteration:  44%|████▍     | 60/137 [00:22<00:29,  2.59it/s]\u001B[A\n",
      "Iteration:  45%|████▍     | 61/137 [00:22<00:27,  2.73it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 62/137 [00:22<00:26,  2.79it/s]\u001B[A\n",
      "Iteration:  46%|████▌     | 63/137 [00:23<00:28,  2.60it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 64/137 [00:23<00:26,  2.72it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 65/137 [00:23<00:25,  2.79it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 66/137 [00:24<00:27,  2.61it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 67/137 [00:24<00:25,  2.74it/s]\u001B[A\n",
      "Iteration:  50%|████▉     | 68/137 [00:25<00:24,  2.79it/s]\u001B[A\n",
      "Iteration:  50%|█████     | 69/137 [00:25<00:26,  2.60it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 70/137 [00:25<00:24,  2.73it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 71/137 [00:26<00:23,  2.79it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 72/137 [00:26<00:25,  2.58it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 73/137 [00:26<00:23,  2.73it/s]\u001B[A\n",
      "Iteration:  54%|█████▍    | 74/137 [00:27<00:22,  2.78it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 75/137 [00:27<00:23,  2.59it/s]\u001B[A\n",
      "Iteration:  55%|█████▌    | 76/137 [00:28<00:22,  2.74it/s]\u001B[A\n",
      "Iteration:  56%|█████▌    | 77/137 [00:28<00:21,  2.80it/s]\u001B[A\n",
      "Iteration:  57%|█████▋    | 78/137 [00:28<00:22,  2.59it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 79/137 [00:29<00:21,  2.73it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 80/137 [00:29<00:20,  2.80it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 81/137 [00:29<00:21,  2.62it/s]\u001B[A\n",
      "Iteration:  60%|█████▉    | 82/137 [00:30<00:20,  2.74it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 83/137 [00:30<00:19,  2.79it/s]\u001B[A\n",
      "Iteration:  61%|██████▏   | 84/137 [00:31<00:20,  2.62it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 85/137 [00:31<00:18,  2.74it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 86/137 [00:31<00:18,  2.80it/s]\u001B[A\n",
      "Iteration:  64%|██████▎   | 87/137 [00:32<00:19,  2.62it/s]\u001B[A\n",
      "Iteration:  64%|██████▍   | 88/137 [00:32<00:17,  2.74it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 89/137 [00:32<00:17,  2.81it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 90/137 [00:33<00:18,  2.60it/s]\u001B[A\n",
      "Iteration:  66%|██████▋   | 91/137 [00:33<00:16,  2.72it/s]\u001B[A\n",
      "Iteration:  67%|██████▋   | 92/137 [00:33<00:16,  2.79it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 93/137 [00:34<00:16,  2.60it/s]\u001B[A\n",
      "Iteration:  69%|██████▊   | 94/137 [00:34<00:15,  2.73it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 95/137 [00:35<00:15,  2.78it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 96/137 [00:35<00:16,  2.56it/s]\u001B[A\n",
      "Iteration:  71%|███████   | 97/137 [00:35<00:14,  2.68it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 98/137 [00:36<00:14,  2.76it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 99/137 [00:36<00:14,  2.57it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 100/137 [00:36<00:13,  2.71it/s]\u001B[A\n",
      "Iteration:  74%|███████▎  | 101/137 [00:37<00:12,  2.78it/s]\u001B[A\n",
      "Iteration:  74%|███████▍  | 102/137 [00:37<00:13,  2.60it/s]\u001B[A\n",
      "Iteration:  75%|███████▌  | 103/137 [00:38<00:12,  2.72it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 104/137 [00:38<00:11,  2.79it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 105/137 [00:38<00:12,  2.59it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 106/137 [00:39<00:11,  2.71it/s]\u001B[A\n",
      "Iteration:  78%|███████▊  | 107/137 [00:39<00:10,  2.79it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 108/137 [00:39<00:11,  2.57it/s]\u001B[A\n",
      "Iteration:  80%|███████▉  | 109/137 [00:40<00:10,  2.71it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 110/137 [00:40<00:09,  2.77it/s]\u001B[A\n",
      "Iteration:  81%|████████  | 111/137 [00:41<00:10,  2.59it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 112/137 [00:41<00:09,  2.72it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 113/137 [00:41<00:08,  2.78it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 114/137 [00:42<00:08,  2.58it/s]\u001B[A\n",
      "Iteration:  84%|████████▍ | 115/137 [00:42<00:08,  2.70it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 116/137 [00:42<00:07,  2.76it/s]\u001B[A\n",
      "Iteration:  85%|████████▌ | 117/137 [00:43<00:07,  2.56it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 118/137 [00:43<00:07,  2.68it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 119/137 [00:44<00:06,  2.75it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 120/137 [00:44<00:06,  2.58it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 121/137 [00:44<00:05,  2.70it/s]\u001B[A\n",
      "Iteration:  89%|████████▉ | 122/137 [00:45<00:05,  2.77it/s]\u001B[A\n",
      "Iteration:  90%|████████▉ | 123/137 [00:45<00:05,  2.59it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 124/137 [00:45<00:04,  2.72it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 125/137 [00:46<00:04,  2.78it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 126/137 [00:46<00:04,  2.55it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 127/137 [00:47<00:03,  2.67it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 128/137 [00:47<00:03,  2.76it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 129/137 [00:47<00:03,  2.57it/s]\u001B[A\n",
      "Iteration:  95%|█████████▍| 130/137 [00:48<00:02,  2.70it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 131/137 [00:48<00:02,  2.76it/s]\u001B[A\n",
      "Iteration:  96%|█████████▋| 132/137 [00:48<00:01,  2.56it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 133/137 [00:49<00:01,  2.68it/s]\u001B[A\n",
      "Iteration:  98%|█████████▊| 134/137 [00:49<00:01,  2.75it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 135/137 [00:50<00:00,  2.57it/s]\u001B[A\n",
      "Iteration:  99%|█████████▉| 136/137 [00:50<00:00,  2.69it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 137/137 [00:50<00:00,  2.70it/s]\u001B[A\n",
      "Epoch:  20%|██        | 1/5 [00:50<03:23, 50.78s/it]\n",
      "Iteration:   0%|          | 0/137 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|          | 1/137 [00:00<00:48,  2.82it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 2/137 [00:00<00:47,  2.82it/s]\u001B[A\n",
      "Iteration:   2%|▏         | 3/137 [00:01<00:54,  2.47it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 4/137 [00:01<00:49,  2.67it/s]\u001B[A\n",
      "Iteration:   4%|▎         | 5/137 [00:01<00:48,  2.74it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 6/137 [00:02<00:52,  2.51it/s]\u001B[A\n",
      "Iteration:   5%|▌         | 7/137 [00:02<00:49,  2.62it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 8/137 [00:03<00:47,  2.69it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 9/137 [00:03<00:50,  2.51it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 10/137 [00:03<00:47,  2.65it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 11/137 [00:04<00:47,  2.68it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 12/137 [00:04<00:50,  2.49it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 13/137 [00:04<00:47,  2.61it/s]\u001B[A\n",
      "Iteration:  10%|█         | 14/137 [00:05<00:46,  2.67it/s]\u001B[A\n",
      "Iteration:  11%|█         | 15/137 [00:05<00:48,  2.51it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 16/137 [00:06<00:46,  2.62it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 17/137 [00:06<00:44,  2.68it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 18/137 [00:06<00:47,  2.50it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 19/137 [00:07<00:44,  2.63it/s]\u001B[A\n",
      "Iteration:  15%|█▍        | 20/137 [00:07<00:43,  2.70it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 21/137 [00:08<00:46,  2.50it/s]\u001B[A\n",
      "Iteration:  16%|█▌        | 22/137 [00:08<00:43,  2.63it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 23/137 [00:08<00:42,  2.67it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 24/137 [00:09<00:45,  2.48it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 25/137 [00:09<00:43,  2.59it/s]\u001B[A\n",
      "Iteration:  19%|█▉        | 26/137 [00:09<00:41,  2.67it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 27/137 [00:10<00:44,  2.48it/s]\u001B[A\n",
      "Iteration:  20%|██        | 28/137 [00:10<00:41,  2.62it/s]\u001B[A\n",
      "Iteration:  21%|██        | 29/137 [00:11<00:40,  2.64it/s]\u001B[A\n",
      "Iteration:  22%|██▏       | 30/137 [00:11<00:43,  2.48it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 31/137 [00:11<00:40,  2.61it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 32/137 [00:12<00:39,  2.66it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 33/137 [00:12<00:41,  2.50it/s]\u001B[A\n",
      "Iteration:  25%|██▍       | 34/137 [00:13<00:40,  2.57it/s]\u001B[A\n",
      "Iteration:  26%|██▌       | 35/137 [00:13<00:39,  2.59it/s]\u001B[A\n",
      "Iteration:  26%|██▋       | 36/137 [00:13<00:42,  2.40it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 37/137 [00:14<00:40,  2.50it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 38/137 [00:14<00:38,  2.57it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 39/137 [00:15<00:40,  2.43it/s]\u001B[A\n",
      "Iteration:  29%|██▉       | 40/137 [00:15<00:41,  2.33it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 41/137 [00:16<00:46,  2.05it/s]\u001B[A\n",
      "Iteration:  31%|███       | 42/137 [00:16<00:48,  1.97it/s]\u001B[A\n",
      "Iteration:  31%|███▏      | 43/137 [00:17<00:45,  2.08it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 44/137 [00:17<00:43,  2.13it/s]\u001B[A\n",
      "Iteration:  33%|███▎      | 45/137 [00:18<00:56,  1.62it/s]\u001B[A\n",
      "Iteration:  34%|███▎      | 46/137 [00:20<01:17,  1.17it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 47/137 [00:21<01:42,  1.14s/it]\u001B[A\n",
      "Iteration:  35%|███▌      | 48/137 [00:22<01:36,  1.08s/it]\u001B[A\n",
      "Iteration:  36%|███▌      | 49/137 [00:23<01:18,  1.12it/s]\u001B[A\n",
      "Iteration:  36%|███▋      | 50/137 [00:23<01:05,  1.32it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 51/137 [00:24<00:59,  1.44it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 52/137 [00:24<00:51,  1.65it/s]\u001B[A\n",
      "Iteration:  39%|███▊      | 53/137 [00:25<00:45,  1.84it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 54/137 [00:25<00:45,  1.82it/s]\u001B[A\n",
      "Iteration:  40%|████      | 55/137 [00:25<00:40,  2.01it/s]\u001B[A\n",
      "Iteration:  41%|████      | 56/137 [00:26<00:38,  2.13it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 57/137 [00:26<00:39,  2.01it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 58/137 [00:27<00:36,  2.15it/s]\u001B[A\n",
      "Iteration:  43%|████▎     | 59/137 [00:27<00:35,  2.18it/s]\u001B[A\n",
      "Iteration:  44%|████▍     | 60/137 [00:28<00:38,  1.98it/s]\u001B[A\n",
      "Iteration:  45%|████▍     | 61/137 [00:28<00:36,  2.11it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 62/137 [00:29<00:34,  2.18it/s]\u001B[A\n",
      "Iteration:  46%|████▌     | 63/137 [00:29<00:35,  2.11it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 64/137 [00:30<00:32,  2.25it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 65/137 [00:30<00:31,  2.29it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 66/137 [00:31<00:33,  2.13it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 67/137 [00:31<00:30,  2.26it/s]\u001B[A\n",
      "Iteration:  50%|████▉     | 68/137 [00:31<00:30,  2.30it/s]\u001B[A\n",
      "Iteration:  50%|█████     | 69/137 [00:32<00:31,  2.15it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 70/137 [00:32<00:29,  2.29it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 71/137 [00:33<00:27,  2.36it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 72/137 [00:34<00:39,  1.65it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 73/137 [00:35<00:48,  1.33it/s]\u001B[A\n",
      "Iteration:  54%|█████▍    | 74/137 [00:36<00:54,  1.15it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 75/137 [00:38<01:19,  1.29s/it]\u001B[A\n",
      "Iteration:  55%|█████▌    | 76/137 [00:39<01:03,  1.05s/it]\u001B[A\n",
      "Iteration:  56%|█████▌    | 77/137 [00:39<00:53,  1.13it/s]\u001B[A\n",
      "Iteration:  57%|█████▋    | 78/137 [00:40<00:47,  1.23it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 79/137 [00:40<00:40,  1.42it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 80/137 [00:41<00:36,  1.58it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 81/137 [00:41<00:34,  1.61it/s]\u001B[A\n",
      "Iteration:  60%|█████▉    | 82/137 [00:42<00:31,  1.77it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 83/137 [00:42<00:28,  1.92it/s]\u001B[A\n",
      "Iteration:  61%|██████▏   | 84/137 [00:43<00:28,  1.83it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 85/137 [00:43<00:25,  2.01it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 86/137 [00:44<00:23,  2.14it/s]\u001B[A\n",
      "Iteration:  64%|██████▎   | 87/137 [00:44<00:24,  2.05it/s]\u001B[A\n",
      "Iteration:  64%|██████▍   | 88/137 [00:45<00:22,  2.17it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 89/137 [00:45<00:21,  2.24it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 90/137 [00:46<00:23,  2.02it/s]\u001B[A\n",
      "Iteration:  66%|██████▋   | 91/137 [00:46<00:21,  2.09it/s]\u001B[A\n",
      "Iteration:  67%|██████▋   | 92/137 [00:46<00:21,  2.10it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 93/137 [00:47<00:23,  1.88it/s]\u001B[A\n",
      "Iteration:  69%|██████▊   | 94/137 [00:48<00:21,  1.99it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 95/137 [00:48<00:22,  1.90it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 96/137 [00:51<00:50,  1.22s/it]\u001B[A\n",
      "Iteration:  71%|███████   | 97/137 [00:53<00:52,  1.32s/it]\u001B[A\n",
      "Iteration:  72%|███████▏  | 98/137 [00:53<00:40,  1.03s/it]\u001B[A\n",
      "Iteration:  72%|███████▏  | 99/137 [00:53<00:32,  1.15it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 100/137 [00:54<00:26,  1.40it/s]\u001B[A\n",
      "Iteration:  74%|███████▎  | 101/137 [00:54<00:21,  1.64it/s]\u001B[A\n",
      "Iteration:  74%|███████▍  | 102/137 [00:55<00:20,  1.73it/s]\u001B[A\n",
      "Iteration:  75%|███████▌  | 103/137 [00:55<00:17,  1.96it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 104/137 [00:55<00:15,  2.13it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 105/137 [00:56<00:15,  2.10it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 106/137 [00:56<00:13,  2.28it/s]\u001B[A\n",
      "Iteration:  78%|███████▊  | 107/137 [00:57<00:12,  2.38it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 108/137 [00:57<00:12,  2.29it/s]\u001B[A\n",
      "Iteration:  80%|███████▉  | 109/137 [00:57<00:11,  2.42it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 110/137 [00:58<00:10,  2.48it/s]\u001B[A\n",
      "Iteration:  81%|████████  | 111/137 [00:58<00:11,  2.31it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 112/137 [00:59<00:10,  2.44it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 113/137 [00:59<00:09,  2.49it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 114/137 [00:59<00:09,  2.32it/s]\u001B[A\n",
      "Iteration:  84%|████████▍ | 115/137 [01:00<00:09,  2.44it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 116/137 [01:00<00:08,  2.50it/s]\u001B[A\n",
      "Iteration:  85%|████████▌ | 117/137 [01:01<00:08,  2.34it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 118/137 [01:01<00:07,  2.47it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 119/137 [01:01<00:07,  2.53it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 120/137 [01:02<00:07,  2.35it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 121/137 [01:02<00:06,  2.47it/s]\u001B[A\n",
      "Iteration:  89%|████████▉ | 122/137 [01:03<00:05,  2.52it/s]\u001B[A\n",
      "Iteration:  90%|████████▉ | 123/137 [01:03<00:05,  2.37it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 124/137 [01:04<00:05,  2.47it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 125/137 [01:04<00:04,  2.54it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 126/137 [01:04<00:04,  2.35it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 127/137 [01:05<00:04,  2.47it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 128/137 [01:05<00:03,  2.49it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 129/137 [01:06<00:03,  2.33it/s]\u001B[A\n",
      "Iteration:  95%|█████████▍| 130/137 [01:06<00:02,  2.45it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 131/137 [01:06<00:02,  2.51it/s]\u001B[A\n",
      "Iteration:  96%|█████████▋| 132/137 [01:07<00:02,  2.36it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 133/137 [01:07<00:01,  2.46it/s]\u001B[A\n",
      "Iteration:  98%|█████████▊| 134/137 [01:08<00:01,  2.51it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 135/137 [01:08<00:00,  2.36it/s]\u001B[A\n",
      "Iteration:  99%|█████████▉| 136/137 [01:08<00:00,  2.47it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 137/137 [01:09<00:00,  1.98it/s]\u001B[A\n",
      "Epoch:  40%|████      | 2/5 [02:00<03:05, 61.68s/it]\n",
      "Iteration:   0%|          | 0/137 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|          | 1/137 [00:00<00:51,  2.65it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 2/137 [00:00<00:50,  2.65it/s]\u001B[A\n",
      "Iteration:   2%|▏         | 3/137 [00:01<00:57,  2.34it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 4/137 [00:01<00:52,  2.53it/s]\u001B[A\n",
      "Iteration:   4%|▎         | 5/137 [00:01<00:51,  2.55it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 6/137 [00:02<00:56,  2.33it/s]\u001B[A\n",
      "Iteration:   5%|▌         | 7/137 [00:02<00:52,  2.49it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 8/137 [00:03<00:51,  2.53it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 9/137 [00:03<00:53,  2.39it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 10/137 [00:04<00:50,  2.52it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 11/137 [00:04<00:48,  2.60it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 12/137 [00:04<00:51,  2.43it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 13/137 [00:05<00:48,  2.54it/s]\u001B[A\n",
      "Iteration:  10%|█         | 14/137 [00:05<00:47,  2.60it/s]\u001B[A\n",
      "Iteration:  11%|█         | 15/137 [00:06<00:50,  2.42it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 16/137 [00:06<00:47,  2.53it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 17/137 [00:06<00:46,  2.59it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 18/137 [00:07<00:49,  2.42it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 19/137 [00:07<00:46,  2.54it/s]\u001B[A\n",
      "Iteration:  15%|█▍        | 20/137 [00:07<00:45,  2.59it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 21/137 [00:08<00:47,  2.44it/s]\u001B[A\n",
      "Iteration:  16%|█▌        | 22/137 [00:08<00:45,  2.56it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 23/137 [00:09<00:43,  2.61it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 24/137 [00:09<00:46,  2.45it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 25/137 [00:09<00:43,  2.56it/s]\u001B[A\n",
      "Iteration:  19%|█▉        | 26/137 [00:10<00:42,  2.62it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 27/137 [00:10<00:45,  2.43it/s]\u001B[A\n",
      "Iteration:  20%|██        | 28/137 [00:11<00:42,  2.55it/s]\u001B[A\n",
      "Iteration:  21%|██        | 29/137 [00:11<00:41,  2.62it/s]\u001B[A\n",
      "Iteration:  22%|██▏       | 30/137 [00:11<00:43,  2.45it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 31/137 [00:12<00:41,  2.54it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 32/137 [00:12<00:40,  2.60it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 33/137 [00:13<00:42,  2.44it/s]\u001B[A\n",
      "Iteration:  25%|██▍       | 34/137 [00:13<00:40,  2.56it/s]\u001B[A\n",
      "Iteration:  26%|██▌       | 35/137 [00:13<00:38,  2.62it/s]\u001B[A\n",
      "Iteration:  26%|██▋       | 36/137 [00:14<00:41,  2.46it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 37/137 [00:14<00:39,  2.56it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 38/137 [00:15<00:38,  2.57it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 39/137 [00:15<00:40,  2.43it/s]\u001B[A\n",
      "Iteration:  29%|██▉       | 40/137 [00:15<00:38,  2.54it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 41/137 [00:16<00:36,  2.60it/s]\u001B[A\n",
      "Iteration:  31%|███       | 42/137 [00:16<00:38,  2.44it/s]\u001B[A\n",
      "Iteration:  31%|███▏      | 43/137 [00:17<00:36,  2.56it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 44/137 [00:17<00:35,  2.62it/s]\u001B[A\n",
      "Iteration:  33%|███▎      | 45/137 [00:17<00:37,  2.45it/s]\u001B[A\n",
      "Iteration:  34%|███▎      | 46/137 [00:18<00:35,  2.56it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 47/137 [00:18<00:34,  2.62it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 48/137 [00:19<00:36,  2.44it/s]\u001B[A\n",
      "Iteration:  36%|███▌      | 49/137 [00:19<00:34,  2.58it/s]\u001B[A\n",
      "Iteration:  36%|███▋      | 50/137 [00:19<00:33,  2.63it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 51/137 [00:20<00:35,  2.45it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 52/137 [00:20<00:33,  2.56it/s]\u001B[A\n",
      "Iteration:  39%|███▊      | 53/137 [00:20<00:32,  2.62it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 54/137 [00:21<00:33,  2.46it/s]\u001B[A\n",
      "Iteration:  40%|████      | 55/137 [00:21<00:31,  2.57it/s]\u001B[A\n",
      "Iteration:  41%|████      | 56/137 [00:22<00:30,  2.63it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 57/137 [00:22<00:32,  2.47it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 58/137 [00:22<00:30,  2.56it/s]\u001B[A\n",
      "Iteration:  43%|████▎     | 59/137 [00:23<00:29,  2.62it/s]\u001B[A\n",
      "Iteration:  44%|████▍     | 60/137 [00:23<00:31,  2.45it/s]\u001B[A\n",
      "Iteration:  45%|████▍     | 61/137 [00:24<00:29,  2.57it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 62/137 [00:24<00:28,  2.61it/s]\u001B[A\n",
      "Iteration:  46%|████▌     | 63/137 [00:24<00:30,  2.44it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 64/137 [00:25<00:28,  2.54it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 65/137 [00:25<00:27,  2.59it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 66/137 [00:26<00:29,  2.41it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 67/137 [00:26<00:27,  2.54it/s]\u001B[A\n",
      "Iteration:  50%|████▉     | 68/137 [00:26<00:26,  2.61it/s]\u001B[A\n",
      "Iteration:  50%|█████     | 69/137 [00:27<00:27,  2.45it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 70/137 [00:27<00:26,  2.56it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 71/137 [00:28<00:25,  2.61it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 72/137 [00:28<00:26,  2.46it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 73/137 [00:28<00:24,  2.56it/s]\u001B[A\n",
      "Iteration:  54%|█████▍    | 74/137 [00:29<00:24,  2.62it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 75/137 [00:29<00:25,  2.46it/s]\u001B[A\n",
      "Iteration:  55%|█████▌    | 76/137 [00:30<00:23,  2.58it/s]\u001B[A\n",
      "Iteration:  56%|█████▌    | 77/137 [00:30<00:22,  2.62it/s]\u001B[A\n",
      "Iteration:  57%|█████▋    | 78/137 [00:30<00:24,  2.44it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 79/137 [00:31<00:22,  2.56it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 80/137 [00:31<00:21,  2.61it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 81/137 [00:32<00:22,  2.45it/s]\u001B[A\n",
      "Iteration:  60%|█████▉    | 82/137 [00:32<00:21,  2.55it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 83/137 [00:32<00:20,  2.62it/s]\u001B[A\n",
      "Iteration:  61%|██████▏   | 84/137 [00:33<00:21,  2.46it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 85/137 [00:33<00:20,  2.57it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 86/137 [00:33<00:19,  2.63it/s]\u001B[A\n",
      "Iteration:  64%|██████▎   | 87/137 [00:34<00:20,  2.46it/s]\u001B[A\n",
      "Iteration:  64%|██████▍   | 88/137 [00:34<00:19,  2.57it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 89/137 [00:35<00:18,  2.62it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 90/137 [00:35<00:19,  2.46it/s]\u001B[A\n",
      "Iteration:  66%|██████▋   | 91/137 [00:35<00:17,  2.57it/s]\u001B[A\n",
      "Iteration:  67%|██████▋   | 92/137 [00:36<00:17,  2.62it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 93/137 [00:36<00:17,  2.46it/s]\u001B[A\n",
      "Iteration:  69%|██████▊   | 94/137 [00:37<00:16,  2.56it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 95/137 [00:37<00:15,  2.63it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 96/137 [00:37<00:16,  2.48it/s]\u001B[A\n",
      "Iteration:  71%|███████   | 97/137 [00:38<00:15,  2.59it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 98/137 [00:38<00:14,  2.64it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 99/137 [00:39<00:15,  2.46it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 100/137 [00:39<00:14,  2.55it/s]\u001B[A\n",
      "Iteration:  74%|███████▎  | 101/137 [00:39<00:13,  2.61it/s]\u001B[A\n",
      "Iteration:  74%|███████▍  | 102/137 [00:40<00:14,  2.45it/s]\u001B[A\n",
      "Iteration:  75%|███████▌  | 103/137 [00:40<00:13,  2.57it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 104/137 [00:41<00:12,  2.63it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 105/137 [00:41<00:12,  2.46it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 106/137 [00:41<00:12,  2.57it/s]\u001B[A\n",
      "Iteration:  78%|███████▊  | 107/137 [00:42<00:11,  2.63it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 108/137 [00:42<00:11,  2.45it/s]\u001B[A\n",
      "Iteration:  80%|███████▉  | 109/137 [00:42<00:10,  2.58it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 110/137 [00:43<00:10,  2.64it/s]\u001B[A\n",
      "Iteration:  81%|████████  | 111/137 [00:43<00:10,  2.46it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 112/137 [00:44<00:09,  2.57it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 113/137 [00:44<00:09,  2.62it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 114/137 [00:45<00:09,  2.45it/s]\u001B[A\n",
      "Iteration:  84%|████████▍ | 115/137 [00:45<00:08,  2.56it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 116/137 [00:45<00:08,  2.61it/s]\u001B[A\n",
      "Iteration:  85%|████████▌ | 117/137 [00:46<00:08,  2.43it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 118/137 [00:46<00:07,  2.54it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 119/137 [00:46<00:06,  2.61it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 120/137 [00:47<00:06,  2.44it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 121/137 [00:47<00:06,  2.55it/s]\u001B[A\n",
      "Iteration:  89%|████████▉ | 122/137 [00:48<00:05,  2.62it/s]\u001B[A\n",
      "Iteration:  90%|████████▉ | 123/137 [00:48<00:05,  2.43it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 124/137 [00:48<00:05,  2.54it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 125/137 [00:49<00:04,  2.61it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 126/137 [00:49<00:04,  2.44it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 127/137 [00:50<00:03,  2.55it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 128/137 [00:50<00:03,  2.62it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 129/137 [00:50<00:03,  2.42it/s]\u001B[A\n",
      "Iteration:  95%|█████████▍| 130/137 [00:51<00:02,  2.54it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 131/137 [00:51<00:02,  2.61it/s]\u001B[A\n",
      "Iteration:  96%|█████████▋| 132/137 [00:52<00:02,  2.46it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 133/137 [00:52<00:01,  2.56it/s]\u001B[A\n",
      "Iteration:  98%|█████████▊| 134/137 [00:52<00:01,  2.61it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 135/137 [00:53<00:00,  2.44it/s]\u001B[A\n",
      "Iteration:  99%|█████████▉| 136/137 [00:53<00:00,  2.55it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 137/137 [00:54<00:00,  2.54it/s]\u001B[A\n",
      "Epoch:  60%|██████    | 3/5 [02:54<01:56, 58.19s/it]\n",
      "Iteration:   0%|          | 0/137 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|          | 1/137 [00:00<00:49,  2.77it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 2/137 [00:00<00:48,  2.78it/s]\u001B[A\n",
      "Iteration:   2%|▏         | 3/137 [00:01<00:55,  2.42it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 4/137 [00:01<00:52,  2.55it/s]\u001B[A\n",
      "Iteration:   4%|▎         | 5/137 [00:01<00:50,  2.62it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 6/137 [00:02<00:53,  2.43it/s]\u001B[A\n",
      "Iteration:   5%|▌         | 7/137 [00:02<00:50,  2.56it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 8/137 [00:03<00:49,  2.60it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 9/137 [00:03<00:52,  2.42it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 10/137 [00:03<00:49,  2.54it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 11/137 [00:04<00:48,  2.60it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 12/137 [00:04<00:51,  2.45it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 13/137 [00:05<00:48,  2.57it/s]\u001B[A\n",
      "Iteration:  10%|█         | 14/137 [00:05<00:47,  2.62it/s]\u001B[A\n",
      "Iteration:  11%|█         | 15/137 [00:05<00:49,  2.45it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 16/137 [00:06<00:47,  2.57it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 17/137 [00:06<00:45,  2.63it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 18/137 [00:07<00:48,  2.46it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 19/137 [00:07<00:45,  2.57it/s]\u001B[A\n",
      "Iteration:  15%|█▍        | 20/137 [00:07<00:44,  2.62it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 21/137 [00:08<00:47,  2.42it/s]\u001B[A\n",
      "Iteration:  16%|█▌        | 22/137 [00:08<00:45,  2.52it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 23/137 [00:09<00:43,  2.59it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 24/137 [00:09<00:46,  2.43it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 25/137 [00:09<00:44,  2.54it/s]\u001B[A\n",
      "Iteration:  19%|█▉        | 26/137 [00:10<00:42,  2.59it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 27/137 [00:10<00:45,  2.44it/s]\u001B[A\n",
      "Iteration:  20%|██        | 28/137 [00:11<00:42,  2.55it/s]\u001B[A\n",
      "Iteration:  21%|██        | 29/137 [00:11<00:41,  2.61it/s]\u001B[A\n",
      "Iteration:  22%|██▏       | 30/137 [00:11<00:43,  2.45it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 31/137 [00:12<00:41,  2.55it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 32/137 [00:12<00:40,  2.60it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 33/137 [00:13<00:42,  2.43it/s]\u001B[A\n",
      "Iteration:  25%|██▍       | 34/137 [00:13<00:40,  2.55it/s]\u001B[A\n",
      "Iteration:  26%|██▌       | 35/137 [00:13<00:39,  2.60it/s]\u001B[A\n",
      "Iteration:  26%|██▋       | 36/137 [00:14<00:41,  2.44it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 37/137 [00:14<00:39,  2.53it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 38/137 [00:14<00:38,  2.57it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 39/137 [00:15<00:40,  2.43it/s]\u001B[A\n",
      "Iteration:  29%|██▉       | 40/137 [00:15<00:38,  2.53it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 41/137 [00:16<00:36,  2.59it/s]\u001B[A\n",
      "Iteration:  31%|███       | 42/137 [00:16<00:39,  2.43it/s]\u001B[A\n",
      "Iteration:  31%|███▏      | 43/137 [00:16<00:37,  2.54it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 44/137 [00:17<00:35,  2.61it/s]\u001B[A\n",
      "Iteration:  33%|███▎      | 45/137 [00:17<00:37,  2.42it/s]\u001B[A\n",
      "Iteration:  34%|███▎      | 46/137 [00:18<00:35,  2.54it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 47/137 [00:18<00:34,  2.62it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 48/137 [00:19<00:36,  2.43it/s]\u001B[A\n",
      "Iteration:  36%|███▌      | 49/137 [00:19<00:34,  2.56it/s]\u001B[A\n",
      "Iteration:  36%|███▋      | 50/137 [00:19<00:33,  2.62it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 51/137 [00:20<00:34,  2.46it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 52/137 [00:20<00:33,  2.57it/s]\u001B[A\n",
      "Iteration:  39%|███▊      | 53/137 [00:20<00:32,  2.62it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 54/137 [00:21<00:33,  2.47it/s]\u001B[A\n",
      "Iteration:  40%|████      | 55/137 [00:21<00:31,  2.58it/s]\u001B[A\n",
      "Iteration:  41%|████      | 56/137 [00:22<00:31,  2.61it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 57/137 [00:22<00:32,  2.46it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 58/137 [00:22<00:30,  2.56it/s]\u001B[A\n",
      "Iteration:  43%|████▎     | 59/137 [00:23<00:29,  2.62it/s]\u001B[A\n",
      "Iteration:  44%|████▍     | 60/137 [00:23<00:31,  2.46it/s]\u001B[A\n",
      "Iteration:  45%|████▍     | 61/137 [00:24<00:29,  2.57it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 62/137 [00:24<00:28,  2.63it/s]\u001B[A\n",
      "Iteration:  46%|████▌     | 63/137 [00:24<00:30,  2.46it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 64/137 [00:25<00:28,  2.57it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 65/137 [00:25<00:27,  2.61it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 66/137 [00:26<00:29,  2.43it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 67/137 [00:26<00:27,  2.54it/s]\u001B[A\n",
      "Iteration:  50%|████▉     | 68/137 [00:26<00:26,  2.61it/s]\u001B[A\n",
      "Iteration:  50%|█████     | 69/137 [00:27<00:27,  2.45it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 70/137 [00:27<00:26,  2.56it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 71/137 [00:27<00:25,  2.62it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 72/137 [00:28<00:26,  2.47it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 73/137 [00:28<00:24,  2.57it/s]\u001B[A\n",
      "Iteration:  54%|█████▍    | 74/137 [00:29<00:23,  2.63it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 75/137 [00:29<00:25,  2.47it/s]\u001B[A\n",
      "Iteration:  55%|█████▌    | 76/137 [00:29<00:23,  2.57it/s]\u001B[A\n",
      "Iteration:  56%|█████▌    | 77/137 [00:30<00:22,  2.63it/s]\u001B[A\n",
      "Iteration:  57%|█████▋    | 78/137 [00:30<00:24,  2.43it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 79/137 [00:31<00:22,  2.54it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 80/137 [00:31<00:21,  2.61it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 81/137 [00:31<00:22,  2.45it/s]\u001B[A\n",
      "Iteration:  60%|█████▉    | 82/137 [00:32<00:21,  2.56it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 83/137 [00:32<00:20,  2.63it/s]\u001B[A\n",
      "Iteration:  61%|██████▏   | 84/137 [00:33<00:21,  2.47it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 85/137 [00:33<00:20,  2.58it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 86/137 [00:33<00:19,  2.60it/s]\u001B[A\n",
      "Iteration:  64%|██████▎   | 87/137 [00:34<00:20,  2.46it/s]\u001B[A\n",
      "Iteration:  64%|██████▍   | 88/137 [00:34<00:19,  2.56it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 89/137 [00:35<00:18,  2.63it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 90/137 [00:35<00:19,  2.46it/s]\u001B[A\n",
      "Iteration:  66%|██████▋   | 91/137 [00:35<00:17,  2.56it/s]\u001B[A\n",
      "Iteration:  67%|██████▋   | 92/137 [00:36<00:17,  2.60it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 93/137 [00:36<00:18,  2.44it/s]\u001B[A\n",
      "Iteration:  69%|██████▊   | 94/137 [00:37<00:16,  2.56it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 95/137 [00:37<00:16,  2.62it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 96/137 [00:37<00:16,  2.46it/s]\u001B[A\n",
      "Iteration:  71%|███████   | 97/137 [00:38<00:15,  2.57it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 98/137 [00:38<00:14,  2.61it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 99/137 [00:39<00:15,  2.42it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 100/137 [00:39<00:14,  2.54it/s]\u001B[A\n",
      "Iteration:  74%|███████▎  | 101/137 [00:39<00:13,  2.60it/s]\u001B[A\n",
      "Iteration:  74%|███████▍  | 102/137 [00:40<00:14,  2.45it/s]\u001B[A\n",
      "Iteration:  75%|███████▌  | 103/137 [00:40<00:13,  2.57it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 104/137 [00:40<00:12,  2.63it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 105/137 [00:41<00:13,  2.46it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 106/137 [00:41<00:12,  2.55it/s]\u001B[A\n",
      "Iteration:  78%|███████▊  | 107/137 [00:42<00:11,  2.60it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 108/137 [00:42<00:11,  2.44it/s]\u001B[A\n",
      "Iteration:  80%|███████▉  | 109/137 [00:42<00:10,  2.55it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 110/137 [00:43<00:10,  2.61it/s]\u001B[A\n",
      "Iteration:  81%|████████  | 111/137 [00:43<00:10,  2.45it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 112/137 [00:44<00:09,  2.56it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 113/137 [00:44<00:09,  2.62it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 114/137 [00:44<00:09,  2.46it/s]\u001B[A\n",
      "Iteration:  84%|████████▍ | 115/137 [00:45<00:08,  2.56it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 116/137 [00:45<00:08,  2.62it/s]\u001B[A\n",
      "Iteration:  85%|████████▌ | 117/137 [00:46<00:08,  2.45it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 118/137 [00:46<00:07,  2.57it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 119/137 [00:46<00:06,  2.61it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 120/137 [00:47<00:06,  2.45it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 121/137 [00:47<00:06,  2.56it/s]\u001B[A\n",
      "Iteration:  89%|████████▉ | 122/137 [00:48<00:05,  2.59it/s]\u001B[A\n",
      "Iteration:  90%|████████▉ | 123/137 [00:48<00:05,  2.36it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 124/137 [00:48<00:05,  2.46it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 125/137 [00:49<00:04,  2.53it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 126/137 [00:49<00:04,  2.38it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 127/137 [00:50<00:04,  2.49it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 128/137 [00:50<00:03,  2.54it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 129/137 [00:51<00:03,  2.35it/s]\u001B[A\n",
      "Iteration:  95%|█████████▍| 130/137 [00:51<00:02,  2.46it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 131/137 [00:51<00:02,  2.48it/s]\u001B[A\n",
      "Iteration:  96%|█████████▋| 132/137 [00:52<00:02,  2.26it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 133/137 [00:52<00:01,  2.39it/s]\u001B[A\n",
      "Iteration:  98%|█████████▊| 134/137 [00:53<00:01,  2.42it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 135/137 [00:53<00:00,  2.26it/s]\u001B[A\n",
      "Iteration:  99%|█████████▉| 136/137 [00:53<00:00,  2.36it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 137/137 [00:54<00:00,  2.52it/s]\u001B[A\n",
      "Epoch:  80%|████████  | 4/5 [03:48<00:56, 56.68s/it]\n",
      "Iteration:   0%|          | 0/137 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|          | 1/137 [00:00<00:51,  2.65it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 2/137 [00:00<00:51,  2.64it/s]\u001B[A\n",
      "Iteration:   2%|▏         | 3/137 [00:01<00:57,  2.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 4/137 [00:01<00:53,  2.47it/s]\u001B[A\n",
      "Iteration:   4%|▎         | 5/137 [00:01<00:52,  2.53it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 6/137 [00:02<00:56,  2.31it/s]\u001B[A\n",
      "Iteration:   5%|▌         | 7/137 [00:02<00:53,  2.45it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 8/137 [00:03<00:51,  2.51it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 9/137 [00:03<00:55,  2.31it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 10/137 [00:04<00:52,  2.43it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 11/137 [00:04<00:50,  2.50it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 12/137 [00:04<00:53,  2.33it/s]\u001B[A\n",
      "Iteration:   9%|▉         | 13/137 [00:05<00:51,  2.42it/s]\u001B[A\n",
      "Iteration:  10%|█         | 14/137 [00:05<00:49,  2.48it/s]\u001B[A\n",
      "Iteration:  11%|█         | 15/137 [00:06<00:52,  2.32it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 16/137 [00:06<00:49,  2.43it/s]\u001B[A\n",
      "Iteration:  12%|█▏        | 17/137 [00:06<00:47,  2.52it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 18/137 [00:07<00:49,  2.40it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 19/137 [00:07<00:46,  2.52it/s]\u001B[A\n",
      "Iteration:  15%|█▍        | 20/137 [00:08<00:45,  2.58it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 21/137 [00:08<00:47,  2.43it/s]\u001B[A\n",
      "Iteration:  16%|█▌        | 22/137 [00:08<00:46,  2.50it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 23/137 [00:09<00:46,  2.47it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 24/137 [00:09<00:48,  2.32it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 25/137 [00:10<00:45,  2.45it/s]\u001B[A\n",
      "Iteration:  19%|█▉        | 26/137 [00:10<00:43,  2.54it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 27/137 [00:11<00:46,  2.34it/s]\u001B[A\n",
      "Iteration:  20%|██        | 28/137 [00:11<00:44,  2.46it/s]\u001B[A\n",
      "Iteration:  21%|██        | 29/137 [00:11<00:43,  2.48it/s]\u001B[A\n",
      "Iteration:  22%|██▏       | 30/137 [00:12<00:46,  2.32it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 31/137 [00:12<00:43,  2.44it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 32/137 [00:13<00:42,  2.49it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 33/137 [00:13<00:45,  2.28it/s]\u001B[A\n",
      "Iteration:  25%|██▍       | 34/137 [00:14<00:43,  2.38it/s]\u001B[A\n",
      "Iteration:  26%|██▌       | 35/137 [00:14<00:42,  2.38it/s]\u001B[A\n",
      "Iteration:  26%|██▋       | 36/137 [00:14<00:45,  2.22it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 37/137 [00:15<00:42,  2.33it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 38/137 [00:15<00:40,  2.43it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 39/137 [00:16<00:42,  2.30it/s]\u001B[A\n",
      "Iteration:  29%|██▉       | 40/137 [00:16<00:40,  2.42it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 41/137 [00:16<00:38,  2.51it/s]\u001B[A\n",
      "Iteration:  31%|███       | 42/137 [00:17<00:40,  2.36it/s]\u001B[A\n",
      "Iteration:  31%|███▏      | 43/137 [00:17<00:37,  2.48it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 44/137 [00:18<00:36,  2.55it/s]\u001B[A\n",
      "Iteration:  33%|███▎      | 45/137 [00:18<00:39,  2.35it/s]\u001B[A\n",
      "Iteration:  34%|███▎      | 46/137 [00:18<00:36,  2.46it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 47/137 [00:19<00:35,  2.51it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 48/137 [00:19<00:37,  2.37it/s]\u001B[A\n",
      "Iteration:  36%|███▌      | 49/137 [00:20<00:35,  2.45it/s]\u001B[A\n",
      "Iteration:  36%|███▋      | 50/137 [00:20<00:34,  2.52it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 51/137 [00:21<00:36,  2.36it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 52/137 [00:21<00:34,  2.45it/s]\u001B[A\n",
      "Iteration:  39%|███▊      | 53/137 [00:21<00:33,  2.52it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 54/137 [00:22<00:35,  2.34it/s]\u001B[A\n",
      "Iteration:  40%|████      | 55/137 [00:22<00:33,  2.45it/s]\u001B[A\n",
      "Iteration:  41%|████      | 56/137 [00:23<00:32,  2.52it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 57/137 [00:23<00:33,  2.37it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 58/137 [00:23<00:31,  2.49it/s]\u001B[A\n",
      "Iteration:  43%|████▎     | 59/137 [00:24<00:30,  2.53it/s]\u001B[A\n",
      "Iteration:  44%|████▍     | 60/137 [00:24<00:33,  2.31it/s]\u001B[A\n",
      "Iteration:  45%|████▍     | 61/137 [00:25<00:31,  2.38it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 62/137 [00:25<00:31,  2.36it/s]\u001B[A\n",
      "Iteration:  46%|████▌     | 63/137 [00:26<00:33,  2.22it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 64/137 [00:26<00:31,  2.35it/s]\u001B[A\n",
      "Iteration:  47%|████▋     | 65/137 [00:26<00:29,  2.43it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 66/137 [00:27<00:30,  2.34it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 67/137 [00:27<00:28,  2.46it/s]\u001B[A\n",
      "Iteration:  50%|████▉     | 68/137 [00:28<00:27,  2.50it/s]\u001B[A\n",
      "Iteration:  50%|█████     | 69/137 [00:28<00:28,  2.38it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 70/137 [00:28<00:26,  2.51it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 71/137 [00:29<00:25,  2.59it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 72/137 [00:29<00:26,  2.42it/s]\u001B[A\n",
      "Iteration:  53%|█████▎    | 73/137 [00:30<00:25,  2.54it/s]\u001B[A\n",
      "Iteration:  54%|█████▍    | 74/137 [00:30<00:24,  2.61it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 75/137 [00:30<00:25,  2.46it/s]\u001B[A\n",
      "Iteration:  55%|█████▌    | 76/137 [00:31<00:23,  2.59it/s]\u001B[A\n",
      "Iteration:  56%|█████▌    | 77/137 [00:31<00:22,  2.64it/s]\u001B[A\n",
      "Iteration:  57%|█████▋    | 78/137 [00:32<00:23,  2.46it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 79/137 [00:32<00:22,  2.57it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 80/137 [00:32<00:21,  2.62it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 81/137 [00:33<00:22,  2.45it/s]\u001B[A\n",
      "Iteration:  60%|█████▉    | 82/137 [00:33<00:21,  2.54it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 83/137 [00:33<00:20,  2.60it/s]\u001B[A\n",
      "Iteration:  61%|██████▏   | 84/137 [00:34<00:22,  2.40it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 85/137 [00:34<00:20,  2.50it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 86/137 [00:35<00:19,  2.57it/s]\u001B[A\n",
      "Iteration:  64%|██████▎   | 87/137 [00:35<00:20,  2.43it/s]\u001B[A\n",
      "Iteration:  64%|██████▍   | 88/137 [00:35<00:19,  2.54it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 89/137 [00:36<00:18,  2.60it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 90/137 [00:36<00:19,  2.43it/s]\u001B[A\n",
      "Iteration:  66%|██████▋   | 91/137 [00:37<00:18,  2.55it/s]\u001B[A\n",
      "Iteration:  67%|██████▋   | 92/137 [00:37<00:17,  2.60it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 93/137 [00:38<00:18,  2.44it/s]\u001B[A\n",
      "Iteration:  69%|██████▊   | 94/137 [00:38<00:16,  2.55it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 95/137 [00:38<00:16,  2.61it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 96/137 [00:39<00:16,  2.46it/s]\u001B[A\n",
      "Iteration:  71%|███████   | 97/137 [00:39<00:15,  2.57it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 98/137 [00:39<00:14,  2.63it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 99/137 [00:40<00:15,  2.43it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 100/137 [00:40<00:14,  2.54it/s]\u001B[A\n",
      "Iteration:  74%|███████▎  | 101/137 [00:41<00:13,  2.60it/s]\u001B[A\n",
      "Iteration:  74%|███████▍  | 102/137 [00:41<00:14,  2.44it/s]\u001B[A\n",
      "Iteration:  75%|███████▌  | 103/137 [00:41<00:13,  2.54it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 104/137 [00:42<00:12,  2.60it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 105/137 [00:42<00:13,  2.43it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 106/137 [00:43<00:12,  2.55it/s]\u001B[A\n",
      "Iteration:  78%|███████▊  | 107/137 [00:43<00:11,  2.58it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 108/137 [00:43<00:12,  2.41it/s]\u001B[A\n",
      "Iteration:  80%|███████▉  | 109/137 [00:44<00:11,  2.51it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 110/137 [00:44<00:10,  2.55it/s]\u001B[A\n",
      "Iteration:  81%|████████  | 111/137 [00:45<00:10,  2.40it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 112/137 [00:45<00:09,  2.52it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 113/137 [00:45<00:09,  2.58it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 114/137 [00:46<00:09,  2.43it/s]\u001B[A\n",
      "Iteration:  84%|████████▍ | 115/137 [00:46<00:08,  2.55it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 116/137 [00:47<00:08,  2.61it/s]\u001B[A\n",
      "Iteration:  85%|████████▌ | 117/137 [00:47<00:08,  2.42it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 118/137 [00:47<00:07,  2.53it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 119/137 [00:48<00:06,  2.59it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 120/137 [00:48<00:06,  2.44it/s]\u001B[A\n",
      "Iteration:  88%|████████▊ | 121/137 [00:49<00:06,  2.54it/s]\u001B[A\n",
      "Iteration:  89%|████████▉ | 122/137 [00:49<00:05,  2.60it/s]\u001B[A\n",
      "Iteration:  90%|████████▉ | 123/137 [00:49<00:05,  2.45it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 124/137 [00:50<00:05,  2.58it/s]\u001B[A\n",
      "Iteration:  91%|█████████ | 125/137 [00:50<00:04,  2.62it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 126/137 [00:51<00:04,  2.44it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 127/137 [00:51<00:03,  2.56it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 128/137 [00:51<00:03,  2.58it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 129/137 [00:52<00:03,  2.44it/s]\u001B[A\n",
      "Iteration:  95%|█████████▍| 130/137 [00:52<00:02,  2.53it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 131/137 [00:53<00:02,  2.60it/s]\u001B[A\n",
      "Iteration:  96%|█████████▋| 132/137 [00:53<00:02,  2.43it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 133/137 [00:53<00:01,  2.54it/s]\u001B[A\n",
      "Iteration:  98%|█████████▊| 134/137 [00:54<00:01,  2.60it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 135/137 [00:54<00:00,  2.44it/s]\u001B[A\n",
      "Iteration:  99%|█████████▉| 136/137 [00:55<00:00,  2.55it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 137/137 [00:55<00:00,  2.47it/s]\u001B[A\n",
      "Epoch: 100%|██████████| 5/5 [04:43<00:00, 56.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0710827744338247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 28/28 [00:03<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Accuracy: 0.9339305711086227\n",
      "Slot F1-Score: 0.8751094762655456\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from seqeval.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers import BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Global variables\n",
    "device = 'cuda:0'  # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Used to report errors on CUDA side\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "class InputSample:\n",
    "    def __init__(self, id_, words_, intent_, slot_labels_):\n",
    "        self.id = id_\n",
    "        self.words = words_\n",
    "        self.intent = intent_\n",
    "        self.slot_labels = slot_labels_\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class InputFeatures:\n",
    "    def __init__(self, input_id, attention_mask, token_type_id, intent_label_id, slot_labels_ids):\n",
    "        self.input_id = input_id\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_id = token_type_id\n",
    "        self.intent_label_id = intent_label_id\n",
    "        self.slot_labels_ids = slot_labels_ids\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, train_raw_, dev_raw_, test_raw_, intent_labels_, slot_labels_):\n",
    "        self.train_raw = train_raw_\n",
    "        self.dev_raw = dev_raw_\n",
    "        self.test_raw = test_raw_\n",
    "\n",
    "        self.intent_labels = intent_labels_\n",
    "        self.slot_labels = slot_labels_\n",
    "\n",
    "    def create_birt_format(self, mode):\n",
    "        data_ = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            raw = self.train_raw\n",
    "        elif mode == \"dev\":\n",
    "            raw = self.dev_raw\n",
    "        elif mode == \"test\":\n",
    "            raw = self.test_raw\n",
    "        else:\n",
    "            raise ValueError(\"The mode should be in train, dev or test\")\n",
    "\n",
    "        for i_, input_dict in enumerate(raw):\n",
    "            input_id = \"%s-%d\" % (mode, i_)\n",
    "\n",
    "            words_ = input_dict['utterance'].split()\n",
    "            intent_ = input_dict['intent']\n",
    "            intent_label_ = self.intent_labels.index(intent_) if intent_ in self.intent_labels else self.intent_labels.index('UNK')\n",
    "            slots_ = input_dict['slots'].split()\n",
    "            slot_labels = [self.slot_labels.index(s) if s in self.slot_labels else self.slot_labels.index('UNK') for s in slots_]\n",
    "\n",
    "            assert len(words_) == len(slot_labels)  # Sanity check\n",
    "\n",
    "            data_.append(InputSample(id_=input_id, words_=words_, intent_=intent_label_, slot_labels_=slot_labels))\n",
    "\n",
    "        return data_\n",
    "\n",
    "\n",
    "class BertForIntentClassificationAndSlotFilling(BertPreTrainedModel):\n",
    "    def __init__(self, config, intent_label_lst, slot_label_lst, dropout_rate=0.1, ignore_index=0, slot_loss_coef=1.0):\n",
    "        super(BertForIntentClassificationAndSlotFilling, self).__init__(config)\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.num_slot_labels = len(slot_label_lst)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.slot_loss_coef = slot_loss_coef\n",
    "\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.intent_classifier = nn.Linear(config.hidden_size, self.num_intent_labels)\n",
    "        self.slot_classifier = nn.Linear(config.hidden_size, self.num_slot_labels)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids, slot_label_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        # pooled_output = self.dropout(sequence_output[:, 0, :])\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        if intent_label_ids is not None and slot_label_ids is not None:\n",
    "            # Intent Classification Loss\n",
    "            intent_loss_fct = nn.CrossEntropyLoss()\n",
    "            intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\n",
    "            total_loss += intent_loss\n",
    "\n",
    "            # Slot Filling Loss\n",
    "            slot_loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_label_id)\n",
    "            if attention_mask is not None:\n",
    "                slot_active_loss = attention_mask.view(-1) == 1\n",
    "                slot_active_logits = slot_logits.view(-1, self.num_slot_labels)[slot_active_loss]\n",
    "                slot_active_labels = slot_label_ids.view(-1)[slot_active_loss]\n",
    "                slot_loss = slot_loss_fct(slot_active_logits, slot_active_labels)\n",
    "            else:\n",
    "                slot_loss = slot_loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_label_ids.view(-1))\n",
    "\n",
    "            total_loss += slot_loss * self.slot_loss_coef\n",
    "\n",
    "        outputs = ((intent_logits, slot_logits),) + outputs[2:]\n",
    "        outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Function to load the data from the json file\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "        input: path/to/data\n",
    "        output: json\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def input2features(input_sample, max_seq_len, tokenizer, pad_token_label_id=100, cls_token_segment_id=0,\n",
    "                   pad_token_segment_id=0, sequence_a_segment_id=0, mask_padding_with_zero=True):\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i, sample in enumerate(input_sample):\n",
    "        tokens = []\n",
    "        slot_labels_ids = []\n",
    "\n",
    "        for word, slot_label in zip(sample.words, sample.slot_labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # handle bad encoded word\n",
    "            if len(word_tokens) == 0:\n",
    "                word_tokens = [unk_token]\n",
    "\n",
    "            tokens.extend(word_tokens)\n",
    "\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_labels_ids.extend([int(slot_label)] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = tokenizer.num_special_tokens_to_add()\n",
    "        # special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_len - special_tokens_count)]\n",
    "            slot_labels_ids = slot_labels_ids[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        slot_labels_ids += [pad_token_label_id]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        slot_labels_ids += [pad_token_label_id]\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # 0 padding up to the sequence length.\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids += ([pad_token_id] * padding_length)\n",
    "        attention_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        slot_labels_ids += ([pad_token_label_id] * padding_length)\n",
    "        token_type_ids += ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with input length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(token_type_ids), max_seq_len)\n",
    "        assert len(slot_labels_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(slot_labels_ids), max_seq_len)\n",
    "\n",
    "        intent_label_id = int(sample.intent)\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_id=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_id=token_type_ids,\n",
    "                slot_labels_ids=slot_labels_ids,\n",
    "                intent_label_id=intent_label_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_to_tensor(features):\n",
    "    all_input_ids = torch.tensor([f.input_id for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_id for f in features], dtype=torch.long)\n",
    "    all_slot_labels_ids = torch.tensor([f.slot_labels_ids for f in features], dtype=torch.long)\n",
    "    all_intent_label_ids = torch.tensor([f.intent_label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_slot_labels_ids, all_intent_label_ids)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train(model, train_dataset, optimizer, n_epochs, gradient_accumulation_steps, warmup_steps, max_grad_norm, device):\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * n_epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss = 0.0\n",
    "    model.zero_grad()\n",
    "\n",
    "    train_iterator = trange(int(n_epochs), desc=\"Epoch\")\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'intent_label_ids': batch[4],\n",
    "                      'slot_label_ids': batch[3]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "# evaluation method\n",
    "def evaluate_test(model, dataset, slot_label_lst, batch_size, device, pad_token_label_id):\n",
    "    data_sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
    "\n",
    "    loss = 0.0\n",
    "    nb_steps = 0\n",
    "    intent_preds = None\n",
    "    slot_preds = None\n",
    "    out_intent_label_ids = None\n",
    "    out_slot_labels_ids = None\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'intent_label_ids': batch[4],\n",
    "                      'slot_label_ids': batch[3]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            temp_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "            loss += temp_loss.mean().item()\n",
    "\n",
    "        nb_steps += 1\n",
    "\n",
    "        # intent prediction\n",
    "        if intent_preds is None:\n",
    "            intent_preds = intent_logits.detach().cpu().numpy()\n",
    "            out_intent_label_ids = inputs['intent_label_ids'].detach().cpu().numpy()\n",
    "        else:\n",
    "            intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
    "            out_intent_label_ids = np.append(out_intent_label_ids, inputs['intent_label_ids'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        # slot prediction/\n",
    "        if slot_preds is None:\n",
    "            slot_preds = slot_logits.detach().cpu().numpy()\n",
    "            out_slot_labels_ids = inputs[\"slot_label_ids\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "            out_slot_labels_ids = np.append(out_slot_labels_ids, inputs[\"slot_label_ids\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = loss / nb_steps\n",
    "    results = {\n",
    "        \"loss\": eval_loss\n",
    "    }\n",
    "\n",
    "    # Intent result\n",
    "    intent_preds = np.argmax(intent_preds, axis=1)\n",
    "    out_intent_label_ids = out_intent_label_ids.reshape(-1)\n",
    "    intent_result = (intent_preds == out_intent_label_ids).mean()\n",
    "\n",
    "    # Slot result\n",
    "    slot_preds = np.argmax(slot_preds, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    slot_preds_list = [[] for _ in range(slot_preds.shape[0])]\n",
    "    slot_label_list = [[] for _ in range(slot_preds.shape[0])]\n",
    "\n",
    "    for i in range(slot_preds.shape[0]):\n",
    "        for j in range(slot_preds.shape[1]):\n",
    "            if out_slot_labels_ids[i, j] != pad_token_label_id:\n",
    "                slot_preds_list[i].append(slot_label_lst[slot_preds[i][j]])\n",
    "                slot_label_list[i].append(slot_label_lst[out_slot_labels_ids[i][j]])\n",
    "\n",
    "    slot_result = f1_score(slot_label_list, slot_preds_list)\n",
    "\n",
    "    results[\"intent_result\"] = intent_result\n",
    "    results[\"slot_result\"] = slot_result\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 1. Load data\n",
    "tmp_train_raw = load_data(os.path.join('dataset', 'ATIS', 'train.json'))\n",
    "test_raw = load_data(os.path.join('dataset', 'ATIS', 'test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "# 2. Create a train, test and dev set\n",
    "portion = round(((len(tmp_train_raw) + len(test_raw)) * 0.10) / (len(tmp_train_raw)), 2)\n",
    "intents = [x['intent'] for x in tmp_train_raw]  # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "mini_Train = []\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1:  # If some intents occur once only, we put them in training\n",
    "        X.append(tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(tmp_train_raw[id_y])\n",
    "\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, Y, test_size=portion, random_state=42, shuffle=True, stratify=Y)\n",
    "X_train.extend(mini_Train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k: round(v / len(y_train), 3) * 100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'),\n",
    "pprint({k: round(v / len(y_dev), 3) * 100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:')\n",
    "pprint({k: round(v / len(y_test), 3) * 100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('=' * 89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))\n",
    "\n",
    "# 3. Prepare data for BERT model\n",
    "words = sum([x['utterance'].split() for x in train_raw], [])  # No set() since we want to compute the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw  # We do not want unk labels, # however this depends on the research purpose\n",
    "slots_ = set(sum([line['slots'].split() for line in corpus], []))\n",
    "intents_ = set([line['intent'] for line in corpus])\n",
    "\n",
    "slots = list(slots_)\n",
    "intents = list(intents_)\n",
    "intents.append(\"UNK\")\n",
    "slots.append(\"UNK\")\n",
    "\n",
    "# 4. Create a data processor that will create the BERT input data\n",
    "data_processor = DataProcessor(X_train, X_dev, test_raw, intents, slots)\n",
    "train_data = data_processor.create_birt_format(\"train\")\n",
    "dev_data = data_processor.create_birt_format(\"dev\")\n",
    "test_data = data_processor.create_birt_format(\"test\")\n",
    "\n",
    "pad_token_label_id = 0  # ignore index 0: \"O\" token\n",
    "max_seq_len = 50\n",
    "\n",
    "# 5. Convert data to features\n",
    "train_features = input2features(train_data, max_seq_len, tokenizer, pad_token_label_id=pad_token_label_id)\n",
    "test_features = input2features(test_data, max_seq_len, tokenizer, pad_token_label_id=pad_token_label_id)\n",
    "dev_features = input2features(dev_data, max_seq_len, tokenizer, pad_token_label_id=pad_token_label_id)\n",
    "\n",
    "# 6. Convert features to tensors\n",
    "train_dataset = convert_to_tensor(train_features)\n",
    "test_dataset = convert_to_tensor(test_features)\n",
    "dev_dataset = convert_to_tensor(dev_features)\n",
    "\n",
    "# 7. Initiate Model\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', finetuning_task='intent-classification-and-slot-filling')\n",
    "model = BertForIntentClassificationAndSlotFilling.from_pretrained('bert-base-uncased', config=config,\n",
    "                                                                  intent_label_lst=intents, slot_label_lst=slots)\n",
    "model.to(device)\n",
    "\n",
    "# 8. Hyperparameters for training\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "n_epochs = 5.0\n",
    "gradient_accumulation_steps = 3\n",
    "lr = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if\n",
    "                not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if\n",
    "                any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    "\n",
    "\n",
    "# 9. Train and evaluate\n",
    "_, tr_loss = train(model, train_dataset, optimizer, n_epochs, gradient_accumulation_steps, warmup_steps, max_grad_norm,\n",
    "                   device)\n",
    "\n",
    "print(\"Train loss: {}\".format(tr_loss))\n",
    "\n",
    "results = evaluate_test(model, test_dataset, slots, eval_batch_size, device, pad_token_label_id)\n",
    "print(\"Intent Accuracy: {}\".format(results[\"intent_result\"]))\n",
    "print(\"Slot F1-Score: {}\".format(results[\"slot_result\"]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T22:59:02.162499100Z",
     "start_time": "2023-06-15T22:54:03.056388500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
