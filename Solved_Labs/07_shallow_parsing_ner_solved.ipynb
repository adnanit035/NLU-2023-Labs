{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling: Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding: \n",
    "    - relation between Sequence Labeling and Shallow Parsing\n",
    "    - IOB Notation\n",
    "    - Joint Segmentation and Classification\n",
    "    - Feature Engineering\n",
    "- Learning how to:\n",
    "    - use Named Entity Recognition in \n",
    "        - spacy\n",
    "        - NLTK\n",
    "    - train, test, and evaluate Conditional Random Fields models\n",
    "    - perform feature engineering with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Conditional Random Fields\n",
    "    - Lafferty et al. (2001) [Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf) (__original paper__)\n",
    "    - Sutton & McCallum's [An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf)\n",
    "    - Edwin Chen's [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
    "    - Michael Collin's [Log-Linear Models, MEMMs, and CRFs](http://www.cs.columbia.edu/~mcollins/crf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 8: Sequence Labeling for Parts of Speech and Named Entities](https://web.stanford.edu/~jurafsky/slp3/8.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 7: Extracting Information from Text](https://www.nltk.org/book/ch07.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [`conll.py`](https://github.com/esrel/LUS/) (in `src` folder)\n",
    "- [CRFsuite](http://www.chokkan.org/software/crfsuite/)\n",
    "    - [python-crfsuite](https://python-crfsuite.readthedocs.io) python binding to `CRFsuite`.\n",
    "    - [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) `python-crfsuite` wrapper providing API similar to `scikit-learn`\n",
    "    - you need to install both `python_crfsuite` and `sklearn_crfsuite` to use `sklearn_crfsuite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Sequence Labeling and Shallow Parsing\n",
    "\n",
    "Below are some examples of NLP tasks that Sequence Labeling is applied to as one of the methods.\n",
    "\n",
    "The scenario when members of a sequence are mapped to higher order units (i.e. grouped together `[['a'],['b','c']]`) and assigned a category) is known as __shallow parsing__.\n",
    "\n",
    "- [Part-of-Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) (Chunking)\n",
    "    - [Phrase Chunking](https://en.wikipedia.org/wiki/Phrase_chunking)\n",
    "    - [Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) \n",
    "    - [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)\n",
    "    - Dependency [Parsing](https://en.wikipedia.org/wiki/Parsing) \n",
    "    - Discourse Parsing\n",
    "    - (Natural/Spoken) __Language Understanding__: Concept Tagging/Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Shallow Parsing\n",
    "\n",
    "As we have already mentioned, [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) is a kind of Sequence Labeling. The main difference from Sequence Labeling task, such as Part-of-Speech tagging, where there is an output label (tag) per token; Shallow Parsing additionally performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "In other words, we want to be able to capture information that expressions like `\"New York\"` that consist of 2 tokens, constitute a single unit.\n",
    "\n",
    "What this means in practice is that Shallow Parsing performs *jointly* (or not) 2 tasks:\n",
    "- __Segmentation__ of input into constituents (__spans__)\n",
    "- __Classification__ (Categorization, Labeling) of these constituents into predefined set of labels (__types__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Encoding Segmentation Information: CoNLL Corpus Format\n",
    "\n",
    "Corpus in CoNLL format consists of series of sentences, separated by blank lines. Each sentence is encoded using a table (or \"grid\") of values, where each line corresponds to a single word, and each column corresponds to an annotation type. \n",
    "\n",
    "The set of columns used by CoNLL-style files can vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. [IOB Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "- The notation scheme is used to label *multi-word* spans in token-per-line format.\n",
    "    - *Los Angeles* is a *LOCATION* concept that has 2 tokens\n",
    "- Both, prefix and suffix notations are commons: \n",
    "    - prefix: __B-LOC__\n",
    "    - suffix: __LOC-B__\n",
    "\n",
    "- Meaning of Prefixes\n",
    "    - __B__ for (__B__)eginning of span\n",
    "    - __I__ for (__I__)nside of span\n",
    "    - __O__ for (__O__)utside of span (no prefix or suffix, just `O`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative Schemes:\n",
    "- No prefix or suffix (useful when there are no *multi-word* concepts)\n",
    "```\n",
    "        Alex       PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        LOC\n",
    "        Angeles    LOC\n",
    "        in         O\n",
    "        California LOC\n",
    "```\n",
    "- __IOB/IOB2/BIO__\n",
    "\n",
    "- __IOBE__\n",
    "    - IOB + \n",
    "    - __E__ for (__E__)nd of span (or __L__ for (__L__)ast)\n",
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```\n",
    "    \n",
    "- __BILOU/BIOES__\n",
    "    - IOB + \n",
    "    - __L__ for (__L__)ast word of span\n",
    "    - __U__ for (__U__)nit word (or __S__ for (__S__)ingleton)\n",
    "```\n",
    "        Alex       U-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California U-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Choice of Scheme\n",
    "- It is possible to convert IOB, IOBE, & BILOU formats to each other\n",
    "- Each prefix is applied to every concept label, consequently we increase the number of transitions whose probabilities we need to estimate; \n",
    "    - increasing data sparseness, as for each label we will have less observations\n",
    "- The choice of scheme depends on the amount of available data:\n",
    "    - __IOB__ for least amount\n",
    "    - __BILOU__ for the most amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Terminology\n",
    "There is no strict naming convention regarding schemes (see alternatives) or how each constituent is termed. \n",
    "Below is the terminology used in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Interpretation\n",
    "Segmentation and Labeling data formats encode the following information:\n",
    "- in string (sentence) `\"Alex is going to Los Angeles in California\"`\n",
    "- there are 3 __entities__ (a.k.a. chunks, concepts or slots, depending on NLP task and perspective), that have __types__ (labels)\n",
    "    - `PERSON`\n",
    "    - `LOCATION`\n",
    "    \n",
    "- entity of __type__ `PERSON`: \n",
    "    - has __span__:\n",
    "        - as tokens from `0` for *CoNLL*: `[0:1]`\n",
    "    - has __value__: `\"Alex\"`\n",
    "        - string *covered by* (*on included*) in __span__\n",
    " \n",
    "*CoNLL* format encodes __tokenization__ informations. In other words, how string `\"Alex is going to Los Angeles in California\"` is split into tokens. Since most Sequence Labeling algorithms operate on token level, internally the strings are split into tokens, applying *IOB*-like schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Wirte the IOB tags for the sentence `\"Steve Jobs established Apple Inc.\"`\n",
    "    - 'Steve Jobs' is PER\n",
    "    - 'Apple Inc' is ORG\n",
    "#### Solution\n",
    "    - 'Steve' is B-PER\n",
    "    - 'Jobs' is I-PER\n",
    "    - 'established' is 0\n",
    "    - 'Apple' is B-ORG\n",
    "    - 'Inc' is I-ORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Named Entity Recognition with NLTK\n",
    "[NLTK](https://www.nltk.org/api/nltk.tag.html) provides implementations of popular sequence labeling algorithms for Part-of-Speech Tagging (including [HMM](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)), that can be used for Sequence Labeling in general. \n",
    "\n",
    "- Loading & working with CoNLL format corpora in NLTK\n",
    "- Tagger training & testing (running)\n",
    "\n",
    "To have a custom tagger that labels input text with our __custom label set__, we need to __train__ it on a corpus annotated with this __custom label set__.\n",
    "\n",
    "Addtionally, NLTK provides [Chunking](http://www.nltk.org/api/nltk.chunk.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. NLTK Pre-trained NE Chunker\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as `NE`; otherwise, the classifier adds category labels such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:06:57.318167100Z",
     "start_time": "2023-06-03T01:06:38.826202800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\adnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\adnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:07:09.588468300Z",
     "start_time": "2023-06-03T01:07:09.394457600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON Pierre/NNP)\n",
      "  (ORGANIZATION Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n",
      "(S\n",
      "  (NE Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "for s in treebank.tagged_sents():\n",
    "    print(s)\n",
    "    print(nltk.ne_chunk(s))\n",
    "    print(nltk.ne_chunk(s, binary=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:08:04.640487700Z",
     "start_time": "2023-06-03T01:08:02.366815400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\adnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2002.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:08:15.985480700Z",
     "start_time": "2023-06-03T01:08:14.086095100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35651\n",
      "('LOC', 'PER', 'ORG', 'MISC')\n",
      "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
      "[('Melbourne', 'NP'), ('(', 'Fpa'), ('Australia', 'NP'), (')', 'Fpt'), (',', 'Fc'), ('25', 'Z'), ('may', 'NC'), ('(', 'Fpa'), ('EFE', 'NC'), (')', 'Fpt'), ('.', 'Fp')]\n",
      "(S\n",
      "  (LOC Melbourne/NP)\n",
      "  (/Fpa\n",
      "  (LOC Australia/NP)\n",
      "  )/Fpt\n",
      "  ,/Fc\n",
      "  25/Z\n",
      "  may/NC\n",
      "  (/Fpa\n",
      "  (ORG EFE/NC)\n",
      "  )/Fpt\n",
      "  ./Fp)\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "\n",
    "print(len(conll2002.tagged_sents()))\n",
    "print(conll2002._chunk_types)\n",
    "print(conll2002.sents('esp.train')[0])\n",
    "print(conll2002.tagged_sents('esp.train')[0])\n",
    "print(conll2002.chunked_sents('esp.train')[0])\n",
    "print(conll2002.iob_sents('esp.train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:08:30.968637300Z",
     "start_time": "2023-06-03T01:08:17.894618700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n",
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# training hmm on training data: exactly as above\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents[0])\n",
    "\n",
    "tst_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_ner = hmm_model.train(trn_sents)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_ner.accuracy(tst_sents)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### NLTK Chunk Tagger Note\n",
    "HMM uses only words as input, NLTK also povides trainable MaxEnt Chunker Tagger, which unfortunatelly requires `megam` file. Unfortunatelly, it is very convoluted to install. (http://www.umiacs.umd.edu/~hal/megam/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Segmentation \n",
    "Train a tagger to perform *segmentation* of input sentences into constituents\n",
    "- Strip concept information from output labels (i.e. keep only IOB-prefix)\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:47:25.775522300Z",
     "start_time": "2023-06-03T01:47:17.670521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "[('Melbourne', 'B'), ('(', 'O'), ('Australia', 'B'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B'), (')', 'O'), ('.', 'O')]\n",
      "[('Sao', 'B'), ('Paulo', 'I'), ('(', 'O'), ('Brasil', 'B'), (')', 'O'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "C:\\Users\\adnan\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4297\n"
     ]
    }
   ],
   "source": [
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "# Hint: split iob with using '-'\n",
    "trn_sents_seg = [[(text, iob.split('-')[0]) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents_seg[0])\n",
    "tst_sents_seg = [[(text, iob.split('-')[0]) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "print(tst_sents_seg[0])\n",
    "\n",
    "# Train tagger to predict segmentation labels\n",
    "hmm_seg = hmm_model.train(trn_sents_seg)\n",
    "\n",
    "# evaluation\n",
    "accuracy = hmm_seg.accuracy(tst_sents_seg)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CoNLL Eval\n",
    "CoNLL Community developed a perl script to evaluate *segmentation* and *labeling* performance jointly using IOB information. Such evaluation provides more accurate assessment of the shallow parsing performance, in comparison to token-level metrics (e.g. NLTK accuracy).\n",
    "\n",
    "- import `evaluate` function from `conll.py` (example shown)\n",
    "- evaluate tagger predictions\n",
    "- compare performances to token-level accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:48:47.332750800Z",
     "start_time": "2023-06-03T01:48:36.455179400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.691  0.280  0.399   735\nORG    0.795  0.396  0.528  1400\nMISC   0.570  0.203  0.299   340\nLOC    0.030  0.849  0.058  1084\ntotal  0.055  0.491  0.098  3559",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.691</td>\n      <td>0.280</td>\n      <td>0.399</td>\n      <td>735</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.795</td>\n      <td>0.396</td>\n      <td>0.528</td>\n      <td>1400</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.570</td>\n      <td>0.203</td>\n      <td>0.299</td>\n      <td>340</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.030</td>\n      <td>0.849</td>\n      <td>0.058</td>\n      <td>1084</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.055</td>\n      <td>0.491</td>\n      <td>0.098</td>\n      <td>3559</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd\n",
    "\n",
    "# getting references (note that it is testb this time)\n",
    "refs = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]\n",
    "print(refs[0])\n",
    "# getting hypotheses\n",
    "hyps = [hmm_ner.tag(s) for s in conll2002.sents('esp.testb')]\n",
    "print(hyps[0])\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-03T01:50:26.565533900Z",
     "start_time": "2023-06-03T01:50:22.063208100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre Vinken', '61 years old', 'Nov. 29']\n",
      "[('PERSON', 'B'), ('PERSON', 'I'), ('', 'O'), ('DATE', 'B'), ('DATE', 'I'), ('DATE', 'I'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('DATE', 'B'), ('DATE', 'I'), ('', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "txt = 'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'\n",
    "doc = nlp(txt)\n",
    "\n",
    "print([ent.text for ent in doc.ents])\n",
    "print([(t.ent_type_, t.ent_iob_) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shallow Parsing with Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CRFs](https://en.wikipedia.org/wiki/Conditional_random_field) are a type of __discriminative undirected probabilistic graphical model__. \n",
    "It is a generalization of __any__ undirected graph structure.\n",
    "In Natural Language Processing, the structure is a *sequence* of words, and conditioning is on *previous transition*. This is known as __Linear Chain CRFs__.\n",
    "\n",
    "For general graphs, the problem of exact inference in CRFs is intractable. For __Linear Chain CRFs__, however, there is an exact solution, and the used algorithm is \"analogous to the forward-backward and Viterbi algorithm for the case of Hidden Markov Models\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python CRF Tutorials\n",
    "\n",
    "Authors of the python packages provide tutorials in the form of notebooks already!\n",
    "\n",
    "__Follow the tutorials to learn the tools.__\n",
    "\n",
    "- [sklearn-crfsuite notebook](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "- [python-crfsuite notebook](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb)\n",
    "\n",
    "- `bias` is explained [here](https://github.com/scrapinghub/python-crfsuite/issues/73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a CRF baseline for our dataset that:\n",
    "- considers only word itself and the previous tag (similar to HMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:07:04.681335400Z",
     "start_time": "2023-06-03T19:07:04.575546500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n",
      "1915\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# data set\n",
    "print(len(trn_sents))\n",
    "print(len(tst_sents))\n",
    "print(trn_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy & re-define feature extraction functions from the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:07:08.968623600Z",
     "start_time": "2023-06-03T19:07:08.922625600Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:07:09.910019300Z",
     "start_time": "2023-06-03T19:07:09.880021Z"
    }
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'bias': 1.0, 'word.lower()': word.lower()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our baseline features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:07:11.456229700Z",
     "start_time": "2023-06-03T19:07:11.399227900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'bias': 1.0, 'word.lower()': 'melbourne'}"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(trn_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:07:14.714108300Z",
     "start_time": "2023-06-03T19:07:13.928111200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 469 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_feats = [sent2features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T03:13:08.856415700Z",
     "start_time": "2023-06-03T03:13:08.812414500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T03:13:23.735533100Z",
     "start_time": "2023-06-03T03:13:10.014540400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.3 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T03:13:23.906533Z",
     "start_time": "2023-06-03T03:13:23.738533100Z"
    }
   },
   "outputs": [],
   "source": [
    "tst_feats = [sent2features(s) for s in tst_sents]\n",
    "pred = crf.predict(tst_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use our `conll` evaluation script. (Notice that tools report token level metrics.)\n",
    "\n",
    "For that we will need to modify our prediction output a bit, to make it a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T03:13:23.921528800Z",
     "start_time": "2023-06-03T03:13:23.908532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T03:13:25.427168Z",
     "start_time": "2023-06-03T03:13:25.393167400Z"
    }
   },
   "outputs": [],
   "source": [
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T03:13:26.533168200Z",
     "start_time": "2023-06-03T03:13:26.261167100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.859  0.375  0.522  1222\nORG    0.729  0.442  0.551  1700\nMISC   0.542  0.261  0.352   445\nLOC    0.699  0.713  0.706   985\ntotal  0.729  0.466  0.569  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.859</td>\n      <td>0.375</td>\n      <td>0.522</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.729</td>\n      <td>0.442</td>\n      <td>0.551</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.542</td>\n      <td>0.261</td>\n      <td>0.352</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.699</td>\n      <td>0.713</td>\n      <td>0.706</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.729</td>\n      <td>0.466</td>\n      <td>0.569</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of CRFs lies in its ability to make use of rich feature representation. The process of extracting features from raw data is know as [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering).\n",
    "\n",
    "Common features used in sequence labeling with CRFs are:\n",
    "- part-of-speech tags\n",
    "- lemmas\n",
    "- token character prefixes and suffixes (e.g. first and last 1, 2, 3 characters of a word; `word[-3:]` in tutorial is suffix of length 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. SpaCy Token Features\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a convenient way to augment our feature set with common features using in Natural Language Processing. \n",
    "\n",
    "The list of provided token-level features is available [here](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Adding Features to CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify `sent2features` function to make use of spaCy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:06:20.609054900Z",
     "start_time": "2023-06-03T19:06:16.237625800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_sm in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from es_core_news_sm) (3.5.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.10.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (65.6.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.24.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (8.1.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es_core_news_sm) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adnan\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "# get Spanish model of spacy\n",
    "!pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:07:34.226779500Z",
     "start_time": "2023-06-03T19:07:33.138778600Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:09:29.572186800Z",
     "start_time": "2023-06-03T19:07:34.896298300Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:09:29.592190100Z",
     "start_time": "2023-06-03T19:09:29.583187300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'melbourne', 'pos': 'PROPN', 'lemma': 'Melbourne'}, {'bias': 1.0, 'word.lower()': '(', 'pos': 'PUNCT', 'lemma': '('}, {'bias': 1.0, 'word.lower()': 'australia', 'pos': 'PROPN', 'lemma': 'Australia'}, {'bias': 1.0, 'word.lower()': ')', 'pos': 'PUNCT', 'lemma': ')'}, {'bias': 1.0, 'word.lower()': ',', 'pos': 'PUNCT', 'lemma': ','}, {'bias': 1.0, 'word.lower()': '25', 'pos': 'NUM', 'lemma': '25'}, {'bias': 1.0, 'word.lower()': 'may', 'pos': 'NOUN', 'lemma': 'may'}, {'bias': 1.0, 'word.lower()': '(', 'pos': 'PUNCT', 'lemma': '('}, {'bias': 1.0, 'word.lower()': 'efe', 'pos': 'PROPN', 'lemma': 'EFE'}, {'bias': 1.0, 'word.lower()': ')', 'pos': 'PUNCT', 'lemma': ')'}, {'bias': 1.0, 'word.lower()': '.', 'pos': 'PUNCT', 'lemma': '.'}]\n"
     ]
    }
   ],
   "source": [
    "print(trn_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:09:29.678187800Z",
     "start_time": "2023-06-03T19:09:29.592190100Z"
    }
   },
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:09:47.737006800Z",
     "start_time": "2023-06-03T19:09:29.609189300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:09:48.027729500Z",
     "start_time": "2023-06-03T19:09:47.742009200Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T19:09:48.404844900Z",
     "start_time": "2023-06-03T19:09:48.033728200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.820  0.649  0.725  1222\nORG    0.606  0.725  0.660  1700\nMISC   0.623  0.427  0.507   445\nLOC    0.723  0.735  0.729   985\ntotal  0.683  0.676  0.679  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.820</td>\n      <td>0.649</td>\n      <td>0.725</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.606</td>\n      <td>0.725</td>\n      <td>0.660</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.623</td>\n      <td>0.427</td>\n      <td>0.507</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.723</td>\n      <td>0.735</td>\n      <td>0.729</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.683</td>\n      <td>0.676</td>\n      <td>0.679</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exsecise is to experiment with a CRF model on NER task. You have to train and test a CRF with using different features on the conll2002 corpus (the same that we used here in the lab). \n",
    "\n",
    "The features that you have to experiment with are:\n",
    "- Baseline using the fetures in sent2spacy_features\n",
    "    - Train the model and print results on the test set\n",
    "- Add the \"suffix\" feature\n",
    "    - Train the model and print results on the test set\n",
    "- Add all the features used in the [tutorial on CoNLL dataset](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "    - Train the model and print results on the test set\n",
    "- Increase the feature window (number of previous and next token) to:\n",
    "    - `[-1, +1]`\n",
    "        - Train the model and print results on the test set\n",
    "    - `[-2, +2]`\n",
    "        - Train the model and print results on the test set\n",
    "\n",
    "        \n",
    "The format of the results has to be the table that we used so far, that is:\n",
    "```python\n",
    "results = evaluate(tst_sents, hyp)\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experimenting with a CRF model on NER task.\n",
    "Train and test a CRF with using different features on the conll2002 corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:29:10.343899300Z",
     "start_time": "2023-06-03T19:29:10.316866200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_sm in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from es_core_news_sm) (3.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (23.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.10.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.28.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es_core_news_sm) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adnan\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "# get Spanish model of spacy\n",
    "!pip install es_core_news_sm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:29:15.138363800Z",
     "start_time": "2023-06-03T19:29:12.090363600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Baseline using the features in sent2spacy_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import es_core_news_sm\n",
    "\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "\n",
    "    return feats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:33:21.235715900Z",
     "start_time": "2023-06-03T19:33:20.489720100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:33:21.879767700Z",
     "start_time": "2023-06-03T19:33:21.868765100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:34:28.876075300Z",
     "start_time": "2023-06-03T19:33:22.921935600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# train the model\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:34:32.631300700Z",
     "start_time": "2023-06-03T19:34:32.613302800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.8 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:34:46.860898900Z",
     "start_time": "2023-06-03T19:34:33.270902100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.820  0.649  0.725  1222\nORG    0.606  0.725  0.660  1700\nMISC   0.623  0.427  0.507   445\nLOC    0.723  0.735  0.729   985\ntotal  0.683  0.676  0.679  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.820</td>\n      <td>0.649</td>\n      <td>0.725</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.606</td>\n      <td>0.725</td>\n      <td>0.660</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.623</td>\n      <td>0.427</td>\n      <td>0.507</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.723</td>\n      <td>0.735</td>\n      <td>0.729</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.683</td>\n      <td>0.676</td>\n      <td>0.679</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:34:47.481899600Z",
     "start_time": "2023-06-03T19:34:46.864903800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Add the \"suffix\" feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1. Add the \"suffix\" feature to sent2spacy_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_,\n",
    "            'suffix': token.lower_[-3:]\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "\n",
    "    return feats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:35:34.180506300Z",
     "start_time": "2023-06-03T19:35:34.166507900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:36:39.097729400Z",
     "start_time": "2023-06-03T19:35:36.717726700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# train the model\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:36:59.128408300Z",
     "start_time": "2023-06-03T19:36:59.084565200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:37:14.782074300Z",
     "start_time": "2023-06-03T19:37:00.044951900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.827  0.679  0.746  1222\nORG    0.684  0.679  0.682  1700\nMISC   0.620  0.436  0.512   445\nLOC    0.656  0.768  0.707   985\ntotal  0.706  0.674  0.690  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.827</td>\n      <td>0.679</td>\n      <td>0.746</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.684</td>\n      <td>0.679</td>\n      <td>0.682</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.620</td>\n      <td>0.436</td>\n      <td>0.512</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.656</td>\n      <td>0.768</td>\n      <td>0.707</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.706</td>\n      <td>0.674</td>\n      <td>0.690</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:37:15.375075600Z",
     "start_time": "2023-06-03T19:37:14.786075700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Add all the features used in the tutorial on CoNLL dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "# define the features in sent2spacy_features\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'word[-3:]': token.lower_[-3:],\n",
    "            'word[-2:]': token.lower_[-2:],\n",
    "            'word.isupper()': token.is_upper,\n",
    "            'word.istitle()': token.is_title,\n",
    "            'word.isdigit()': token.is_digit,\n",
    "            'postag': token.pos_,\n",
    "            'postag[:2]': token.pos_[:2],\n",
    "            'lemma': token.lemma_,\n",
    "            'suffix': token.lower_[-1:],\n",
    "            'suffix2': token.lower_[-2:],\n",
    "            'suffix3': token.lower_[-3:]\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "\n",
    "    return feats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:45:08.614292100Z",
     "start_time": "2023-06-03T19:45:08.604204800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:46:14.698141200Z",
     "start_time": "2023-06-03T19:45:11.335142200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'bias': 1.0,\n  'word.lower()': 'melbourne',\n  'word[-3:]': 'rne',\n  'word[-2:]': 'ne',\n  'word.isupper()': False,\n  'word.istitle()': True,\n  'word.isdigit()': False,\n  'postag': 'PROPN',\n  'postag[:2]': 'PR',\n  'lemma': 'Melbourne',\n  'suffix': 'e',\n  'suffix2': 'ne',\n  'suffix3': 'rne'},\n {'bias': 1.0,\n  'word.lower()': '(',\n  'word[-3:]': '(',\n  'word[-2:]': '(',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': '(',\n  'suffix': '(',\n  'suffix2': '(',\n  'suffix3': '('},\n {'bias': 1.0,\n  'word.lower()': 'australia',\n  'word[-3:]': 'lia',\n  'word[-2:]': 'ia',\n  'word.isupper()': False,\n  'word.istitle()': True,\n  'word.isdigit()': False,\n  'postag': 'PROPN',\n  'postag[:2]': 'PR',\n  'lemma': 'Australia',\n  'suffix': 'a',\n  'suffix2': 'ia',\n  'suffix3': 'lia'},\n {'bias': 1.0,\n  'word.lower()': ')',\n  'word[-3:]': ')',\n  'word[-2:]': ')',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': ')',\n  'suffix': ')',\n  'suffix2': ')',\n  'suffix3': ')'},\n {'bias': 1.0,\n  'word.lower()': ',',\n  'word[-3:]': ',',\n  'word[-2:]': ',',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': ',',\n  'suffix': ',',\n  'suffix2': ',',\n  'suffix3': ','},\n {'bias': 1.0,\n  'word.lower()': '25',\n  'word[-3:]': '25',\n  'word[-2:]': '25',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': True,\n  'postag': 'NUM',\n  'postag[:2]': 'NU',\n  'lemma': '25',\n  'suffix': '5',\n  'suffix2': '25',\n  'suffix3': '25'},\n {'bias': 1.0,\n  'word.lower()': 'may',\n  'word[-3:]': 'may',\n  'word[-2:]': 'ay',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'NOUN',\n  'postag[:2]': 'NO',\n  'lemma': 'may',\n  'suffix': 'y',\n  'suffix2': 'ay',\n  'suffix3': 'may'},\n {'bias': 1.0,\n  'word.lower()': '(',\n  'word[-3:]': '(',\n  'word[-2:]': '(',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': '(',\n  'suffix': '(',\n  'suffix2': '(',\n  'suffix3': '('},\n {'bias': 1.0,\n  'word.lower()': 'efe',\n  'word[-3:]': 'efe',\n  'word[-2:]': 'fe',\n  'word.isupper()': True,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PROPN',\n  'postag[:2]': 'PR',\n  'lemma': 'EFE',\n  'suffix': 'e',\n  'suffix2': 'fe',\n  'suffix3': 'efe'},\n {'bias': 1.0,\n  'word.lower()': ')',\n  'word[-3:]': ')',\n  'word[-2:]': ')',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': ')',\n  'suffix': ')',\n  'suffix2': ')',\n  'suffix3': ')'},\n {'bias': 1.0,\n  'word.lower()': '.',\n  'word[-3:]': '.',\n  'word[-2:]': '.',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': '.',\n  'suffix': '.',\n  'suffix2': '.',\n  'suffix3': '.'}]"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_feats[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:46:18.884592600Z",
     "start_time": "2023-06-03T19:46:18.854585100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "# train the model\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:46:23.075866300Z",
     "start_time": "2023-06-03T19:46:23.052969700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.4 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:46:48.135973200Z",
     "start_time": "2023-06-03T19:46:24.833973800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:46:48.150401200Z",
     "start_time": "2023-06-03T19:46:48.136973800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.834  0.732  0.780  1222\nORG    0.751  0.688  0.718  1700\nMISC   0.588  0.490  0.534   445\nLOC    0.635  0.771  0.696   985\ntotal  0.725  0.699  0.712  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.834</td>\n      <td>0.732</td>\n      <td>0.780</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.751</td>\n      <td>0.688</td>\n      <td>0.718</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.588</td>\n      <td>0.490</td>\n      <td>0.534</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.635</td>\n      <td>0.771</td>\n      <td>0.696</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.725</td>\n      <td>0.699</td>\n      <td>0.712</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:46:48.830435900Z",
     "start_time": "2023-06-03T19:46:48.153405600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Increase the feature window (number of previous and next token) to: [-1, +1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "# define the features in sent2spacy_features\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for i, token in enumerate(spacy_sent):\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'word[-3:]': token.lower_[-3:],\n",
    "            'word[-2:]': token.lower_[-2:],\n",
    "            'word.isupper()': token.is_upper,\n",
    "            'word.istitle()': token.is_title,\n",
    "            'word.isdigit()': token.is_digit,\n",
    "            'postag': token.pos_,\n",
    "            'postag[:2]': token.pos_[:2],\n",
    "            'lemma': token.lemma_,\n",
    "            'suffix': token.lower_[-1:],\n",
    "            'suffix2': token.lower_[-2:],\n",
    "            'suffix3': token.lower_[-3:]\n",
    "        }\n",
    "        if i > 0:\n",
    "            token1 = spacy_sent[i-1]\n",
    "            token_feats.update({\n",
    "                '-1:word.lower()': token1.lower_,\n",
    "                '-1:word.istitle()': token1.is_title,\n",
    "                '-1:word.isupper()': token1.is_upper,\n",
    "                '-1:postag': token1.pos_,\n",
    "                '-1:postag[:2]': token1.pos_[:2],\n",
    "                '-1:lemma': token1.lemma_,\n",
    "                '-1:suffix': token1.lower_[-1:],\n",
    "                '-1:suffix2': token1.lower_[-2:],\n",
    "                '-1:suffix3': token1.lower_[-3:]\n",
    "            })\n",
    "        else:\n",
    "            token_feats['BOS'] = True\n",
    "\n",
    "        if i < len(spacy_sent)-1:\n",
    "            token1 = spacy_sent[i+1]\n",
    "            token_feats.update({\n",
    "                '+1:word.lower()': token1.lower_,\n",
    "                '+1:word.istitle()': token1.is_title,\n",
    "                '+1:word.isupper()': token1.is_upper,\n",
    "                '+1:postag': token1.pos_,\n",
    "                '+1:postag[:2]': token1.pos_[:2],\n",
    "                '+1:lemma': token1.lemma_,\n",
    "                '+1:suffix': token1.lower_[-1:],\n",
    "                '+1:suffix2': token1.lower_[-2:],\n",
    "                '+1:suffix3': token1.lower_[-3:]\n",
    "            })\n",
    "        else:\n",
    "            token_feats['EOS'] = True\n",
    "\n",
    "        feats.append(token_feats)\n",
    "\n",
    "    return feats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:48:26.098757200Z",
     "start_time": "2023-06-03T19:48:26.072719600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:50:02.385830400Z",
     "start_time": "2023-06-03T19:48:54.908823600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'bias': 1.0,\n  'word.lower()': 'melbourne',\n  'word[-3:]': 'rne',\n  'word[-2:]': 'ne',\n  'word.isupper()': False,\n  'word.istitle()': True,\n  'word.isdigit()': False,\n  'postag': 'PROPN',\n  'postag[:2]': 'PR',\n  'lemma': 'Melbourne',\n  'suffix': 'e',\n  'suffix2': 'ne',\n  'suffix3': 'rne',\n  'BOS': True,\n  '+1:word.lower()': '(',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PUNCT',\n  '+1:postag[:2]': 'PU',\n  '+1:lemma': '(',\n  '+1:suffix': '(',\n  '+1:suffix2': '(',\n  '+1:suffix3': '('},\n {'bias': 1.0,\n  'word.lower()': '(',\n  'word[-3:]': '(',\n  'word[-2:]': '(',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': '(',\n  'suffix': '(',\n  'suffix2': '(',\n  'suffix3': '(',\n  '-1:word.lower()': 'melbourne',\n  '-1:word.istitle()': True,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PROPN',\n  '-1:postag[:2]': 'PR',\n  '-1:lemma': 'Melbourne',\n  '-1:suffix': 'e',\n  '-1:suffix2': 'ne',\n  '-1:suffix3': 'rne',\n  '+1:word.lower()': 'australia',\n  '+1:word.istitle()': True,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PROPN',\n  '+1:postag[:2]': 'PR',\n  '+1:lemma': 'Australia',\n  '+1:suffix': 'a',\n  '+1:suffix2': 'ia',\n  '+1:suffix3': 'lia'},\n {'bias': 1.0,\n  'word.lower()': 'australia',\n  'word[-3:]': 'lia',\n  'word[-2:]': 'ia',\n  'word.isupper()': False,\n  'word.istitle()': True,\n  'word.isdigit()': False,\n  'postag': 'PROPN',\n  'postag[:2]': 'PR',\n  'lemma': 'Australia',\n  'suffix': 'a',\n  'suffix2': 'ia',\n  'suffix3': 'lia',\n  '-1:word.lower()': '(',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PUNCT',\n  '-1:postag[:2]': 'PU',\n  '-1:lemma': '(',\n  '-1:suffix': '(',\n  '-1:suffix2': '(',\n  '-1:suffix3': '(',\n  '+1:word.lower()': ')',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PUNCT',\n  '+1:postag[:2]': 'PU',\n  '+1:lemma': ')',\n  '+1:suffix': ')',\n  '+1:suffix2': ')',\n  '+1:suffix3': ')'},\n {'bias': 1.0,\n  'word.lower()': ')',\n  'word[-3:]': ')',\n  'word[-2:]': ')',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': ')',\n  'suffix': ')',\n  'suffix2': ')',\n  'suffix3': ')',\n  '-1:word.lower()': 'australia',\n  '-1:word.istitle()': True,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PROPN',\n  '-1:postag[:2]': 'PR',\n  '-1:lemma': 'Australia',\n  '-1:suffix': 'a',\n  '-1:suffix2': 'ia',\n  '-1:suffix3': 'lia',\n  '+1:word.lower()': ',',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PUNCT',\n  '+1:postag[:2]': 'PU',\n  '+1:lemma': ',',\n  '+1:suffix': ',',\n  '+1:suffix2': ',',\n  '+1:suffix3': ','},\n {'bias': 1.0,\n  'word.lower()': ',',\n  'word[-3:]': ',',\n  'word[-2:]': ',',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': ',',\n  'suffix': ',',\n  'suffix2': ',',\n  'suffix3': ',',\n  '-1:word.lower()': ')',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PUNCT',\n  '-1:postag[:2]': 'PU',\n  '-1:lemma': ')',\n  '-1:suffix': ')',\n  '-1:suffix2': ')',\n  '-1:suffix3': ')',\n  '+1:word.lower()': '25',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'NUM',\n  '+1:postag[:2]': 'NU',\n  '+1:lemma': '25',\n  '+1:suffix': '5',\n  '+1:suffix2': '25',\n  '+1:suffix3': '25'},\n {'bias': 1.0,\n  'word.lower()': '25',\n  'word[-3:]': '25',\n  'word[-2:]': '25',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': True,\n  'postag': 'NUM',\n  'postag[:2]': 'NU',\n  'lemma': '25',\n  'suffix': '5',\n  'suffix2': '25',\n  'suffix3': '25',\n  '-1:word.lower()': ',',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PUNCT',\n  '-1:postag[:2]': 'PU',\n  '-1:lemma': ',',\n  '-1:suffix': ',',\n  '-1:suffix2': ',',\n  '-1:suffix3': ',',\n  '+1:word.lower()': 'may',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'NOUN',\n  '+1:postag[:2]': 'NO',\n  '+1:lemma': 'may',\n  '+1:suffix': 'y',\n  '+1:suffix2': 'ay',\n  '+1:suffix3': 'may'},\n {'bias': 1.0,\n  'word.lower()': 'may',\n  'word[-3:]': 'may',\n  'word[-2:]': 'ay',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'NOUN',\n  'postag[:2]': 'NO',\n  'lemma': 'may',\n  'suffix': 'y',\n  'suffix2': 'ay',\n  'suffix3': 'may',\n  '-1:word.lower()': '25',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'NUM',\n  '-1:postag[:2]': 'NU',\n  '-1:lemma': '25',\n  '-1:suffix': '5',\n  '-1:suffix2': '25',\n  '-1:suffix3': '25',\n  '+1:word.lower()': '(',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PUNCT',\n  '+1:postag[:2]': 'PU',\n  '+1:lemma': '(',\n  '+1:suffix': '(',\n  '+1:suffix2': '(',\n  '+1:suffix3': '('},\n {'bias': 1.0,\n  'word.lower()': '(',\n  'word[-3:]': '(',\n  'word[-2:]': '(',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': '(',\n  'suffix': '(',\n  'suffix2': '(',\n  'suffix3': '(',\n  '-1:word.lower()': 'may',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'NOUN',\n  '-1:postag[:2]': 'NO',\n  '-1:lemma': 'may',\n  '-1:suffix': 'y',\n  '-1:suffix2': 'ay',\n  '-1:suffix3': 'may',\n  '+1:word.lower()': 'efe',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': True,\n  '+1:postag': 'PROPN',\n  '+1:postag[:2]': 'PR',\n  '+1:lemma': 'EFE',\n  '+1:suffix': 'e',\n  '+1:suffix2': 'fe',\n  '+1:suffix3': 'efe'},\n {'bias': 1.0,\n  'word.lower()': 'efe',\n  'word[-3:]': 'efe',\n  'word[-2:]': 'fe',\n  'word.isupper()': True,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PROPN',\n  'postag[:2]': 'PR',\n  'lemma': 'EFE',\n  'suffix': 'e',\n  'suffix2': 'fe',\n  'suffix3': 'efe',\n  '-1:word.lower()': '(',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PUNCT',\n  '-1:postag[:2]': 'PU',\n  '-1:lemma': '(',\n  '-1:suffix': '(',\n  '-1:suffix2': '(',\n  '-1:suffix3': '(',\n  '+1:word.lower()': ')',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PUNCT',\n  '+1:postag[:2]': 'PU',\n  '+1:lemma': ')',\n  '+1:suffix': ')',\n  '+1:suffix2': ')',\n  '+1:suffix3': ')'},\n {'bias': 1.0,\n  'word.lower()': ')',\n  'word[-3:]': ')',\n  'word[-2:]': ')',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': ')',\n  'suffix': ')',\n  'suffix2': ')',\n  'suffix3': ')',\n  '-1:word.lower()': 'efe',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': True,\n  '-1:postag': 'PROPN',\n  '-1:postag[:2]': 'PR',\n  '-1:lemma': 'EFE',\n  '-1:suffix': 'e',\n  '-1:suffix2': 'fe',\n  '-1:suffix3': 'efe',\n  '+1:word.lower()': '.',\n  '+1:word.istitle()': False,\n  '+1:word.isupper()': False,\n  '+1:postag': 'PUNCT',\n  '+1:postag[:2]': 'PU',\n  '+1:lemma': '.',\n  '+1:suffix': '.',\n  '+1:suffix2': '.',\n  '+1:suffix3': '.'},\n {'bias': 1.0,\n  'word.lower()': '.',\n  'word[-3:]': '.',\n  'word[-2:]': '.',\n  'word.isupper()': False,\n  'word.istitle()': False,\n  'word.isdigit()': False,\n  'postag': 'PUNCT',\n  'postag[:2]': 'PU',\n  'lemma': '.',\n  'suffix': '.',\n  'suffix2': '.',\n  'suffix3': '.',\n  '-1:word.lower()': ')',\n  '-1:word.istitle()': False,\n  '-1:word.isupper()': False,\n  '-1:postag': 'PUNCT',\n  '-1:postag[:2]': 'PU',\n  '-1:lemma': ')',\n  '-1:suffix': ')',\n  '-1:suffix2': ')',\n  '-1:suffix3': ')',\n  'EOS': True}]"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_feats[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:50:02.457834400Z",
     "start_time": "2023-06-03T19:50:02.389822300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# train the model\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:50:49.971529300Z",
     "start_time": "2023-06-03T19:50:49.938139400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 44.9 s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:51:37.812502900Z",
     "start_time": "2023-06-03T19:50:52.078613600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.874  0.787  0.828  1222\nORG    0.776  0.737  0.756  1700\nMISC   0.559  0.470  0.510   445\nLOC    0.649  0.797  0.716   985\ntotal  0.747  0.737  0.742  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.874</td>\n      <td>0.787</td>\n      <td>0.828</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.776</td>\n      <td>0.737</td>\n      <td>0.756</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.559</td>\n      <td>0.470</td>\n      <td>0.510</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.649</td>\n      <td>0.797</td>\n      <td>0.716</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.747</td>\n      <td>0.737</td>\n      <td>0.742</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:51:38.664500300Z",
     "start_time": "2023-06-03T19:51:37.816500700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Increase the feature window (number of previous and next token) to: [-2, +2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# define the features in sent2spacy_features\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for i, token in enumerate(spacy_sent):\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'word[-3:]': token.lower_[-3:],\n",
    "            'word[-2:]': token.lower_[-2:],\n",
    "            'word.isupper()': token.is_upper,\n",
    "            'word.istitle()': token.is_title,\n",
    "            'word.isdigit()': token.is_digit,\n",
    "            'postag': token.pos_,\n",
    "            'postag[:2]': token.pos_[:2],\n",
    "            'lemma': token.lemma_,\n",
    "            'suffix': token.lower_[-1:],\n",
    "            'suffix2': token.lower_[-2:],\n",
    "            'suffix3': token.lower_[-3:]\n",
    "        }\n",
    "        if i > 0:\n",
    "            token1 = spacy_sent[i-1]\n",
    "            token_feats.update({\n",
    "                '-1:word.lower()': token1.lower_,\n",
    "                '-1:word.istitle()': token1.is_title,\n",
    "                '-1:word.isupper()': token1.is_upper,\n",
    "                '-1:postag': token1.pos_,\n",
    "                '-1:postag[:2]': token1.pos_[:2],\n",
    "                '-1:lemma': token1.lemma_,\n",
    "                '-1:suffix': token1.lower_[-1:],\n",
    "                '-1:suffix2': token1.lower_[-2:],\n",
    "                '-1:suffix3': token1.lower_[-3:]\n",
    "            })\n",
    "        else:\n",
    "            token_feats['BOS'] = True\n",
    "\n",
    "        if i < len(spacy_sent)-1:\n",
    "            token1 = spacy_sent[i+1]\n",
    "            token_feats.update({\n",
    "                '+1:word.lower()': token1.lower_,\n",
    "                '+1:word.istitle()': token1.is_title,\n",
    "                '+1:word.isupper()': token1.is_upper,\n",
    "                '+1:postag': token1.pos_,\n",
    "                '+1:postag[:2]': token1.pos_[:2],\n",
    "                '+1:lemma': token1.lemma_,\n",
    "                '+1:suffix': token1.lower_[-1:],\n",
    "                '+1:suffix2': token1.lower_[-2:],\n",
    "                '+1:suffix3': token1.lower_[-3:]\n",
    "            })\n",
    "        else:\n",
    "            token_feats['EOS'] = True\n",
    "\n",
    "        if i > 1:\n",
    "            token1 = spacy_sent[i-2]\n",
    "            token_feats.update({\n",
    "                '-2:word.lower()': token1.lower_,\n",
    "                '-2:word.istitle()': token1.is_title,\n",
    "                '-2:word.isupper()': token1.is_upper,\n",
    "                '-2:postag': token1.pos_,\n",
    "                '-2:postag[:2]': token1.pos_[:2],\n",
    "                '-2:lemma': token1.lemma_,\n",
    "                '-2:suffix': token1.lower_[-1:],\n",
    "                '-2:suffix2': token1.lower_[-2:],\n",
    "                '-2:suffix3': token1.lower_[-3:]\n",
    "            })\n",
    "        else:\n",
    "            token_feats['BOS'] = True\n",
    "\n",
    "        if i < len(spacy_sent)-2:\n",
    "            token1 = spacy_sent[i+2]\n",
    "            token_feats.update({\n",
    "                '+2:word.lower()': token1.lower_,\n",
    "                '+2:word.istitle()': token1.is_title,\n",
    "                '+2:word.isupper()': token1.is_upper,\n",
    "                '+2:postag': token1.pos_,\n",
    "                '+2:postag[:2]': token1.pos_[:2],\n",
    "                '+2:lemma': token1.lemma_,\n",
    "                '+2:suffix': token1.lower_[-1:],\n",
    "                '+2:suffix2': token1.lower_[-2:],\n",
    "                '+2:suffix3': token1.lower_[-3:]\n",
    "            })\n",
    "        else:\n",
    "            token_feats['EOS'] = True\n",
    "\n",
    "        feats.append(token_feats)\n",
    "\n",
    "    return feats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:52:46.241572200Z",
     "start_time": "2023-06-03T19:52:46.206370800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:54:09.780742400Z",
     "start_time": "2023-06-03T19:53:00.284738300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "# train the model\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:54:09.796738100Z",
     "start_time": "2023-06-03T19:54:09.784738200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 11s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:55:21.967738800Z",
     "start_time": "2023-06-03T19:54:09.798738200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "           p      r      f     s\nPER    0.871  0.792  0.830  1222\nORG    0.781  0.747  0.763  1700\nMISC   0.543  0.472  0.505   445\nLOC    0.668  0.805  0.730   985\ntotal  0.751  0.745  0.748  4352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.871</td>\n      <td>0.792</td>\n      <td>0.830</td>\n      <td>1222</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.781</td>\n      <td>0.747</td>\n      <td>0.763</td>\n      <td>1700</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.543</td>\n      <td>0.472</td>\n      <td>0.505</td>\n      <td>445</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.668</td>\n      <td>0.805</td>\n      <td>0.730</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.751</td>\n      <td>0.745</td>\n      <td>0.748</td>\n      <td>4352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T19:55:23.185740600Z",
     "start_time": "2023-06-03T19:55:21.972746500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
