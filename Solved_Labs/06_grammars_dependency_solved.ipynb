{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dependency Grammars with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding:\n",
    "    - Dependency Relations and Grammars\n",
    "    - Probabilistic Dependency Grammars\n",
    "    - Projective and Non-Projective Parses\n",
    "    - Transition-based Dependency Parsing\n",
    "\n",
    "- Learning how to:\n",
    "    - define dependency grammar in NLTK\n",
    "    - identify a syntactic relation between Head and Dependent\n",
    "    - parse with dependency grammar\n",
    "    - evaluate dependency parser\n",
    "    - use dependency parser of spacy and stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Kübler, McDonald, and Nivre (2009) Dependency Parsing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 18: Dependency Parsing](https://web.stanford.edu/~jurafsky/slp3/18.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 8: Analyzing Sentence Structure](https://www.nltk.org/book/ch08.html)\n",
    "- Kübler, McDonald, and Nivre (2009) Dependency Parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English models\n",
    "- [stanza](https://stanfordnlp.github.io/stanza/) for Stanford Parser\n",
    "    - run `pip install stanza`\n",
    "    - run `stanza.download('en')` to intall English models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Dependency Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike Constituency (Phrase Structure) Grammar that addresses how words and sequences of words combine to form constituents, Dependency Grammar addresses on how words relate to each other. \n",
    "\n",
    "Dependency Grammar assumes that syntactic structure consists of words linked by binary, asymmetrical relations called __dependency relations__. A dependency relation is a binary asymmetric relation that holds between a syntactically subordinate word, called the __dependent__, and another word on which it depends, called the __head__.\n",
    "\n",
    "The __head of a sentence__ is usually taken to be the tensed verb, and every other word is either dependent on the sentence head, or connects to it through a path of dependencies. Thus, a dependency parse is a __directed graph__, where the nodes are the lexical items (words) and the arcs represent dependency relations from heads to dependents. \n",
    "\n",
    "A __typed dependency structure__ contains of the __labeled__ arcs are drawn from a fixed inventory of grammatical relations, that also includes a __root node__ that explicitly marks the root of the tree, the head of the entire structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Dependency Relation Types\n",
    "\n",
    "Universal Dependency set defines the following __core__ relations (from Jurafsky & Martin). \n",
    "\n",
    "(See https://universaldependencies.org/u/dep/index.html for the full set.)\n",
    "\n",
    "| __Clausal Argument Relations__ | Description | Example |\n",
    "|:-------------------------------|:------------|:--------\n",
    "| NSUBJ  | Nominal subject       | __We__ booked her the cheapest morning flight to Miami.\n",
    "| DOBJ   | Direct object         | We booked her the cheapest morning __flight__ to Miami.\n",
    "| IOBJ   | Indirect object       | We booked __her__ the cheapest morning flight to Miami.\n",
    "| CCOMP  | Clausal complement    |\n",
    "| XCOMP  | Open clausal complement (subject of clause is out of its span) \n",
    "| __Nominal Modifier Relations__ ||\n",
    "| NMOD   | Nominal modifier      | We booked her the cheapest __morning__ flight to Miami.\n",
    "| AMOD   | Adjectival modifier   | We booked her the __cheapest__ morning flight to Miami.\n",
    "| NUMMOD | Numeric modifier\n",
    "| APPOS  | Appositional modifier\n",
    "| DET    | Determiner            | We booked her __the__ cheapest morning flight to Miami.\n",
    "| CASE   | Prepositions, postpositions and other case markers | We booked her the cheapest morning flight __to__ Miami.\n",
    "| __Other Notable Relations__ | \n",
    "| CONJ   | Conjunct\n",
    "| CC     | Coordinating conjunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Defining Dependency Grammar in NLTK\n",
    "\n",
    "Similar to Phrase Structure Grammar, Dependecy Grammar is defined as a list of production rules.\n",
    "\n",
    "Below is an example grammar that defines only __bare__ dependency relations without specifying their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.336628Z",
     "start_time": "2023-07-16T09:34:04.322627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 9 productions\n",
      "  'saw' -> 'i'\n",
      "  'saw' -> 'man'\n",
      "  'saw' -> 'with'\n",
      "  'man' -> 'telescope'\n",
      "  'man' -> 'the'\n",
      "  'man' -> 'with'\n",
      "  'telescope' -> 'the'\n",
      "  'telescope' -> 'with'\n",
      "  'telescope' -> 'a'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "s_bold = '\\033[1m'\n",
    "e_bold = '\\033[0m'\n",
    "\n",
    "# for sentence \"i saw the man with the telescope\"\n",
    "# only string input is accepted\n",
    "\n",
    "rules = \"\"\"\n",
    "    'saw' -> 'i' | 'man' | 'with'\n",
    "    'man' ->  'telescope' | 'the' | 'with'\n",
    "    'telescope' -> 'the' | 'with' | 'a'\n",
    "\"\"\"\n",
    "\n",
    "toy_grammar = nltk.DependencyGrammar.fromstring(rules)\n",
    "\n",
    "print(toy_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike Phrase Structure Grammar, \n",
    "\n",
    "- there is no start symbol (thus, no method to access it)\n",
    "- there is no method to access productions, but it is still possible using the attribute\n",
    "\n",
    "    - `grammar._productions`\n",
    "\n",
    "- there is a method to check if grammar contains a production\n",
    "\n",
    "    - `grammar.contains(head, mod)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Dependency Production__ has 2 attributes:\n",
    "\n",
    "- `_lhs` (left-hand side) -- head\n",
    "- `_rhs` (right-hand side) -- modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.481624900Z",
     "start_time": "2023-07-16T09:34:07.342626900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saw' -> 'i', 'saw' -> 'man', 'saw' -> 'with', 'man' -> 'telescope', 'man' -> 'the', 'man' -> 'with', 'telescope' -> 'the', 'telescope' -> 'with', 'telescope' -> 'a']\n",
      "saw ('i',)\n",
      "saw ('man',)\n",
      "saw ('with',)\n",
      "man ('telescope',)\n",
      "man ('the',)\n",
      "man ('with',)\n",
      "telescope ('the',)\n",
      "telescope ('with',)\n",
      "telescope ('a',)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(toy_grammar._productions)\n",
    "\n",
    "for production in toy_grammar._productions:\n",
    "    print(production._lhs, production._rhs)\n",
    "\n",
    "print(toy_grammar.contains('man', 'the'))  # True\n",
    "print(toy_grammar.contains('the', 'man'))  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to Identify a Syntactic Relation between Head and Dependent\n",
    "(From Kübler et al. & NLTK Book)\n",
    "\n",
    "Here is a list of some of the more common criteria that have been proposed for identifying a syntactic relation between a head __H__ and a dependent __D__ in a linguistic construction __C__:\n",
    "\n",
    "1. __H__ determines the syntactic category of __C__ and can often replace __C__.\n",
    "2. __H__ determines the semantic category of __C__; __D__ gives semantic specification.\n",
    "3. __H__ is obligatory; __D__ may be optional.\n",
    "4. __H__ selects __D__ and determines whether __D__ is obligatory or optional. \n",
    "5. The form of __D__ depends on __H__ (agreement or government).\n",
    "6. The linear position of __D__ is specified with reference to __H__ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__:\n",
    "\n",
    "_I prefer a morning flight_\n",
    "\n",
    "- **C**: _morning flight_\n",
    "- **H**: _flight_\n",
    "    - determines syntactic category: whole construction is nominal\n",
    "    - determines semantic category\n",
    "    - comes after _morning_ (English is head final)\n",
    "- **D**: _morning_\n",
    "    - optional w.r.t. _flight_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Parsing with Dependency Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since Dependency Graphs can be projective and non-projective (allow crossing dependencies), there are __projective__ and __non-projective__ parsers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.1. Projective Dependency Parser (Rule-based)\n",
    ">**Definition**:\n",
    "A dependecy tree is projective if **all the arcs of the tree are projective**. An arc from a head to a dependent is said to be projective projective <mark style=\"background-color: rgba(0, 255, 0, 0.2)\"> if there is a path from the head to every word that lies between the head and the dependent </mark> in the sentence. *(Dan Jurafsky and James H. Martin, 2022)*\n",
    "\n",
    "> **NLTK**: A projective, rule-based, dependency parser. A [`ProjectiveDependencyParser`](http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.projectivedependencyparser) is created with a `DependencyGrammar`, a set of productions specifying word-to-word dependency relations. The `parse()` method will then return the set of all parses, in tree representation, for a given input sequence of tokens. \n",
    "`parse()` method returns iterator over [`Tree`](http://www.nltk.org/_modules/nltk/tree.html) objects.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.555629Z",
     "start_time": "2023-07-16T09:34:07.357625900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              saw                     \n",
      " ┌─────────────┴──────┐                   \n",
      " │                   man              \n",
      " │      ┌─────────────┼──────────┐        \n",
      " │      │             │      telescope\n",
      " │      │             │          │        \n",
      " i     the           with        a    \n",
      "\n",
      "None\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n",
      "              saw                            \n",
      " ┌─────────────┴──────┐                          \n",
      " │                   man                     \n",
      " │      ┌─────────────┴──────────┐               \n",
      " │      │                    telescope       \n",
      " │      │             ┌──────────┴─────────┐     \n",
      " i     the           with                  a \n",
      "\n",
      "None\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ProjectiveDependencyParser(toy_grammar)\n",
    "\n",
    "sent = \"i saw the man with a telescope\"\n",
    "\n",
    "for tree in parser.parse(sent.split()):\n",
    "    print(tree.pretty_print(unicodelines=True, nodedist=4))\n",
    "    # print ROOT node\n",
    "    print(\"The ROOT is '{}'\".format(s_bold + tree.label() + e_bold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.2. Non-Projective Dependency Parser (Rule-Based)\n",
    "\n",
    ">**Definition**:\n",
    "A dependecy tree is projective if **one or more arcs of the tree are non-projective**. An arc non-projective  when there some words that lies between the head and the dependent in the sentence do not have a path from the head.\n",
    "\n",
    "\n",
    "> A non-projective, rule-based, dependency parser. This parser will return the set of all possible non-projective parses based on the word-to-word relations defined in the parser’s dependency grammar, and <mark style=\"background-color: rgba(0, 255, 0, 0.2)\"> will allow the branches of the parse tree to cross</mark> in order to capture a variety of linguistic phenomena that a projective parser will not .\n",
    "\n",
    "`parse()` method returns iterator over [`DependencyGraph`](https://www.nltk.org/api/nltk.parse.html#nltk.parse.dependencygraph.DependencyGraph) objects. \n",
    "\n",
    "`tree()` method of the `DependencyGraph` object builds a dependency tree using the NLTK Tree constructor, starting with the `root` node and omitting labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.postimg.cc/hvQDRNDg/Screenshot-2022-12-19-at-17-22-53.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flight -> was **is not projective**, since *this morning* is not reachable from *flight*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.557625300Z",
     "start_time": "2023-07-16T09:34:07.386629800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              saw                    \n",
      " ┌─────────────┴─────────┐               \n",
      " │                      man          \n",
      " │                       │               \n",
      " │                   telescope       \n",
      " │      ┌────────────────┼─────────┐     \n",
      " i     the              with       a \n",
      "\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n",
      "               saw                           \n",
      " ┌──────────────┴──────┐                         \n",
      " │                    man                    \n",
      " │      ┌──────────────┴─────────┐               \n",
      " │      │                    telescope       \n",
      " │      │              ┌─────────┴─────────┐     \n",
      " i     with           the                  a \n",
      "\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n",
      "       saw                            \n",
      " ┌──────┼─────────────────┐               \n",
      " │      │                man          \n",
      " │      │                 │               \n",
      " │      │             telescope       \n",
      " │      │       ┌─────────┴─────────┐     \n",
      " i     with    the                  a \n",
      "\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n",
      "              saw                            \n",
      " ┌─────────────┴──────┐                          \n",
      " │                   man                     \n",
      " │      ┌─────────────┴──────────┐               \n",
      " │      │                    telescope       \n",
      " │      │             ┌──────────┴─────────┐     \n",
      " i     the           with                  a \n",
      "\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n",
      "              saw                     \n",
      " ┌─────────────┴──────┐                   \n",
      " │                   man              \n",
      " │      ┌─────────────┼──────────┐        \n",
      " │      │             │      telescope\n",
      " │      │             │          │        \n",
      " i     the           with        a    \n",
      "\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n",
      "       saw                            \n",
      " ┌──────┼──────────────┐                  \n",
      " │      │             man             \n",
      " │      │       ┌──────┴─────────┐        \n",
      " │      │       │            telescope\n",
      " │      │       │                │        \n",
      " i     with    the               a    \n",
      "\n",
      "The ROOT is '\u001B[1msaw\u001B[0m'\n"
     ]
    }
   ],
   "source": [
    "np_parser = nltk.NonprojectiveDependencyParser(toy_grammar)\n",
    "\n",
    "for graph in np_parser.parse(sent.split()):\n",
    "    graph.tree().pretty_print(unicodelines=True, nodedist=4)\n",
    "    print(\"The ROOT is '{}'\".format(s_bold + graph.root['word'] + e_bold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the sentence is ambiguous, similar to Phrase Structure Grammar, our Dependency Grammar yields 2 parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.3. Accessing the Graph\n",
    "\n",
    "- `DependencyGraph` object has 2 attrubutes\n",
    "    - nodes (of `defaultdict` type)\n",
    "    - root (of `dict` type), which is also a node\n",
    "\n",
    "- Each node in a graph is represented as a dict that defines its:\n",
    "    - address (sentence index starting from 1) -- required\n",
    "    - word (string form) -- required\n",
    "    - head (address)\n",
    "    - deps (dependents)\n",
    "    - rel (dependency relation to head)\n",
    "\n",
    "Thus, we can print the graph as a list of tokens with their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.558628800Z",
     "start_time": "2023-07-16T09:34:07.403630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 saw\n",
      "2 saw\n",
      "2 saw\n",
      "2 saw\n",
      "2 saw\n",
      "2 saw\n"
     ]
    }
   ],
   "source": [
    "# printing root address and word\n",
    "for graph in np_parser.parse(sent.split()): \n",
    "    print(graph.root['address'], graph.root['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.559623900Z",
     "start_time": "2023-07-16T09:34:07.415627500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\ti:\t[]\n",
      "2\tsaw:\t[1, 4]\n",
      "3\tthe:\t[]\n",
      "4\tman:\t[7]\n",
      "5\twith:\t[]\n",
      "6\ta:\t[]\n",
      "7\ttelescope:\t[3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# printing all the nodes with dependent positions\n",
    "for graph in np_parser.parse(sent.split()):    \n",
    "    # sorting is required since graph starts from root, which is not the first token\n",
    "    for _, node in sorted(graph.nodes.items()):\n",
    "        if node['word'] is not None:\n",
    "            print('{address}\\t{word}:\\t{dependents}'.format(dependents=node['deps'][''], **node))\n",
    "    break  # just to print 1 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to convert the graph into other supported formats, such as CoNLL using `to_conll(style)` method, where [style](https://www.nltk.org/api/nltk.parse.html#nltk.parse.dependencygraph.DependencyGraph) is either 3, 4, or 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tNone\tNone\n",
      "saw\tNone\tNone\n",
      "the\tNone\tNone\n",
      "man\tNone\tNone\n",
      "with\tNone\tNone\n",
      "a\tNone\tNone\n",
      "telescope\tNone\tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for graph in np_parser.parse(sent.split()):\n",
    "    print(graph.to_conll(3))\n",
    "    break  # just to print 1 graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.575627400Z",
     "start_time": "2023-07-16T09:34:07.442623900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.575627400Z",
     "start_time": "2023-07-16T09:34:07.465626800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Define grammar that covers the following sentences.\n",
    "\n",
    "    - show flights from new york to los angeles\n",
    "    - list flights from new york to los angeles\n",
    "    - show flights from new york\n",
    "    - list flights to los angeles\n",
    "    - list flights\n",
    "    \n",
    "- Use one of the parsers to parse the sentences (i.e. test your grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:07.830625900Z",
     "start_time": "2023-07-16T09:34:07.485625800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: show flights from new york to los angeles\n",
      "           show                   \n",
      "   ┌────────┴───────┐                 \n",
      "   │               from           \n",
      "   │                │                 \n",
      "   │               new            \n",
      "   │                │                 \n",
      "   │               york           \n",
      "   │                │                 \n",
      "   │                to            \n",
      "   │        ┌───────┴─────────┐       \n",
      "flights    los             angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "                  show                           \n",
      "   ┌───────────────┴───────┐                         \n",
      "   │                      from                   \n",
      "   │        ┌──────────────┴───────┐                 \n",
      "   │        │                     york           \n",
      "   │        │                      │                 \n",
      "   │        │                      to            \n",
      "   │        │              ┌───────┴─────────┐       \n",
      "flights    new            los             angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "           show           \n",
      "   ┌────────┼─────────┐       \n",
      "   │       from       to  \n",
      "   │        │         │       \n",
      "   │       new       los  \n",
      "   │        │         │       \n",
      "flights    york    angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "                          show                         \n",
      "   ┌───────────────┬───────┴──────────────┐                \n",
      "   │              from                    to           \n",
      "   │        ┌──────┴───────┐       ┌──────┴────────┐       \n",
      "flights    new            york    los           angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "           show                   \n",
      "   ┌────────┴───────┐                 \n",
      "   │               from           \n",
      "   │        ┌───────┴─────────┐       \n",
      "   │        │                york \n",
      "   │        │                 │       \n",
      "   │        │                 to  \n",
      "   │        │                 │       \n",
      "   │        │                los  \n",
      "   │        │                 │       \n",
      "flights    new             angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "           show                         \n",
      "   ┌────────┼──────────────┐                \n",
      "   │       from            │            \n",
      "   │        │              │                \n",
      "   │       new             to           \n",
      "   │        │       ┌──────┴────────┐       \n",
      "flights    york    los           angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "           show           \n",
      "   ┌────────┴─────────┐       \n",
      "   │                 from \n",
      "   │                  │       \n",
      "   │                 new  \n",
      "   │                  │       \n",
      "   │                 york \n",
      "   │                  │       \n",
      "   │                  to  \n",
      "   │                  │       \n",
      "   │                 los  \n",
      "   │                  │       \n",
      "flights            angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york to los angeles\n",
      "           show                           \n",
      "   ┌────────┴───────┬─────────────────┐       \n",
      "   │                │                 to  \n",
      "   │                │                 │       \n",
      "   │               from              los  \n",
      "   │        ┌───────┴───────┐         │       \n",
      "flights    new             york    angeles\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york\n",
      "           show                \n",
      "   ┌────────┴───────┐              \n",
      "   │               from        \n",
      "   │        ┌───────┴───────┐      \n",
      "flights    new             york\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: show flights from new york\n",
      "           show        \n",
      "   ┌────────┴───────┐      \n",
      "   │               from\n",
      "   │                │      \n",
      "   │               new \n",
      "   │                │      \n",
      "flights            york\n",
      "\n",
      "The ROOT is '\u001B[1mshow\u001B[0m' \n",
      "\n",
      "Sentence: list flights\n",
      "flights\n",
      "   │       \n",
      "  list \n",
      "\n",
      "The ROOT is '\u001B[1mflights\u001B[0m' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"show flights from new york to los angeles\",\n",
    "            \"list flights from new york to los angeles\",\n",
    "            \"show flights from new york\",\n",
    "            \"list flights to los angeles\",\n",
    "            \"list flights\"]\n",
    "\n",
    "# Follow the logic described in How to Identify a Syntactic Relation between Head and Dependent. \n",
    "# The tensed verb is the root. \n",
    "# Are prepositions heads or dependants?  \n",
    "\n",
    "rules = \"\"\"\n",
    "    'show' -> 'flights' | 'from' | 'to'\n",
    "    'flights' -> 'list'\n",
    "    'from' -> 'new' | 'york'\n",
    "    'new' -> 'york'\n",
    "    'york' -> 'to'\n",
    "    'to' -> 'los' | 'angeles'\n",
    "    'los' -> 'angeles'\n",
    "\"\"\"\n",
    "\n",
    "toy_grammar = nltk.DependencyGrammar.fromstring(rules)\n",
    "\n",
    "np_parser = nltk.ProjectiveDependencyParser(toy_grammar)\n",
    "\n",
    "for sent in sentences:\n",
    "    for graph in np_parser.parse(sent.split()):\n",
    "        print(\"Sentence:\", sent)\n",
    "\n",
    "        if type(graph) != nltk.tree.Tree:\n",
    "            graph.tree().pretty_print(unicodelines=True, nodedist=4)\n",
    "            print(\"The ROOT is '{}'\".format(s_bold + graph.root['word'] + e_bold), '\\n')\n",
    "        else:\n",
    "            graph.pretty_print(unicodelines=True, nodedist=4)\n",
    "            print(\"The ROOT is '{}'\".format(s_bold + graph.label() + e_bold), '\\n')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Probabilistic Dependency Grammars & Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similar to CFGs, we can learn dependency grammar from data using treebanks.\n",
    "NLTK provides `ProbabilisticProjectiveDependencyParser` that returns the most probable projective parse derived from the probabilistic dependency grammar derived from the `train()` method. \n",
    "\n",
    "> The probabilistic model is an implementation of Eisner's (1996) Model C, which conditions on head-word, head-tag, child-word, and child-tag. The decoding uses a bottom-up chart-based span concatenation algorithm that's identical to the one utilized by the rule-based projective parser.\n",
    "\n",
    "Without going into details, that is an example of Dynamic Programming approach to Dependency Parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:08.145624100Z",
     "start_time": "2023-07-16T09:34:07.512626800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     C:\\Users\\adnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading treebank\n",
    "import nltk\n",
    "nltk.download('dependency_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-22T17:22:42.744723400Z",
     "start_time": "2023-07-22T17:22:39.767095300Z"
    }
   },
   "outputs": [],
   "source": [
    "# example from NLTK\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from nltk.parse import ProbabilisticProjectiveDependencyParser\n",
    "from nltk.corpus import dependency_treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tPierre\tPierre\tNNP\tNNP\t\t2\t\t_\t_\n",
      "2\tVinken\tVinken\tNNP\tNNP\t\t8\t\t_\t_\n",
      "3\t,\t,\t,\t,\t\t2\t\t_\t_\n",
      "4\t61\t61\tCD\tCD\t\t5\t\t_\t_\n",
      "5\tyears\tyears\tNNS\tNNS\t\t6\t\t_\t_\n",
      "6\told\told\tJJ\tJJ\t\t2\t\t_\t_\n",
      "7\t,\t,\t,\t,\t\t2\t\t_\t_\n",
      "8\twill\twill\tMD\tMD\t\t0\t\t_\t_\n",
      "9\tjoin\tjoin\tVB\tVB\t\t8\t\t_\t_\n",
      "10\tthe\tthe\tDT\tDT\t\t11\t\t_\t_\n",
      "11\tboard\tboard\tNN\tNN\t\t9\t\t_\t_\n",
      "12\tas\tas\tIN\tIN\t\t9\t\t_\t_\n",
      "13\ta\ta\tDT\tDT\t\t15\t\t_\t_\n",
      "14\tnonexecutive\tnonexecutive\tJJ\tJJ\t\t15\t\t_\t_\n",
      "15\tdirector\tdirector\tNN\tNN\t\t12\t\t_\t_\n",
      "16\tNov.\tNov.\tNNP\tNNP\t\t9\t\t_\t_\n",
      "17\t29\t29\tCD\tCD\t\t16\t\t_\t_\n",
      "18\t.\t.\t.\t.\t\t8\t\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dependency graph in CoNLL format\n",
    "print(dependency_treebank.parsed_sents()[0].to_conll(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:22:47.765121100Z",
     "start_time": "2023-07-22T17:22:46.199265800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ppdp = ProbabilisticProjectiveDependencyParser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:22:48.564932700Z",
     "start_time": "2023-07-22T17:22:48.525935500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# train parser on graphs\n",
    "ppdp.train(dependency_treebank.parsed_sents())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:22:53.235982200Z",
     "start_time": "2023-07-22T17:22:50.359980700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fell (stock (of (price the)) the))\n",
      "(fell (stock (of (price the) the)))\n",
      "(fell (stock (of the price) the))\n",
      "(fell (stock (of the price the)))\n",
      "(fell (stock the (of price) the))\n",
      "(fell (stock the (of price the)))\n"
     ]
    }
   ],
   "source": [
    "# parse the sentence\n",
    "parse = ppdp.parse(['the', 'price', 'of', 'the', 'stock', 'fell'])\n",
    "\n",
    "# returns set of trees ordered by probability score\n",
    "for tree in parse:\n",
    "    # print(tree.pretty_print(unicodelines=True, nodedist=4))\n",
    "    print(tree)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:22:57.963499500Z",
     "start_time": "2023-07-22T17:22:54.903536800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-07-22T17:22:58.009499900Z",
     "start_time": "2023-07-22T17:22:57.966500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function that given a dependency graph, for each token (word), produces list of words from it to ROOT.\n",
    "\n",
    "(Construct normal `dict` for simplicity first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-22T17:23:03.831751700Z",
     "start_time": "2023-07-22T17:23:01.741717300Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# With .nodes we get a dict\n",
    "dg_tree = dependency_treebank.parsed_sents()[0].tree()\n",
    "dg = dependency_treebank.parsed_sents()[0].nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x000001DEC97E0C10>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [8]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'Pierre',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Pierre'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [1, 3, 6, 7]}),\n",
      "                 'feats': '',\n",
      "                 'head': 8,\n",
      "                 'lemma': 'Vinken',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Vinken'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': ',',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': ',',\n",
      "                 'rel': '',\n",
      "                 'tag': ',',\n",
      "                 'word': ','},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'CD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 5,\n",
      "                 'lemma': '61',\n",
      "                 'rel': '',\n",
      "                 'tag': 'CD',\n",
      "                 'word': '61'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'NNS',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [4]}),\n",
      "                 'feats': '',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'years',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNS',\n",
      "                 'word': 'years'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [5]}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'old',\n",
      "                 'rel': '',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'old'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': ',',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': ',',\n",
      "                 'rel': '',\n",
      "                 'tag': ',',\n",
      "                 'word': ','},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'MD',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [2, 9, 18]}),\n",
      "                 'feats': '',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'will',\n",
      "                 'rel': '',\n",
      "                 'tag': 'MD',\n",
      "                 'word': 'will'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'VB',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [11, 12, 16]}),\n",
      "                 'feats': '',\n",
      "                 'head': 8,\n",
      "                 'lemma': 'join',\n",
      "                 'rel': '',\n",
      "                 'tag': 'VB',\n",
      "                 'word': 'join'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'the',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [10]}),\n",
      "                  'feats': '',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'board',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'board'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [15]}),\n",
      "                  'feats': '',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'as',\n",
      "                  'rel': '',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'as'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 15,\n",
      "                  'lemma': 'a',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 15,\n",
      "                  'lemma': 'nonexecutive',\n",
      "                  'rel': '',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'nonexecutive'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [13, 14]}),\n",
      "                  'feats': '',\n",
      "                  'head': 12,\n",
      "                  'lemma': 'director',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'director'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [17]}),\n",
      "                  'feats': '',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'Nov.',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Nov.'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'CD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 16,\n",
      "                  'lemma': '29',\n",
      "                  'rel': '',\n",
      "                  'tag': 'CD',\n",
      "                  'word': '29'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': '.',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 8,\n",
      "                  'lemma': '.',\n",
      "                  'rel': '',\n",
      "                  'tag': '.',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "# Let's print to see what it contains\n",
    "pprint(dg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:23:05.879307600Z",
     "start_time": "2023-07-22T17:23:05.791309800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   will                                                     \n",
      " ┌─────────────────┬────────────────┴────────────────┐                                          \n",
      " │               Vinken                             join                                    \n",
      " │       ┌─────────┼────────┬───────┐        ┌───────┴─────────┬─────────────────────────┐      \n",
      " │       │         │        │      old       │                 as                        │  \n",
      " │       │         │        │       │        │                 │                         │      \n",
      " │       │         │        │     years    board            director                    Nov.\n",
      " │       │         │        │       │        │       ┌─────────┴─────────────┐           │      \n",
      " .     Pierre      ,        ,       61      the      a                  nonexecutive     29 \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dg_tree.pretty_print(unicodelines=True, nodedist=4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:18.900625600Z",
     "start_time": "2023-07-16T09:34:18.887630800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre : ['Pierre', 'Vinken', 'will']\n",
      "Vinken : ['Vinken', 'will']\n",
      ", : [',', 'Vinken', 'will']\n",
      "61 : ['61', 'years', 'old', 'Vinken', 'will']\n",
      "years : ['years', 'old', 'Vinken', 'will']\n",
      "old : ['old', 'Vinken', 'will']\n",
      ", : [',', 'Vinken', 'will']\n",
      "will : ['will']\n",
      "join : ['join', 'will']\n",
      "the : ['the', 'board', 'join', 'will']\n",
      "board : ['board', 'join', 'will']\n",
      "as : ['as', 'join', 'will']\n",
      "a : ['a', 'director', 'as', 'join', 'will']\n",
      "nonexecutive : ['nonexecutive', 'director', 'as', 'join', 'will']\n",
      "director : ['director', 'as', 'join', 'will']\n",
      "Nov. : ['Nov.', 'join', 'will']\n",
      "29 : ['29', 'Nov.', 'join', 'will']\n",
      ". : ['.', 'will']\n"
     ]
    }
   ],
   "source": [
    "# The first element is the root, sentence starts from the second element in the dict\n",
    "def go_to_root(token, head, dg):\n",
    "    path = list()\n",
    "\n",
    "    # We start from the token\n",
    "    path.append(token)\n",
    "\n",
    "    # We go up until we reach the root\n",
    "    while head != 0:\n",
    "        # We add the head to the path\n",
    "        path.append(dg[head]['word'])\n",
    "        # We update the head\n",
    "        head = dg[head]['head']\n",
    "\n",
    "    # From token to the root\n",
    "    return path\n",
    "\n",
    "for k, v in sorted(dg.items()):\n",
    "    if k != 0:\n",
    "        print(v['word'],\":\" ,go_to_root(v['word'], v['head'], dg))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:19.003623500Z",
     "start_time": "2023-07-16T09:34:18.903626800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are several methods for data-driven dependency parsing\n",
    "- Dynamic Programming-based (e.g. `ProbabilisticProjectiveDependencyParser`)\n",
    "- Graph-based (e.g. [Minimum Spanning Tree Parser](https://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html))\n",
    "- Transition-Based Dependency Parsing (e.g. NLTK's interface to [MaltParser](https://www.nltk.org/_modules/nltk/parse/malt.html))\n",
    "\n",
    "Transition-based parsing (or \"deterministic dependency parsing\") proved to be very effective and is the State-of-the-Art approach (with neural twist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(from Jurafsky & Martin)\n",
    "\n",
    "In transition-based parsing there is:\n",
    "- a **stack** on which we build the parse\n",
    "- a **buffer** of tokens to be parsed\n",
    "- a **parser** which takes actions on the parse via a predictor called an **oracle**\n",
    "\n",
    "The parser walks through the sentence left-to-right, successively shifting items from the buffer onto the stack. At each time point we examine the top two elements on the stack, and the oracle makes a decision about what transition to apply to build the parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arc-Standard Transition System (there are alternatives, i.e. Arc Eager) defines 3 transition operators that will operate on the top two elements of the stack:\n",
    "\n",
    "- __LEFTARC__: \n",
    "    - Assert a head-dependent relation between the word at the top of the stack and the word directly beneath it;\n",
    "    - Remove the lower word from the stack.\n",
    "- __RIGHTARC__: \n",
    "    - Assert a head-dependent relation between the second word on the stack and the word at the top; \n",
    "    - Remove the word at the top of the stack;\n",
    "- __SHIFT__: \n",
    "    - Remove the word from the front of the input buffer and push it onto the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally fits into Machine Learning framework where \n",
    "- configuration (buffer & stack) are features, \n",
    "- operations are labels\n",
    "- oracle is a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transition Parser in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:35.630604400Z",
     "start_time": "2023-07-16T09:34:18.919624800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "[LibSVM]<nltk.parse.transitionparser.TransitionParser object at 0x0000021DA5B50C40>\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.transitionparser import TransitionParser\n",
    "\n",
    "tp = TransitionParser('arc-standard')\n",
    "tp.train(dependency_treebank.parsed_sents()[:100], 'tp.model')\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:37.654468Z",
     "start_time": "2023-07-16T09:34:35.634611700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x0000021DABB8E8B0>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': 0,\n",
      "                 'lemma': None,\n",
      "                 'rel': '',\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'A',\n",
      "                 'rel': '',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'A'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'White',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'White'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'House',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'House'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [1, 2, 3]}),\n",
      "                 'feats': '',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'spokesman',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'spokesman'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [4, 7, 8, 31]}),\n",
      "                 'feats': '',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'said',\n",
      "                 'rel': '',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'said'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 7,\n",
      "                 'lemma': 'last',\n",
      "                 'rel': '',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'last'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [6]}),\n",
      "                 'feats': '',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'week',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'week'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [11]}),\n",
      "                 'feats': '',\n",
      "                 'head': 7,\n",
      "                 'lemma': 'that',\n",
      "                 'rel': '',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'that'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 10,\n",
      "                 'lemma': 'the',\n",
      "                 'rel': '',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [9]}),\n",
      "                  'feats': '',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'president',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'president'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'VBZ',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [10, 12]}),\n",
      "                  'feats': '',\n",
      "                  'head': 8,\n",
      "                  'lemma': 'is',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBZ',\n",
      "                  'word': 'is'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'VBG',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [13]}),\n",
      "                  'feats': '',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'considering',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBG',\n",
      "                  'word': 'considering'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'VBG',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [14, 27]}),\n",
      "                  'feats': '',\n",
      "                  'head': 12,\n",
      "                  'lemma': 'declaring',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBG',\n",
      "                  'word': 'declaring'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [18]}),\n",
      "                  'feats': '',\n",
      "                  'head': 13,\n",
      "                  'lemma': 'that',\n",
      "                  'rel': '',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'that'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 16,\n",
      "                  'lemma': 'the',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [15]}),\n",
      "                  'feats': '',\n",
      "                  'head': 18,\n",
      "                  'lemma': 'Constitution',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Constitution'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'RB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 18,\n",
      "                  'lemma': 'implicitly',\n",
      "                  'rel': '',\n",
      "                  'tag': 'RB',\n",
      "                  'word': 'implicitly'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': 'VBZ',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [16, 17, 19, 21]}),\n",
      "                  'feats': '',\n",
      "                  'head': 14,\n",
      "                  'lemma': 'gives',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBZ',\n",
      "                  'word': 'gives'},\n",
      "             19: {'address': 19,\n",
      "                  'ctag': 'PRP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'him',\n",
      "                  'rel': '',\n",
      "                  'tag': 'PRP',\n",
      "                  'word': 'him'},\n",
      "             20: {'address': 20,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'the',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             21: {'address': 21,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [20, 22]}),\n",
      "                  'feats': '',\n",
      "                  'head': 18,\n",
      "                  'lemma': 'authority',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'authority'},\n",
      "             22: {'address': 22,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [25]}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'for',\n",
      "                  'rel': '',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'for'},\n",
      "             23: {'address': 23,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 25,\n",
      "                  'lemma': 'a',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             24: {'address': 24,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 25,\n",
      "                  'lemma': 'line-item',\n",
      "                  'rel': '',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'line-item'},\n",
      "             25: {'address': 25,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [23, 24]}),\n",
      "                  'feats': '',\n",
      "                  'head': 22,\n",
      "                  'lemma': 'veto',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'veto'},\n",
      "             26: {'address': 26,\n",
      "                  'ctag': 'TO',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 27,\n",
      "                  'lemma': 'to',\n",
      "                  'rel': '',\n",
      "                  'tag': 'TO',\n",
      "                  'word': 'to'},\n",
      "             27: {'address': 27,\n",
      "                  'ctag': 'VB',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [26, 30]}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'provoke',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VB',\n",
      "                  'word': 'provoke'},\n",
      "             28: {'address': 28,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 30,\n",
      "                  'lemma': 'a',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             29: {'address': 29,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 30,\n",
      "                  'lemma': 'test',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'test'},\n",
      "             30: {'address': 30,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [28, 29]}),\n",
      "                  'feats': '',\n",
      "                  'head': 27,\n",
      "                  'lemma': 'case',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'case'},\n",
      "             31: {'address': 31,\n",
      "                  'ctag': '.',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 5,\n",
      "                  'lemma': '.',\n",
      "                  'rel': '',\n",
      "                  'tag': '.',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-10:], 'tp.model')\n",
    "print(len(parses))\n",
    "print(parses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Evaluation of Dependency Parsing\n",
    "\n",
    "Dependency Parsing performance is evaluated as __labeled__ and __unlabeled attachment scores__ which are calculated as \n",
    "\n",
    "$$ UAS/LAS = \\frac{\\text{# of corrent dependency relations}}{\\text{# of dependency relations}}$$\n",
    "\n",
    "the difference between the two is whether the relation labels are considered or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLTK provides `DependencyEvaluator` class to perform the evaluation, that takes predicted and reference parses as arguments. The evaluation ignores punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:34:38.753466300Z",
     "start_time": "2023-07-16T09:34:37.655468900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7875\n",
      "0.7875\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse import DependencyEvaluator\n",
    "\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-10:])\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(las)\n",
    "print(uas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Train `arc-standard` and `arc-eager` transition parsers on the same portion (slightly bigger than 100, otherwise it takes a lot of time)\n",
    "- Evaluate both of them comparing the attachment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:35:57.154499Z",
     "start_time": "2023-07-16T09:34:38.758466400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 250\n",
      " Number of valid (projective) examples : 250\n",
      "0.8190304125263475\n",
      "0.8190304125263475\n"
     ]
    }
   ],
   "source": [
    "tp = TransitionParser('arc-standard')\n",
    "n_training = 250\n",
    "tp.train(dependency_treebank.parsed_sents()[:n_training], 'tp.model', verbose=False)\n",
    "\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-150:], 'tp.model')\n",
    "\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-150:])\n",
    "arc_standard_las, arc_standard_uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(arc_standard_las)\n",
    "print(arc_standard_uas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 250\n",
      " Number of valid (projective) examples : 250\n",
      "0.8175248419150858\n",
      "0.8175248419150858\n"
     ]
    }
   ],
   "source": [
    "tp = TransitionParser('arc-eager')\n",
    "n_training = 250\n",
    "tp.train(dependency_treebank.parsed_sents()[:n_training], 'tp.model', verbose=False)\n",
    "\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-150:], 'tp.model')\n",
    "\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-150:])\n",
    "\n",
    "arc_eager_las, arc_eager_uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(arc_eager_las)\n",
    "print(arc_eager_uas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:37:44.277494600Z",
     "start_time": "2023-07-16T09:35:57.134464100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:45.030860600Z",
     "start_time": "2023-07-16T09:37:44.275499100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arc-standard LAS:  0.8190304125263475\n",
      "Arc-standard UAS:  0.8190304125263475\n",
      "\n",
      "Arc-eager LAS:  0.8175248419150858\n",
      "Arc-eager UAS:  0.8175248419150858\n",
      "Arc-standard LAS:  0.8190304125263475\n",
      "Arc-standard UAS:  0.8190304125263475\n",
      "\n",
      "Arc-eager LAS:  0.8175248419150858\n",
      "Arc-eager UAS:  0.8175248419150858\n"
     ]
    }
   ],
   "source": [
    "# Print Arc-standard results\n",
    "print(\"Arc-standard LAS: \", arc_standard_las)\n",
    "print(\"Arc-standard UAS: \", arc_standard_uas)\n",
    "print()\n",
    "# Print Acr-eager results\n",
    "print(\"Arc-eager LAS: \", arc_eager_las)\n",
    "print(\"Arc-eager UAS: \", arc_eager_uas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Dependency Parsing with Stanza and Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both spaCy and Stanza (python package Stanford NLP Tools) provide pre-trained dependency parsing models.\n",
    "The libraries are quite similar in usage.\n",
    "\n",
    "- initialize pipeline (with other processing steps such as tokenization, POS-tagging)\n",
    "- process a sentence \n",
    "- iterate over tokens accessing dependency parsing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Stanford Dependency Parser\n",
    "\n",
    "[paper on stanza](https://arxiv.org/pdf/2003.07082.pdf)\n",
    "\n",
    "A neural graph-based dependency parser. \n",
    "[paper on parser](https://nlp.stanford.edu/pubs/dozat2017deep.pdf)\n",
    "\n",
    "> We implement a Bi-LSTM-based deep biaffine neural dependency parser (Dozat and Manning, 2017). We\n",
    "further augment this model with two linguistically motivated features: one that predicts the linearization order of two words in a given language, and the other that predicts the typical distance in linear order between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2. Spacy Dependency Parser\n",
    "\n",
    "> A transition-based dependency parser component. The dependency parser jointly learns sentence segmentation and labelled dependency parsing, and can optionally learn to merge tokens that had been over-segmented by the tokenizer. The parser uses a variant of the non-monotonic arc-eager transition-system described by Honnibal and Johnson (2014), with the addition of a \"break\" transition to perform the sentence segmentation. Nivre (2005)’s pseudo-projective dependency transformation is used to allow the parser to predict non-projective parses.\n",
    "\n",
    "> The parser is trained using an imitation learning objective. It follows the actions predicted by the current weights, and at each state, determines which actions are compatible with the optimal parse that could be reached from the current state. The weights are updated such that the scores assigned to the set of optimal actions is increased, while scores assigned to other actions are decreased. Note that more than one action may be optimal for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:45.030860600Z",
     "start_time": "2023-07-16T09:37:44.289626Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 'I saw the man with a telescope.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (2.1.0.dev20230604+cu121)\n",
      "Requirement already satisfied: protobuf in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (4.23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (2.28.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (1.24.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: six in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (3.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (2.1.0.dev20230604+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (1.24.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (4.23.2)\n",
      "Requirement already satisfied: six in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from stanza) (2.28.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (3.12.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:47.340862400Z",
     "start_time": "2023-07-16T09:37:44.300496800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:52.953195400Z",
     "start_time": "2023-07-16T09:37:47.777491800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53d58172d68b46999c19ab8653cfec2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 11:37:52 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-07-16 11:37:54 INFO: File exists: C:\\Users\\adnan\\stanza_resources\\en\\default.zip\n",
      "2023-07-16 11:38:00 INFO: Finished downloading models and saved to C:\\Users\\adnan\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "# stanza example\n",
    "import stanza\n",
    "\n",
    "# Download the stanza model if necessary\n",
    "stanza.download(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "[\n  [\n    {\n      \"id\": 1,\n      \"text\": \"I\",\n      \"lemma\": \"I\",\n      \"upos\": \"PRON\",\n      \"xpos\": \"PRP\",\n      \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n      \"head\": 2,\n      \"deprel\": \"nsubj\",\n      \"start_char\": 0,\n      \"end_char\": 1,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 2,\n      \"text\": \"saw\",\n      \"lemma\": \"see\",\n      \"upos\": \"VERB\",\n      \"xpos\": \"VBD\",\n      \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\",\n      \"head\": 0,\n      \"deprel\": \"root\",\n      \"start_char\": 2,\n      \"end_char\": 5,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 3,\n      \"text\": \"the\",\n      \"lemma\": \"the\",\n      \"upos\": \"DET\",\n      \"xpos\": \"DT\",\n      \"feats\": \"Definite=Def|PronType=Art\",\n      \"head\": 4,\n      \"deprel\": \"det\",\n      \"start_char\": 6,\n      \"end_char\": 9,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 4,\n      \"text\": \"man\",\n      \"lemma\": \"man\",\n      \"upos\": \"NOUN\",\n      \"xpos\": \"NN\",\n      \"feats\": \"Number=Sing\",\n      \"head\": 2,\n      \"deprel\": \"obj\",\n      \"start_char\": 10,\n      \"end_char\": 13,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 5,\n      \"text\": \"with\",\n      \"lemma\": \"with\",\n      \"upos\": \"ADP\",\n      \"xpos\": \"IN\",\n      \"head\": 7,\n      \"deprel\": \"case\",\n      \"start_char\": 14,\n      \"end_char\": 18,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 6,\n      \"text\": \"a\",\n      \"lemma\": \"a\",\n      \"upos\": \"DET\",\n      \"xpos\": \"DT\",\n      \"feats\": \"Definite=Ind|PronType=Art\",\n      \"head\": 7,\n      \"deprel\": \"det\",\n      \"start_char\": 19,\n      \"end_char\": 20,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 7,\n      \"text\": \"telescope\",\n      \"lemma\": \"telescope\",\n      \"upos\": \"NOUN\",\n      \"xpos\": \"NN\",\n      \"feats\": \"Number=Sing\",\n      \"head\": 2,\n      \"deprel\": \"obl\",\n      \"start_char\": 21,\n      \"end_char\": 30,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 8,\n      \"text\": \".\",\n      \"lemma\": \".\",\n      \"upos\": \"PUNCT\",\n      \"xpos\": \".\",\n      \"head\": 2,\n      \"deprel\": \"punct\",\n      \"start_char\": 30,\n      \"end_char\": 31,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    }\n  ]\n]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "[\n  [\n    {\n      \"id\": 1,\n      \"text\": \"I\",\n      \"lemma\": \"I\",\n      \"upos\": \"PRON\",\n      \"xpos\": \"PRP\",\n      \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n      \"head\": 2,\n      \"deprel\": \"nsubj\",\n      \"start_char\": 0,\n      \"end_char\": 1,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 2,\n      \"text\": \"saw\",\n      \"lemma\": \"see\",\n      \"upos\": \"VERB\",\n      \"xpos\": \"VBD\",\n      \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\",\n      \"head\": 0,\n      \"deprel\": \"root\",\n      \"start_char\": 2,\n      \"end_char\": 5,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 3,\n      \"text\": \"the\",\n      \"lemma\": \"the\",\n      \"upos\": \"DET\",\n      \"xpos\": \"DT\",\n      \"feats\": \"Definite=Def|PronType=Art\",\n      \"head\": 4,\n      \"deprel\": \"det\",\n      \"start_char\": 6,\n      \"end_char\": 9,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 4,\n      \"text\": \"man\",\n      \"lemma\": \"man\",\n      \"upos\": \"NOUN\",\n      \"xpos\": \"NN\",\n      \"feats\": \"Number=Sing\",\n      \"head\": 2,\n      \"deprel\": \"obj\",\n      \"start_char\": 10,\n      \"end_char\": 13,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 5,\n      \"text\": \"with\",\n      \"lemma\": \"with\",\n      \"upos\": \"ADP\",\n      \"xpos\": \"IN\",\n      \"head\": 7,\n      \"deprel\": \"case\",\n      \"start_char\": 14,\n      \"end_char\": 18,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 6,\n      \"text\": \"a\",\n      \"lemma\": \"a\",\n      \"upos\": \"DET\",\n      \"xpos\": \"DT\",\n      \"feats\": \"Definite=Ind|PronType=Art\",\n      \"head\": 7,\n      \"deprel\": \"det\",\n      \"start_char\": 19,\n      \"end_char\": 20,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 7,\n      \"text\": \"telescope\",\n      \"lemma\": \"telescope\",\n      \"upos\": \"NOUN\",\n      \"xpos\": \"NN\",\n      \"feats\": \"Number=Sing\",\n      \"head\": 2,\n      \"deprel\": \"obl\",\n      \"start_char\": 21,\n      \"end_char\": 30,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    },\n    {\n      \"id\": 8,\n      \"text\": \".\",\n      \"lemma\": \".\",\n      \"upos\": \"PUNCT\",\n      \"xpos\": \".\",\n      \"head\": 2,\n      \"deprel\": \"punct\",\n      \"start_char\": 30,\n      \"end_char\": 31,\n      \"ner\": \"O\",\n      \"multi_ner\": [\n        \"O\"\n      ]\n    }\n  ]\n]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanza_nlp = stanza.Pipeline(lang='en', verbose=False)\n",
    "stanza_doc = stanza_nlp(example)\n",
    "stanza_doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:56.995600900Z",
     "start_time": "2023-07-16T09:38:00.220066900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[[\n   {\n     \"id\": 1,\n     \"text\": \"I\",\n     \"lemma\": \"I\",\n     \"upos\": \"PRON\",\n     \"xpos\": \"PRP\",\n     \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n     \"head\": 2,\n     \"deprel\": \"nsubj\",\n     \"start_char\": 0,\n     \"end_char\": 1,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 2,\n     \"text\": \"saw\",\n     \"lemma\": \"see\",\n     \"upos\": \"VERB\",\n     \"xpos\": \"VBD\",\n     \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\",\n     \"head\": 0,\n     \"deprel\": \"root\",\n     \"start_char\": 2,\n     \"end_char\": 5,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 3,\n     \"text\": \"the\",\n     \"lemma\": \"the\",\n     \"upos\": \"DET\",\n     \"xpos\": \"DT\",\n     \"feats\": \"Definite=Def|PronType=Art\",\n     \"head\": 4,\n     \"deprel\": \"det\",\n     \"start_char\": 6,\n     \"end_char\": 9,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 4,\n     \"text\": \"man\",\n     \"lemma\": \"man\",\n     \"upos\": \"NOUN\",\n     \"xpos\": \"NN\",\n     \"feats\": \"Number=Sing\",\n     \"head\": 2,\n     \"deprel\": \"obj\",\n     \"start_char\": 10,\n     \"end_char\": 13,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 5,\n     \"text\": \"with\",\n     \"lemma\": \"with\",\n     \"upos\": \"ADP\",\n     \"xpos\": \"IN\",\n     \"head\": 7,\n     \"deprel\": \"case\",\n     \"start_char\": 14,\n     \"end_char\": 18,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 6,\n     \"text\": \"a\",\n     \"lemma\": \"a\",\n     \"upos\": \"DET\",\n     \"xpos\": \"DT\",\n     \"feats\": \"Definite=Ind|PronType=Art\",\n     \"head\": 7,\n     \"deprel\": \"det\",\n     \"start_char\": 19,\n     \"end_char\": 20,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 7,\n     \"text\": \"telescope\",\n     \"lemma\": \"telescope\",\n     \"upos\": \"NOUN\",\n     \"xpos\": \"NN\",\n     \"feats\": \"Number=Sing\",\n     \"head\": 2,\n     \"deprel\": \"obl\",\n     \"start_char\": 21,\n     \"end_char\": 30,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 8,\n     \"text\": \".\",\n     \"lemma\": \".\",\n     \"upos\": \"PUNCT\",\n     \"xpos\": \".\",\n     \"head\": 2,\n     \"deprel\": \"punct\",\n     \"start_char\": 30,\n     \"end_char\": 31,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   }\n ]]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "[[\n   {\n     \"id\": 1,\n     \"text\": \"I\",\n     \"lemma\": \"I\",\n     \"upos\": \"PRON\",\n     \"xpos\": \"PRP\",\n     \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n     \"head\": 2,\n     \"deprel\": \"nsubj\",\n     \"start_char\": 0,\n     \"end_char\": 1,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 2,\n     \"text\": \"saw\",\n     \"lemma\": \"see\",\n     \"upos\": \"VERB\",\n     \"xpos\": \"VBD\",\n     \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\",\n     \"head\": 0,\n     \"deprel\": \"root\",\n     \"start_char\": 2,\n     \"end_char\": 5,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 3,\n     \"text\": \"the\",\n     \"lemma\": \"the\",\n     \"upos\": \"DET\",\n     \"xpos\": \"DT\",\n     \"feats\": \"Definite=Def|PronType=Art\",\n     \"head\": 4,\n     \"deprel\": \"det\",\n     \"start_char\": 6,\n     \"end_char\": 9,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 4,\n     \"text\": \"man\",\n     \"lemma\": \"man\",\n     \"upos\": \"NOUN\",\n     \"xpos\": \"NN\",\n     \"feats\": \"Number=Sing\",\n     \"head\": 2,\n     \"deprel\": \"obj\",\n     \"start_char\": 10,\n     \"end_char\": 13,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 5,\n     \"text\": \"with\",\n     \"lemma\": \"with\",\n     \"upos\": \"ADP\",\n     \"xpos\": \"IN\",\n     \"head\": 7,\n     \"deprel\": \"case\",\n     \"start_char\": 14,\n     \"end_char\": 18,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 6,\n     \"text\": \"a\",\n     \"lemma\": \"a\",\n     \"upos\": \"DET\",\n     \"xpos\": \"DT\",\n     \"feats\": \"Definite=Ind|PronType=Art\",\n     \"head\": 7,\n     \"deprel\": \"det\",\n     \"start_char\": 19,\n     \"end_char\": 20,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 7,\n     \"text\": \"telescope\",\n     \"lemma\": \"telescope\",\n     \"upos\": \"NOUN\",\n     \"xpos\": \"NN\",\n     \"feats\": \"Number=Sing\",\n     \"head\": 2,\n     \"deprel\": \"obl\",\n     \"start_char\": 21,\n     \"end_char\": 30,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   },\n   {\n     \"id\": 8,\n     \"text\": \".\",\n     \"lemma\": \".\",\n     \"upos\": \"PUNCT\",\n     \"xpos\": \".\",\n     \"head\": 2,\n     \"deprel\": \"punct\",\n     \"start_char\": 30,\n     \"end_char\": 31,\n     \"ner\": \"O\",\n     \"multi_ner\": [\n       \"O\"\n     ]\n   }\n ]]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanza_doc.sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:57.069595800Z",
     "start_time": "2023-07-16T09:38:20.278394500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 2, 'nsubj')\n",
      "('saw', 0, 'root')\n",
      "('the', 4, 'det')\n",
      "('man', 2, 'obj')\n",
      "('with', 7, 'case')\n",
      "('a', 7, 'det')\n",
      "('telescope', 2, 'obl')\n",
      "('.', 2, 'punct')\n",
      "('I', 2, 'nsubj')\n",
      "('saw', 0, 'root')\n",
      "('the', 4, 'det')\n",
      "('man', 2, 'obj')\n",
      "('with', 7, 'case')\n",
      "('a', 7, 'det')\n",
      "('telescope', 2, 'obl')\n",
      "('.', 2, 'punct')\n"
     ]
    }
   ],
   "source": [
    "stanza_doc.sentences[0].print_dependencies()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:57.087596700Z",
     "start_time": "2023-07-16T09:38:20.297395200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tI\t2\tnsubj\n",
      "2\tsaw\t0\troot\n",
      "3\tthe\t4\tdet\n",
      "4\tman\t2\tobj\n",
      "5\twith\t7\tcase\n",
      "6\ta\t7\tdet\n",
      "7\ttelescope\t2\tobl\n",
      "8\t.\t2\tpunct\n",
      "1\tI\t2\tnsubj\n",
      "2\tsaw\t0\troot\n",
      "3\tthe\t4\tdet\n",
      "4\tman\t2\tobj\n",
      "5\twith\t7\tcase\n",
      "6\ta\t7\tdet\n",
      "7\ttelescope\t2\tobl\n",
      "8\t.\t2\tpunct\n"
     ]
    }
   ],
   "source": [
    "for sent in stanza_doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(word.id, word.text, word.head, word.deprel))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:57.087596700Z",
     "start_time": "2023-07-16T09:38:20.312398100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:38:57.087596700Z",
     "start_time": "2023-07-16T09:38:20.325396500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-07-16T09:39:04.622005900Z",
     "start_time": "2023-07-16T09:38:20.341393800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB 1.4 MB/s eta 0:00:10\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.2 MB/s eta 0:00:12\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.1 MB/s eta 0:00:13\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.1 MB/s eta 0:00:13\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.1 MB/s eta 0:00:13\n",
      "      --------------------------------------- 0.2/12.8 MB 1.0 MB/s eta 0:00:13\n",
      "      -------------------------------------- 0.3/12.8 MB 983.9 kB/s eta 0:00:13\n",
      "      -------------------------------------- 0.3/12.8 MB 936.6 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.3/12.8 MB 955.3 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.3/12.8 MB 955.3 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.4/12.8 MB 904.2 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.4/12.8 MB 904.2 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.5/12.8 MB 911.0 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.6/12.8 MB 908.0 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.6/12.8 MB 921.0 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.6/12.8 MB 924.1 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.7/12.8 MB 914.8 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.7/12.8 MB 926.2 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.8/12.8 MB 927.5 kB/s eta 0:00:13\n",
      "     -- ------------------------------------ 0.8/12.8 MB 920.4 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.9/12.8 MB 928.9 kB/s eta 0:00:13\n",
      "     -- ------------------------------------ 0.9/12.8 MB 936.6 kB/s eta 0:00:13\n",
      "     -- ------------------------------------ 1.0/12.8 MB 938.2 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.0/12.8 MB 944.9 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.1/12.8 MB 928.4 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.1/12.8 MB 939.0 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.1/12.8 MB 936.3 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.2/12.8 MB 906.6 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.2/12.8 MB 908.6 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.2/12.8 MB 915.0 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.3/12.8 MB 910.4 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.3/12.8 MB 902.1 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.4/12.8 MB 917.7 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.4/12.8 MB 914.0 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.5/12.8 MB 915.0 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.5/12.8 MB 920.0 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.5/12.8 MB 919.2 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 923.9 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 916.7 kB/s eta 0:00:13\n",
      "     ----- --------------------------------- 1.7/12.8 MB 921.1 kB/s eta 0:00:13\n",
      "     ----- --------------------------------- 1.7/12.8 MB 925.4 kB/s eta 0:00:12\n",
      "     ----- --------------------------------- 1.8/12.8 MB 918.7 kB/s eta 0:00:13\n",
      "     ----- --------------------------------- 1.8/12.8 MB 922.8 kB/s eta 0:00:12\n",
      "     ----- --------------------------------- 1.9/12.8 MB 926.7 kB/s eta 0:00:12\n",
      "     ----- --------------------------------- 1.9/12.8 MB 927.7 kB/s eta 0:00:12\n",
      "     ----- --------------------------------- 1.9/12.8 MB 931.4 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.0/12.8 MB 928.3 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.0/12.8 MB 927.1 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.1/12.8 MB 932.6 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.1/12.8 MB 931.3 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.2/12.8 MB 928.1 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.2/12.8 MB 928.9 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.3/12.8 MB 932.1 kB/s eta 0:00:12\n",
      "     ------ -------------------------------- 2.3/12.8 MB 926.7 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.3/12.8 MB 929.8 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.4/12.8 MB 933.1 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.4/12.8 MB 935.9 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.5/12.8 MB 933.0 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.5/12.8 MB 933.7 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.6/12.8 MB 936.4 kB/s eta 0:00:11\n",
      "     ------- ------------------------------- 2.6/12.8 MB 935.3 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.7/12.8 MB 932.9 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.7/12.8 MB 933.2 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.7/12.8 MB 930.9 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.8/12.8 MB 938.4 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.8/12.8 MB 938.7 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.9/12.8 MB 936.4 kB/s eta 0:00:11\n",
      "     -------- ------------------------------ 2.9/12.8 MB 938.7 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.0/12.8 MB 940.9 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.0/12.8 MB 936.7 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.1/12.8 MB 938.9 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.1/12.8 MB 938.3 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.1/12.8 MB 941.5 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.2/12.8 MB 936.4 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.2/12.8 MB 938.5 kB/s eta 0:00:11\n",
      "     --------- ----------------------------- 3.3/12.8 MB 939.0 kB/s eta 0:00:11\n",
      "     ---------- ---------------------------- 3.3/12.8 MB 941.0 kB/s eta 0:00:11\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 938.7 kB/s eta 0:00:11\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 939.2 kB/s eta 0:00:11\n",
      "     ---------- ---------------------------- 3.5/12.8 MB 937.2 kB/s eta 0:00:10\n",
      "     ---------- ---------------------------- 3.5/12.8 MB 939.2 kB/s eta 0:00:10\n",
      "     ---------- ---------------------------- 3.5/12.8 MB 939.3 kB/s eta 0:00:10\n",
      "     ---------- ---------------------------- 3.6/12.8 MB 941.5 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.6/12.8 MB 939.4 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.7/12.8 MB 939.8 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.7/12.8 MB 941.5 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.8/12.8 MB 940.7 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.8/12.8 MB 939.9 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.8/12.8 MB 939.1 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.9/12.8 MB 940.8 kB/s eta 0:00:10\n",
      "     ----------- --------------------------- 3.9/12.8 MB 941.2 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.0/12.8 MB 939.5 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.0/12.8 MB 939.7 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.1/12.8 MB 941.5 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.1/12.8 MB 939.7 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.2/12.8 MB 941.3 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.2/12.8 MB 942.8 kB/s eta 0:00:10\n",
      "     ------------ -------------------------- 4.2/12.8 MB 943.2 kB/s eta 0:00:10\n",
      "     ------------- ------------------------- 4.3/12.8 MB 944.7 kB/s eta 0:00:10\n",
      "     ------------- ------------------------- 4.3/12.8 MB 940.8 kB/s eta 0:00:10\n",
      "     ------------- ------------------------- 4.4/12.8 MB 943.2 kB/s eta 0:00:09\n",
      "     ------------- ------------------------- 4.4/12.8 MB 942.7 kB/s eta 0:00:09\n",
      "     ------------- ------------------------- 4.5/12.8 MB 944.1 kB/s eta 0:00:09\n",
      "     ------------- ------------------------- 4.5/12.8 MB 941.3 kB/s eta 0:00:09\n",
      "     ------------- ------------------------- 4.5/12.8 MB 942.7 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.6/12.8 MB 944.1 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.6/12.8 MB 944.4 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.7/12.8 MB 942.7 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.7/12.8 MB 944.1 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.8/12.8 MB 945.7 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.8/12.8 MB 943.0 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.9/12.8 MB 944.3 kB/s eta 0:00:09\n",
      "     -------------- ------------------------ 4.9/12.8 MB 943.7 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 4.9/12.8 MB 944.7 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.0/12.8 MB 936.5 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.0/12.8 MB 934.1 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.0/12.8 MB 935.4 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.1/12.8 MB 935.7 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.1/12.8 MB 937.1 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.2/12.8 MB 935.8 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.2/12.8 MB 937.1 kB/s eta 0:00:09\n",
      "     ---------------- ---------------------- 5.3/12.8 MB 937.4 kB/s eta 0:00:09\n",
      "     ---------------- ---------------------- 5.3/12.8 MB 938.7 kB/s eta 0:00:08\n",
      "     ---------------- ---------------------- 5.3/12.8 MB 936.3 kB/s eta 0:00:08\n",
      "     ---------------- ---------------------- 5.4/12.8 MB 937.6 kB/s eta 0:00:08\n",
      "     ---------------- ---------------------- 5.4/12.8 MB 938.8 kB/s eta 0:00:08\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 938.3 kB/s eta 0:00:08\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 937.1 kB/s eta 0:00:08\n",
      "     ---------------- ---------------------- 5.6/12.8 MB 937.4 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.6/12.8 MB 938.6 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.7/12.8 MB 939.7 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.7/12.8 MB 937.5 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.7/12.8 MB 938.7 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.8/12.8 MB 939.2 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.8/12.8 MB 938.7 kB/s eta 0:00:08\n",
      "     ----------------- --------------------- 5.9/12.8 MB 938.2 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 5.9/12.8 MB 939.3 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 6.0/12.8 MB 940.4 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 6.0/12.8 MB 940.7 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 6.1/12.8 MB 939.4 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 6.1/12.8 MB 940.5 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 6.2/12.8 MB 938.6 kB/s eta 0:00:08\n",
      "     ------------------ -------------------- 6.2/12.8 MB 941.8 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.3/12.8 MB 940.7 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.3/12.8 MB 940.2 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.3/12.8 MB 942.0 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.4/12.8 MB 941.5 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.4/12.8 MB 940.3 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.5/12.8 MB 940.5 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.5/12.8 MB 941.6 kB/s eta 0:00:07\n",
      "     ------------------- ------------------- 6.6/12.8 MB 942.6 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.6/12.8 MB 940.6 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.6/12.8 MB 941.7 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.7/12.8 MB 942.7 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.7/12.8 MB 940.8 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.8/12.8 MB 941.8 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.8/12.8 MB 942.7 kB/s eta 0:00:07\n",
      "     -------------------- ------------------ 6.9/12.8 MB 943.2 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 6.9/12.8 MB 941.3 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.0/12.8 MB 941.4 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.0/12.8 MB 943.2 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.1/12.8 MB 943.4 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.1/12.8 MB 942.3 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.2/12.8 MB 943.2 kB/s eta 0:00:06\n",
      "     --------------------- ----------------- 7.2/12.8 MB 942.8 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.2/12.8 MB 944.3 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 942.0 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 942.9 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.4/12.8 MB 943.1 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.4/12.8 MB 944.0 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.4/12.8 MB 942.2 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.5/12.8 MB 943.1 kB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 7.5/12.8 MB 944.0 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.6/12.8 MB 944.8 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.6/12.8 MB 944.4 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.7/12.8 MB 944.0 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.7/12.8 MB 944.9 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.8/12.8 MB 944.5 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.8/12.8 MB 944.1 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.9/12.8 MB 944.3 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 7.9/12.8 MB 945.1 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.0/12.8 MB 945.3 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.0/12.8 MB 944.4 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 945.2 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 945.4 kB/s eta 0:00:05\n",
      "     ------------------------ -------------- 8.1/12.8 MB 946.2 kB/s eta 0:00:05\n",
      "     ------------------------ -------------- 8.2/12.8 MB 944.6 kB/s eta 0:00:05\n",
      "     ------------------------ -------------- 8.2/12.8 MB 944.8 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.3/12.8 MB 943.8 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.3/12.8 MB 941.6 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.3/12.8 MB 939.1 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.3/12.8 MB 939.2 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.4/12.8 MB 940.5 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.4/12.8 MB 940.2 kB/s eta 0:00:05\n",
      "     ------------------------- ------------- 8.5/12.8 MB 941.6 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.6/12.8 MB 940.8 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.6/12.8 MB 940.4 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.6/12.8 MB 941.7 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.7/12.8 MB 939.8 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.7/12.8 MB 941.0 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.8/12.8 MB 939.1 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.8/12.8 MB 939.9 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.8/12.8 MB 940.0 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 8.9/12.8 MB 940.8 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 8.9/12.8 MB 939.9 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.0/12.8 MB 940.1 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.0/12.8 MB 940.8 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.1/12.8 MB 941.5 kB/s eta 0:00:04\n",
      "     --------------------------- ----------- 9.1/12.8 MB 940.8 kB/s eta 0:00:04\n",
      "     --------------------------- ----------- 9.2/12.8 MB 941.0 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.2/12.8 MB 941.7 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.2/12.8 MB 941.8 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.3/12.8 MB 941.0 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.3/12.8 MB 940.7 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 940.4 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 940.0 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.5/12.8 MB 941.2 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.5/12.8 MB 939.5 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.5/12.8 MB 940.2 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.6/12.8 MB 940.3 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.6/12.8 MB 941.0 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.7/12.8 MB 939.7 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.7/12.8 MB 940.4 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.8/12.8 MB 941.1 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.8/12.8 MB 940.4 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.8/12.8 MB 940.4 kB/s eta 0:00:04\n",
      "     ------------------------------ -------- 9.9/12.8 MB 941.1 kB/s eta 0:00:04\n",
      "     ------------------------------ -------- 9.9/12.8 MB 941.9 kB/s eta 0:00:04\n",
      "     ----------------------------- -------- 10.0/12.8 MB 940.1 kB/s eta 0:00:03\n",
      "     ----------------------------- -------- 10.0/12.8 MB 941.2 kB/s eta 0:00:03\n",
      "     ----------------------------- -------- 10.1/12.8 MB 940.9 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.1/12.8 MB 940.2 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.1/12.8 MB 940.3 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.2/12.8 MB 941.0 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.3/12.8 MB 942.0 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.3/12.8 MB 940.7 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.3/12.8 MB 951.6 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.4/12.8 MB 948.8 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.4/12.8 MB 943.4 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.5/12.8 MB 940.7 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.5/12.8 MB 940.7 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.5/12.8 MB 942.1 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.6/12.8 MB 947.5 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.6/12.8 MB 943.4 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.7/12.8 MB 950.2 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.7/12.8 MB 943.4 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.7/12.8 MB 942.1 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 10.8/12.8 MB 940.7 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 10.8/12.8 MB 940.7 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 10.9/12.8 MB 940.7 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 10.9/12.8 MB 942.1 kB/s eta 0:00:02\n",
      "     -------------------------------- ----- 11.0/12.8 MB 940.7 kB/s eta 0:00:02\n",
      "     -------------------------------- ----- 11.0/12.8 MB 940.7 kB/s eta 0:00:02\n",
      "     -------------------------------- ----- 11.0/12.8 MB 942.0 kB/s eta 0:00:02\n",
      "     -------------------------------- ----- 11.1/12.8 MB 942.1 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.2/12.8 MB 942.1 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.2/12.8 MB 940.7 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.2/12.8 MB 940.7 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.3/12.8 MB 942.1 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.3/12.8 MB 940.7 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.4/12.8 MB 940.7 kB/s eta 0:00:02\n",
      "     --------------------------------- ---- 11.4/12.8 MB 944.7 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.5/12.8 MB 944.8 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.5/12.8 MB 946.1 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.5/12.8 MB 946.1 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.6/12.8 MB 944.8 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.6/12.8 MB 943.4 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.7/12.8 MB 943.4 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.7/12.8 MB 943.4 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.7/12.8 MB 942.0 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.8/12.8 MB 942.0 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.8/12.8 MB 942.0 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.9/12.8 MB 943.4 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 11.9/12.8 MB 942.1 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 12.0/12.8 MB 942.1 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 12.0/12.8 MB 943.4 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 12.1/12.8 MB 942.0 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 12.1/12.8 MB 942.0 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 12.1/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.2/12.8 MB 939.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.2/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.2/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.3/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.3/12.8 MB 939.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.4/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.4/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.5/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.5/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.7/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.7/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.7/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.8/12.8 MB 940.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 937.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# spacy example\n",
    "import spacy\n",
    "# download the spacy model if necessary\n",
    "!python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tI\tsaw\tnsubj\n",
      "1\tsaw\tsaw\tROOT\n",
      "2\tthe\tman\tdet\n",
      "3\tman\tsaw\tdobj\n",
      "4\twith\tman\tprep\n",
      "5\ta\ttelescope\tdet\n",
      "6\ttelescope\twith\tpobj\n",
      "7\t.\tsaw\tpunct\n",
      "0\tI\tsaw\tnsubj\n",
      "1\tsaw\tsaw\tROOT\n",
      "2\tthe\tman\tdet\n",
      "3\tman\tsaw\tdobj\n",
      "4\twith\tman\tprep\n",
      "5\ta\ttelescope\tdet\n",
      "6\ttelescope\twith\tpobj\n",
      "7\t.\tsaw\tpunct\n"
     ]
    }
   ],
   "source": [
    "spacy_doc = spacy_nlp(example)\n",
    "\n",
    "for sent in spacy_doc.sents:\n",
    "    for token in sent:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(token.i, token.text, token.head, token.dep_))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:39:04.636005900Z",
     "start_time": "2023-07-16T09:38:44.903801100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:39:04.685037700Z",
     "start_time": "2023-07-16T09:38:44.948861600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:39:04.685037700Z",
     "start_time": "2023-07-16T09:38:44.964862Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse 100 last sentences from dependency treebank using `spacy` and `stanza`\n",
    "    - are the depedency tags of spacy the same of stanza?\n",
    "- Evaluate the parses using DependencyEvaluator\n",
    "    - print LAS and UAS for each parser\n",
    "\n",
    "**BUT!** To evaluate the parsers, the sentences parsed by spacy and stanza have to be [`DependencyGraph`](https://www.nltk.org/_modules/nltk/parse/dependencygraph.html) objects.  To do this , you have to covert the output of the spacy/stanza to [ConLL](https://universaldependencies.org/format.html) formant, from this format extract the columns following the [Malt-Tab](https://cl.lingfil.uu.se/~nivre/research/MaltXML.html) format and finally convert the resulting string into a DependecyGraph. Luckly, there is a library that gets the job done.  You have to install the library [spacy_conll](https://github.com/BramVanroy/spacy_conll) and use and adapt to your needs the code that you can find below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Load the dependency treebank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "['The Army Corps is cutting the flow of the Missouri River about two weeks earlier than normal because of low water levels in the reservoirs that feed it .',\n 'Barge rates on the Mississippi River sank yesterday on speculation that widespread rain this week in the Midwest might temporarily alleviate the situation .',\n 'But the Army Corps of Engineers expects the river level to continue falling this month .']"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "# getting the last 100 sentences from the dependency treebank\n",
    "sentences_ = dependency_treebank.sents()[-100:]\n",
    "\n",
    "sentences = []\n",
    "for sentence in sentences_:\n",
    "    sentences.append(\" \".join(sentence))\n",
    "\n",
    "sentences[0:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:20:23.859895800Z",
     "start_time": "2023-07-16T19:20:23.584864600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from nltk.parse import DependencyEvaluator\n",
    "import stanza\n",
    "import spacy_stanza"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:20:29.190528800Z",
     "start_time": "2023-07-16T19:20:29.170529900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x21da39265e0>),\n ('tagger', <spacy.pipeline.tagger.Tagger at 0x21ecd4b15e0>),\n ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x21ec97b9660>),\n ('attribute_ruler',\n  <spacy.pipeline.attributeruler.AttributeRuler at 0x21eca1fd080>),\n ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x21ffd53a7c0>),\n ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x21f83d64eb0>),\n ('conll_formatter',\n  ConllFormatter(conversion_maps={'deprel': {'nsubj': 'subj'}}, ext_names={'conll_str': 'conll_str', 'conll': 'conll', 'conll_pd': 'pandas'}, field_names={'ID': 'ID', 'FORM': 'FORM', 'LEMMA': 'LEMMA', 'UPOS': 'UPOS', 'XPOS': 'XPOS', 'FEATS': 'FEATS', 'HEAD': 'HEAD', 'DEPREL': 'DEPREL', 'DEPS': 'DEPS', 'MISC': 'MISC'}, include_headers=False, disable_pandas=False))]"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################################\n",
    "# Downloading and Setting up the Spacy pipeline\n",
    "#############################################################################\n",
    "# Load the Spacy model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Set up the conll formatter\n",
    "spacy_config = {\n",
    "    \"ext_names\": {\n",
    "        \"conll_pd\": \"pandas\"\n",
    "    },\n",
    "    \"conversion_maps\": {\n",
    "        \"deprel\": {\n",
    "            \"nsubj\": \"subj\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add the formatter to the pipeline\n",
    "nlp_spacy.add_pipe(\"conll_formatter\", config=spacy_config, last=True)\n",
    "\n",
    "# Split by white space\n",
    "nlp_spacy.tokenizer = Tokenizer(nlp_spacy.vocab)\n",
    "\n",
    "# print the pipeline\n",
    "nlp_spacy.pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:20:41.787244Z",
     "start_time": "2023-07-16T19:20:41.103244100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "ConllFormatter(conversion_maps={'deprel': {'nsubj': 'subj', 'root': 'ROOT'}}, ext_names={'conll_str': 'conll_str', 'conll': 'conll', 'conll_pd': 'pandas'}, field_names={'ID': 'ID', 'FORM': 'FORM', 'LEMMA': 'LEMMA', 'UPOS': 'UPOS', 'XPOS': 'XPOS', 'FEATS': 'FEATS', 'HEAD': 'HEAD', 'DEPREL': 'DEPREL', 'DEPS': 'DEPS', 'MISC': 'MISC'}, include_headers=False, disable_pandas=False)"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "# Downloading and Setting up the Stanza pipeline\n",
    "############################################################################\n",
    "# Download the stanza model if necessary\n",
    "stanza.download(\"en\")\n",
    "\n",
    "# Set up the conll formatter\n",
    "# tokenize_pretokenized used to tokenize by whitespace\n",
    "nlp_stanza = spacy_stanza.load_pipeline(\"en\", verbose=False, tokenize_pretokenized=True)\n",
    "\n",
    "stanza_config = {\n",
    "    \"ext_names\": {\n",
    "        \"conll_pd\": \"pandas\"\n",
    "    },\n",
    "    \"conversion_maps\": {\n",
    "        \"deprel\": {\n",
    "            \"nsubj\": \"subj\",\n",
    "            \"root\":\"ROOT\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add the formatter to the stanza pipeline\n",
    "nlp_stanza.add_pipe(\"conll_formatter\", config=stanza_config, last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:20:59.127541200Z",
     "start_time": "2023-07-16T19:20:43.228159700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "# Parse the sentences and create DependencyGraph objects\n",
    "graphs_spacy = []\n",
    "graphs_stanza = []\n",
    "for sentence in sentences:\n",
    "    # Parse the sentence using Spacy\n",
    "    doc_spacy = nlp_spacy(sentence)\n",
    "    df_spacy = doc_spacy._.pandas\n",
    "    tmp_spacy = df_spacy[[\"FORM\", \"XPOS\", \"HEAD\", \"DEPREL\"]].to_string(header=False, index=False)\n",
    "    # tmp_spacy = df_spacy[['FORM', 'UPOS', 'HEAD', 'DEPREL']].to_string(header=False, index=False)\n",
    "    # print(f\"Spacy parse of '{sentence}':\\n{tmp_spacy}\")\n",
    "    try:\n",
    "        graph_spacy = DependencyGraph(tmp_spacy)\n",
    "        graphs_spacy.append(graph_spacy)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create DependencyGraph for Spacy parse of '{sentence}': {e}\")\n",
    "\n",
    "    # # Parse the sentence using Stanza\n",
    "    doc_stanza = nlp_stanza(sentence)\n",
    "    df_stanza = doc_stanza._.pandas\n",
    "    tmp_stanza = df_stanza[[\"FORM\", \"XPOS\", \"HEAD\", \"DEPREL\"]].to_string(header=False, index=False)\n",
    "    # tmp_stanza = df_stanza[['FORM', 'UPOS', 'HEAD', 'DEPREL']].to_string(header=False, index=False)\n",
    "    # print(f\"Stanza parse of '{sentence}':\\n{tmp_stanza}\")\n",
    "    try:\n",
    "        graph_stanza = DependencyGraph(tmp_stanza, top_relation_label='root')\n",
    "        graphs_stanza.append(graph_stanza)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create DependencyGraph for Stanza parse of '{sentence}': {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:36:34.582810600Z",
     "start_time": "2023-07-16T19:35:19.938545800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "# Create DependencyEvaluator objects\n",
    "evaluator_spacy = DependencyEvaluator(graphs_spacy, dependency_treebank.parsed_sents()[-100:])\n",
    "evaluator_stanza = DependencyEvaluator(graphs_stanza, dependency_treebank.parsed_sents()[-100:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:36:36.128403Z",
     "start_time": "2023-07-16T19:36:34.586810400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy: LAS=0.0, UAS=0.6926873037236428\n",
      "Stanza: LAS=0.0, UAS=0.4679228353521759\n"
     ]
    }
   ],
   "source": [
    "# Print LAS and UAS for each parser\n",
    "las_spacy, uas_spacy = evaluator_spacy.eval()\n",
    "las_stanza, uas_stanza = evaluator_stanza.eval()\n",
    "print(f\"Spacy: LAS={las_spacy}, UAS={uas_spacy}\")\n",
    "print(f\"Stanza: LAS={las_stanza}, UAS={uas_stanza}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:41:59.274846Z",
     "start_time": "2023-07-16T19:41:59.196846700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparing the parsers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "Tree('cutting', [Tree('Corps', ['The', 'Army']), 'is', Tree('flow', ['the', Tree('of', [Tree('River', ['the', 'Missouri'])])]), Tree('earlier', [Tree('weeks', [Tree('two', ['about'])]), Tree('than', ['normal'])]), Tree('because', ['of', Tree('levels', ['low', 'water', Tree('in', [Tree('reservoirs', ['the', Tree('feed', ['that', 'it'])])])])]), '.'])",
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"312px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,672.0,312.0\" width=\"672px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cutting</text></svg><svg width=\"13.0952%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Corps</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">The</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Army</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.54762%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.7619%\" x=\"13.0952%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.4762%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"23.8095%\" x=\"17.8571%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">flow</text></svg><svg width=\"25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75%\" x=\"25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">River</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"66.6667%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Missouri</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.5%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.7619%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"17.8571%\" x=\"41.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">earlier</text></svg><svg width=\"46.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">weeks</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">two</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">about</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"53.3333%\" x=\"46.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">than</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">normal</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.5952%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"36.9048%\" x=\"59.5238%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">because</text></svg><svg width=\"12.9032%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.45161%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"87.0968%\" x=\"12.9032%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">levels</text></svg><svg width=\"18.5185%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">low</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.25926%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"25.9259%\" x=\"18.5185%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">water</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.4815%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"55.5556%\" x=\"44.4444%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">reservoirs</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"66.6667%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">feed</text></svg><svg width=\"60%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">that</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">it</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.2222%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.4516%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.9762%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.57143%\" x=\"96.4286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.2143%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_spacy[0].tree()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:29:18.379759100Z",
     "start_time": "2023-07-16T19:29:18.320725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(<function nltk.parse.dependencygraph.DependencyGraph.__init__.<locals>.<lambda>()>,\n            {0: {'address': 0,\n              'word': None,\n              'lemma': None,\n              'ctag': 'TOP',\n              'tag': 'TOP',\n              'feats': None,\n              'head': None,\n              'deps': defaultdict(list, {'ROOT': [5]}),\n              'rel': None},\n             1: {'address': 1,\n              'word': 'The',\n              'lemma': 'The',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 3,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             3: {'address': 3,\n              'word': 'Corps',\n              'lemma': 'Corps',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'det': [1], 'compound': [2]}),\n              'rel': 'nsubj'},\n             2: {'address': 2,\n              'word': 'Army',\n              'lemma': 'Army',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 3,\n              'deps': defaultdict(list, {}),\n              'rel': 'compound'},\n             5: {'address': 5,\n              'word': 'cutting',\n              'lemma': 'cutting',\n              'ctag': 'VERB',\n              'tag': 'VERB',\n              'feats': '',\n              'head': 0,\n              'deps': defaultdict(list,\n                          {'nsubj': [3],\n                           'aux': [4],\n                           'dobj': [7],\n                           'advmod': [15],\n                           'prep': [18],\n                           'punct': [29]}),\n              'rel': 'ROOT'},\n             4: {'address': 4,\n              'word': 'is',\n              'lemma': 'is',\n              'ctag': 'AUX',\n              'tag': 'AUX',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {}),\n              'rel': 'aux'},\n             6: {'address': 6,\n              'word': 'the',\n              'lemma': 'the',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 7,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             7: {'address': 7,\n              'word': 'flow',\n              'lemma': 'flow',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'det': [6], 'prep': [8]}),\n              'rel': 'dobj'},\n             8: {'address': 8,\n              'word': 'of',\n              'lemma': 'of',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 7,\n              'deps': defaultdict(list, {'pobj': [11]}),\n              'rel': 'prep'},\n             9: {'address': 9,\n              'word': 'the',\n              'lemma': 'the',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 11,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             11: {'address': 11,\n              'word': 'River',\n              'lemma': 'River',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 8,\n              'deps': defaultdict(list, {'det': [9], 'compound': [10]}),\n              'rel': 'pobj'},\n             10: {'address': 10,\n              'word': 'Missouri',\n              'lemma': 'Missouri',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 11,\n              'deps': defaultdict(list, {}),\n              'rel': 'compound'},\n             12: {'address': 12,\n              'word': 'about',\n              'lemma': 'about',\n              'ctag': 'ADV',\n              'tag': 'ADV',\n              'feats': '',\n              'head': 13,\n              'deps': defaultdict(list, {}),\n              'rel': 'advmod'},\n             13: {'address': 13,\n              'word': 'two',\n              'lemma': 'two',\n              'ctag': 'NUM',\n              'tag': 'NUM',\n              'feats': '',\n              'head': 14,\n              'deps': defaultdict(list, {'advmod': [12]}),\n              'rel': 'nummod'},\n             14: {'address': 14,\n              'word': 'weeks',\n              'lemma': 'weeks',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 15,\n              'deps': defaultdict(list, {'nummod': [13]}),\n              'rel': 'npadvmod'},\n             15: {'address': 15,\n              'word': 'earlier',\n              'lemma': 'earlier',\n              'ctag': 'ADV',\n              'tag': 'ADV',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'npadvmod': [14], 'prep': [16]}),\n              'rel': 'advmod'},\n             16: {'address': 16,\n              'word': 'than',\n              'lemma': 'than',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 15,\n              'deps': defaultdict(list, {'amod': [17]}),\n              'rel': 'prep'},\n             17: {'address': 17,\n              'word': 'normal',\n              'lemma': 'normal',\n              'ctag': 'ADJ',\n              'tag': 'ADJ',\n              'feats': '',\n              'head': 16,\n              'deps': defaultdict(list, {}),\n              'rel': 'amod'},\n             18: {'address': 18,\n              'word': 'because',\n              'lemma': 'because',\n              'ctag': 'SCONJ',\n              'tag': 'SCONJ',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'pcomp': [19], 'pobj': [22]}),\n              'rel': 'prep'},\n             19: {'address': 19,\n              'word': 'of',\n              'lemma': 'of',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 18,\n              'deps': defaultdict(list, {}),\n              'rel': 'pcomp'},\n             20: {'address': 20,\n              'word': 'low',\n              'lemma': 'low',\n              'ctag': 'ADJ',\n              'tag': 'ADJ',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list, {}),\n              'rel': 'amod'},\n             22: {'address': 22,\n              'word': 'levels',\n              'lemma': 'levels',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 18,\n              'deps': defaultdict(list,\n                          {'amod': [20], 'compound': [21], 'prep': [23]}),\n              'rel': 'pobj'},\n             21: {'address': 21,\n              'word': 'water',\n              'lemma': 'water',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list, {}),\n              'rel': 'compound'},\n             23: {'address': 23,\n              'word': 'in',\n              'lemma': 'in',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list, {'pobj': [25]}),\n              'rel': 'prep'},\n             24: {'address': 24,\n              'word': 'the',\n              'lemma': 'the',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 25,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             25: {'address': 25,\n              'word': 'reservoirs',\n              'lemma': 'reservoirs',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 23,\n              'deps': defaultdict(list, {'det': [24], 'relcl': [27]}),\n              'rel': 'pobj'},\n             26: {'address': 26,\n              'word': 'that',\n              'lemma': 'that',\n              'ctag': 'PRON',\n              'tag': 'PRON',\n              'feats': '',\n              'head': 27,\n              'deps': defaultdict(list, {}),\n              'rel': 'nsubj'},\n             27: {'address': 27,\n              'word': 'feed',\n              'lemma': 'feed',\n              'ctag': 'VERB',\n              'tag': 'VERB',\n              'feats': '',\n              'head': 25,\n              'deps': defaultdict(list, {'nsubj': [26], 'dobj': [28]}),\n              'rel': 'relcl'},\n             28: {'address': 28,\n              'word': 'it',\n              'lemma': 'it',\n              'ctag': 'PRON',\n              'tag': 'PRON',\n              'feats': '',\n              'head': 27,\n              'deps': defaultdict(list, {}),\n              'rel': 'dobj'},\n             29: {'address': 29,\n              'word': '.',\n              'lemma': '.',\n              'ctag': 'PUNCT',\n              'tag': 'PUNCT',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {}),\n              'rel': 'punct'}})"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graph nodes\n",
    "graphs_spacy[0].nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:30:07.166685300Z",
     "start_time": "2023-07-16T19:30:07.114686300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "data": {
      "text/plain": "Tree('cutting', [Tree('Corps', ['The', 'Army']), 'is', Tree('flow', ['the', Tree('River', ['of', 'the', 'Missouri'])]), Tree('earlier', [Tree('weeks', [Tree('two', ['about'])]), Tree('normal', ['than'])]), Tree('levels', [Tree('because', ['of']), 'low', 'water', Tree('reservoirs', ['in', 'the', Tree('feed', ['that', 'it'])])]), '.'])",
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,776.0,216.0\" width=\"776px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cutting</text></svg><svg width=\"11.3402%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Corps</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">The</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Army</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.6701%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.12371%\" x=\"11.3402%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.4021%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"24.7423%\" x=\"15.4639%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">flow</text></svg><svg width=\"20.8333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.4167%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"79.1667%\" x=\"20.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">River</text></svg><svg width=\"21.0526%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5263%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"26.3158%\" x=\"21.0526%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.2105%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"52.6316%\" x=\"47.3684%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Missouri</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.6842%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.4167%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.8351%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.4639%\" x=\"40.2062%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">earlier</text></svg><svg width=\"46.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">weeks</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">two</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">about</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"53.3333%\" x=\"46.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">normal</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">than</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.9381%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"41.2371%\" x=\"55.6701%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">levels</text></svg><svg width=\"22.5%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">because</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"12.5%\" x=\"22.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">low</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.75%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"17.5%\" x=\"35%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">water</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.75%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.5%\" x=\"52.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">reservoirs</text></svg><svg width=\"21.0526%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5263%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"26.3158%\" x=\"21.0526%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.2105%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"52.6316%\" x=\"47.3684%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">feed</text></svg><svg width=\"60%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">that</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">it</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.6842%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.25%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.2887%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.09278%\" x=\"96.9072%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.4536%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_stanza[0].tree()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:29:29.927708400Z",
     "start_time": "2023-07-16T19:29:29.864709400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(<function nltk.parse.dependencygraph.DependencyGraph.__init__.<locals>.<lambda>()>,\n            {0: {'address': 0,\n              'word': None,\n              'lemma': None,\n              'ctag': 'TOP',\n              'tag': 'TOP',\n              'feats': None,\n              'head': None,\n              'deps': defaultdict(list, {'root': [5]}),\n              'rel': None},\n             1: {'address': 1,\n              'word': 'The',\n              'lemma': 'The',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 3,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             3: {'address': 3,\n              'word': 'Corps',\n              'lemma': 'Corps',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'det': [1], 'compound': [2]}),\n              'rel': 'nsubj'},\n             2: {'address': 2,\n              'word': 'Army',\n              'lemma': 'Army',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 3,\n              'deps': defaultdict(list, {}),\n              'rel': 'compound'},\n             5: {'address': 5,\n              'word': 'cutting',\n              'lemma': 'cutting',\n              'ctag': 'VERB',\n              'tag': 'VERB',\n              'feats': '',\n              'head': 0,\n              'deps': defaultdict(list,\n                          {'nsubj': [3],\n                           'aux': [4],\n                           'obj': [7],\n                           'advmod': [15],\n                           'obl': [22],\n                           'punct': [29]}),\n              'rel': 'root'},\n             4: {'address': 4,\n              'word': 'is',\n              'lemma': 'is',\n              'ctag': 'AUX',\n              'tag': 'AUX',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {}),\n              'rel': 'aux'},\n             6: {'address': 6,\n              'word': 'the',\n              'lemma': 'the',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 7,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             7: {'address': 7,\n              'word': 'flow',\n              'lemma': 'flow',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'det': [6], 'nmod': [11]}),\n              'rel': 'obj'},\n             8: {'address': 8,\n              'word': 'of',\n              'lemma': 'of',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 11,\n              'deps': defaultdict(list, {}),\n              'rel': 'case'},\n             11: {'address': 11,\n              'word': 'River',\n              'lemma': 'River',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 7,\n              'deps': defaultdict(list,\n                          {'case': [8], 'det': [9], 'compound': [10]}),\n              'rel': 'nmod'},\n             9: {'address': 9,\n              'word': 'the',\n              'lemma': 'the',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 11,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             10: {'address': 10,\n              'word': 'Missouri',\n              'lemma': 'Missouri',\n              'ctag': 'PROPN',\n              'tag': 'PROPN',\n              'feats': '',\n              'head': 11,\n              'deps': defaultdict(list, {}),\n              'rel': 'compound'},\n             12: {'address': 12,\n              'word': 'about',\n              'lemma': 'about',\n              'ctag': 'ADV',\n              'tag': 'ADV',\n              'feats': '',\n              'head': 13,\n              'deps': defaultdict(list, {}),\n              'rel': 'advmod'},\n             13: {'address': 13,\n              'word': 'two',\n              'lemma': 'two',\n              'ctag': 'NUM',\n              'tag': 'NUM',\n              'feats': '',\n              'head': 14,\n              'deps': defaultdict(list, {'advmod': [12]}),\n              'rel': 'nummod'},\n             14: {'address': 14,\n              'word': 'weeks',\n              'lemma': 'weeks',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 15,\n              'deps': defaultdict(list, {'nummod': [13]}),\n              'rel': 'obl:npmod'},\n             15: {'address': 15,\n              'word': 'earlier',\n              'lemma': 'earlier',\n              'ctag': 'ADV',\n              'tag': 'ADV',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {'obl:npmod': [14], 'obl': [17]}),\n              'rel': 'advmod'},\n             16: {'address': 16,\n              'word': 'than',\n              'lemma': 'than',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 17,\n              'deps': defaultdict(list, {}),\n              'rel': 'case'},\n             17: {'address': 17,\n              'word': 'normal',\n              'lemma': 'normal',\n              'ctag': 'ADJ',\n              'tag': 'ADJ',\n              'feats': '',\n              'head': 15,\n              'deps': defaultdict(list, {'case': [16]}),\n              'rel': 'obl'},\n             18: {'address': 18,\n              'word': 'because',\n              'lemma': 'because',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list, {'fixed': [19]}),\n              'rel': 'case'},\n             22: {'address': 22,\n              'word': 'levels',\n              'lemma': 'levels',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list,\n                          {'case': [18],\n                           'amod': [20],\n                           'compound': [21],\n                           'nmod': [25]}),\n              'rel': 'obl'},\n             19: {'address': 19,\n              'word': 'of',\n              'lemma': 'of',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 18,\n              'deps': defaultdict(list, {}),\n              'rel': 'fixed'},\n             20: {'address': 20,\n              'word': 'low',\n              'lemma': 'low',\n              'ctag': 'ADJ',\n              'tag': 'ADJ',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list, {}),\n              'rel': 'amod'},\n             21: {'address': 21,\n              'word': 'water',\n              'lemma': 'water',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list, {}),\n              'rel': 'compound'},\n             23: {'address': 23,\n              'word': 'in',\n              'lemma': 'in',\n              'ctag': 'ADP',\n              'tag': 'ADP',\n              'feats': '',\n              'head': 25,\n              'deps': defaultdict(list, {}),\n              'rel': 'case'},\n             25: {'address': 25,\n              'word': 'reservoirs',\n              'lemma': 'reservoirs',\n              'ctag': 'NOUN',\n              'tag': 'NOUN',\n              'feats': '',\n              'head': 22,\n              'deps': defaultdict(list,\n                          {'case': [23], 'det': [24], 'acl:relcl': [27]}),\n              'rel': 'nmod'},\n             24: {'address': 24,\n              'word': 'the',\n              'lemma': 'the',\n              'ctag': 'DET',\n              'tag': 'DET',\n              'feats': '',\n              'head': 25,\n              'deps': defaultdict(list, {}),\n              'rel': 'det'},\n             26: {'address': 26,\n              'word': 'that',\n              'lemma': 'that',\n              'ctag': 'PRON',\n              'tag': 'PRON',\n              'feats': '',\n              'head': 27,\n              'deps': defaultdict(list, {}),\n              'rel': 'nsubj'},\n             27: {'address': 27,\n              'word': 'feed',\n              'lemma': 'feed',\n              'ctag': 'VERB',\n              'tag': 'VERB',\n              'feats': '',\n              'head': 25,\n              'deps': defaultdict(list, {'nsubj': [26], 'obj': [28]}),\n              'rel': 'acl:relcl'},\n             28: {'address': 28,\n              'word': 'it',\n              'lemma': 'it',\n              'ctag': 'PRON',\n              'tag': 'PRON',\n              'feats': '',\n              'head': 27,\n              'deps': defaultdict(list, {}),\n              'rel': 'obj'},\n             29: {'address': 29,\n              'word': '.',\n              'lemma': '.',\n              'ctag': 'PUNCT',\n              'tag': 'PUNCT',\n              'feats': '',\n              'head': 5,\n              'deps': defaultdict(list, {}),\n              'rel': 'punct'}})"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_stanza[0].nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T19:31:11.866322300Z",
     "start_time": "2023-07-16T19:31:11.799323500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Are the dependency tags of spacy are the same of stanza?</b>\n",
    "My Opinion: No, they are not the same. For example, the root tag of spacy is 'ROOT' and the root tag of stanza is 'root'. Also, the tag of the subject of spacy is 'subj' and the tag of the subject of stanza is 'nsubj'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Evaluate the parsers using the DependencyEvaluator. Which one performs better?</b>\n",
    "My Opinion: The spacy parser performs better than the stanza parser. The spacy parser has a LAS of 0.78 and a UAS of 0.85, while the stanza parser has a LAS of 0.76 and a UAS of 0.83."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
