{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - different methods of representing words as vectors\n",
    "    - vectors and similarity between vectors\n",
    "    - evaluation of word embeddings\n",
    "    \n",
    "- Learning how to:\n",
    "    - train word embeddings with gensim\n",
    "    - use pre-trained word embeddings for similarity computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/)\n",
    "- tqdm\n",
    "- matplotlib\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "\n",
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/6.pdf) Chapter 6: Vector Semantics and Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Words as Vectors (Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), [**word embedding**](https://en.wikipedia.org/wiki/Word_embedding) is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings is the process by which words are transformed into vectors of (real) numbers.\n",
    "- Definition of meaning by distributional similarity / usage: similar words are close in \"space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. One-Hot Encoding\n",
    "- sparse vectors\n",
    "- most basic way to turn a token into a vector\n",
    "- method\n",
    "    - associate a unique integer index with every word in a vocabulary of size $V$\n",
    "    - turn this integer index $i$ into a binary vector of size $V$ (i.e. the size of the vocabulary)\n",
    "    - the vector has all values `0` except for the $i$th entry, which is `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Co-Occurence Matrices and Word as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Term-Document Matrix\n",
    "- could be used to represent words, where dimension are documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. TF-IDF\n",
    "- sparse vectors\n",
    "- generally used to represent documents, where dimensions are words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TF: Term Frequency\n",
    "$$\\text{tf}_{t,d} = \\text{count}(t,d)$$\n",
    "$$\\text{tf}_{t,d} = \\log_{10}(\\text{count}(t,d) + 1)$$\n",
    "\n",
    "`+1` is because log of 0 is undefined.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(t,d)), & \\text{if count}(t,d) > 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "$$\\text{idf}_t = \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "Usually in log space, like term frequency.\n",
    "\n",
    "$$\\text{idf}_t = \\log_{10}(\\frac{N}{\\text{df}_t})$$\n",
    "\n",
    "- $\\text{df}_t$ is the number of documents in which term $t$ occurs\n",
    "- $N$ is the total number of documents in the collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The __tf-idf__ weighted value $w_{t,d}$ for word $t$ in document $d$ is the combination of $\\text{tf}_{t,d}$ and $\\text{idf}_t$:\n",
    "\n",
    "$$w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Term-Term Matrix\n",
    "- a.k.a. \"word-word\" or \"word-context\" matrix\n",
    "- words are represented by a function of the counts of nearby words \n",
    "- size $|V| \\times |V|$, where $V$ is the vocabulary size\n",
    "    - usually context is taken to be a document or words in a window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. Pointwise Mutual Information (PMI) and Positive Pointwise Mutual Information (PPMI)\n",
    "- used for term-term matrices\n",
    "- \"the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.1. Pointwise Mutual Information (PMI)\n",
    "- a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$I(x, y) = \\log_2 \\frac{P(x, y)}{P(x)P(y)}$$\n",
    "\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.2. Positive Pointwise Mutual Information (PMI)\n",
    "- PMI values range from negative to positive infinity.\n",
    "- negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable\n",
    "- it is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.3. PPMI Matrix\n",
    "To get a PPMI matrix from a co-occurrence matrix $F$, where $W$ rows are words and $C$ columns are contexts, and $f_{ij}$ is the number of times word $w_i$ appears in context $c_j$ (i.e. value of the cell).\n",
    "\n",
    "$$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(w) = \\frac{\\sum_{j=1}^C f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(c) = \\frac{\\sum_{i=1}^W f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PMI has the problem of being biased toward infrequent events: very rare words tend to have very high PMI values.\n",
    "- Thus, $P(c)$ is computed as $P_{\\alpha}(c)$ that raises the probability of the context word to the power of $\\alpha$ (e.g. $0.75$)\n",
    "    - Alternative is Laplace smoothing\n",
    "\n",
    "$$\\text{PPMI}_{\\alpha}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P_{\\alpha}(c)}, 0)$$\n",
    "\n",
    "$$P_{\\alpha}(c) = \\frac{\\text{count}(c)^{\\alpha}}{\\sum_{c}\\text{count}(c)^{\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Training Word Embeddings with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.1. Word2Vec\n",
    "- dense vectors\n",
    "- representation is created by training a classifier to distinguish nearby and far-away words\n",
    "- Variants\n",
    "    - SKIP-GRAM\n",
    "        - given the target predict the context i.e. $P(w_0, w_1, w_{n-1}|w_n)$\n",
    "    - CBOW (Continuous Bag of Words)\n",
    "        - it's the opposite of skip-gram, given the context predict the target i.e. $P(w_n | w_0, w_1, w_{n-1})$\n",
    "- Refer to [documentation](https://radimrehurek.com/gensim/models/word2vec.html) for details\n",
    "- [Tutorial](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T18:41:12.189508300Z",
     "start_time": "2023-07-14T18:41:05.286181400Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: Levenshtein==0.21.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from python-Levenshtein) (0.21.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from Levenshtein==0.21.0->python-Levenshtein) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from gensim) (6.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T18:41:24.845727400Z",
     "start_time": "2023-07-14T18:41:19.590511200Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from gensim.models import Word2Vec\n",
    "data = ['Iceland is faraway from Padova', 'Rome is the capital of Italy', 'Paris is a big city']\n",
    "model = Word2Vec(sentences=[d.split() for d in data], vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:23:25.647381100Z",
     "start_time": "2023-06-07T01:23:25.606627400Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=14, vector_size=10, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:23:00.794009300Z",
     "start_time": "2023-06-07T01:23:00.733993Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01631476  0.00189917  0.03473637  0.00217777  0.09618826  0.05060603\n",
      " -0.0891739  -0.0704156   0.00901456  0.06392534]\n",
      "[('faraway', 0.5111488103866577), ('Italy', 0.2914133667945862), ('Iceland', 0.07346687465906143)]\n"
     ]
    }
   ],
   "source": [
    "# getting word vectors\n",
    "print(model.wv['Rome'])\n",
    "# getting most similar\n",
    "print(model.wv.most_similar('Rome', topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:20:59.337371Z",
     "start_time": "2023-06-07T01:20:59.315370900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Vector Similarity\n",
    "- two words are similar in meaning if their context __vectors__ are similar\n",
    "- __Cosine similarity__ measures the similarity between two vectors of an __inner product space__. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Dot Product\n",
    "\n",
    "- dot product (inner product)\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum^N_{i=1}v_i w_i = v_1 w_1 + v_2 w_2 + ... + v_N w_N$$\n",
    "\n",
    "- vector length (L2 norm $||v||_2$)\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{\\sum^N_{i=1} v_i^2}$$ \n",
    "\n",
    "$$ |\\vec{v}| = \\sqrt{\\vec{v}\\cdot\\vec{v}} = \\sqrt{\\sum^N_{i=1} v_i v_i} = \\sqrt{\\sum^N_{i=1} v_1 v_1 + v_2 v_2 + ... + v_N v_N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Cosine Similarity\n",
    "\n",
    "- L2 normalized dot product of 2 vectors\n",
    "    - $\\theta$ is the angle between $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = |\\vec{v}||\\vec{w}|\\cos\\theta$$\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$$\n",
    "\n",
    "$$\\text{CosSim}(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum^N_{i=1}v_i w_i}{\\sqrt{\\sum^N_{i=1} v_i^2} \\sqrt{\\sum^N_{i=1} w_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cosine Distance\n",
    "$$\\text{Cosine Distance}(\\vec{v}, \\vec{w}) = 1 - \\text{Cosine Similarity}(\\vec{v}, \\vec{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n",
    "- Implement a function to compute __cosine similarity__ using `numpy` methods\n",
    "    - `np.dot`\n",
    "    - `norm`\n",
    "- Using the defined functions\n",
    "    - compute cosine similarity between two word embeddings for instance `Rome` and `city` or `Paris` and `Rome`\n",
    "    - compare similarity values to the cosine similarity using the output of (`scipy.spatial.distance.cosine`)\n",
    "        - i.e. use *distance* to compute *similarity*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:23:32.304911Z",
     "start_time": "2023-06-07T01:23:32.277252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04265024\n",
      "0.9573497511446476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v, w) / (norm(v) * norm(w))\n",
    "\n",
    "rome = model.wv['Rome']\n",
    "paris = model.wv['Paris']\n",
    "print(cosine_similarity(rome, paris))\n",
    "# print cosine similarity using distance\n",
    "print(cosine(rome, paris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Pre-Trained Embeddings\n",
    "- Training embeddings is computationally expensive\n",
    "- Many pre-trained models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:21:58.966731800Z",
     "start_time": "2023-06-07T01:21:11.680243800Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# Download the 'word2vec-google-news-300' embeddings\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:21:59.058732400Z",
     "start_time": "2023-06-07T01:21:58.970736600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23535156,  0.18652344, -0.0390625 ,  0.31445312, -0.01019287,\n",
       "        0.09375   , -0.3203125 , -0.01635742, -0.06347656,  0.22167969,\n",
       "       -0.17382812,  0.04492188,  0.10595703,  0.06298828, -0.08300781,\n",
       "       -0.03808594, -0.06982422, -0.05395508, -0.00891113,  0.14160156,\n",
       "        0.08984375,  0.0703125 ,  0.2890625 , -0.06079102,  0.3515625 ,\n",
       "        0.01855469,  0.03833008,  0.34375   , -0.24511719, -0.00958252,\n",
       "        0.12060547, -0.04248047, -0.31445312,  0.109375  , -0.15039062,\n",
       "       -0.31054688, -0.01452637,  0.16015625, -0.04711914,  0.14453125,\n",
       "        0.13183594,  0.05541992,  0.34570312,  0.19921875,  0.12695312,\n",
       "        0.0378418 ,  0.07519531,  0.38085938, -0.0135498 ,  0.24414062,\n",
       "        0.01635742,  0.22851562, -0.04638672, -0.1953125 , -0.22949219,\n",
       "        0.18554688, -0.16601562, -0.11914062, -0.19726562, -0.04199219,\n",
       "        0.0859375 ,  0.09765625,  0.02624512, -0.07226562, -0.01055908,\n",
       "       -0.10839844, -0.24804688, -0.03808594,  0.15722656, -0.17382812,\n",
       "       -0.19042969, -0.12304688, -0.17871094,  0.171875  , -0.06933594,\n",
       "       -0.04174805,  0.18164062,  0.07617188, -0.09277344,  0.11962891,\n",
       "       -0.08544922,  0.04956055,  0.12011719, -0.16113281, -0.00216675,\n",
       "       -0.17578125, -0.04980469, -0.23730469,  0.10058594,  0.06982422,\n",
       "       -0.01153564, -0.12207031, -0.3515625 , -0.171875  , -0.06542969,\n",
       "       -0.27539062, -0.02709961, -0.23144531, -0.31640625, -0.15527344,\n",
       "       -0.43164062,  0.16699219,  0.11669922,  0.00146484,  0.00224304,\n",
       "       -0.02124023, -0.18554688, -0.19140625, -0.20019531, -0.10742188,\n",
       "        0.05224609,  0.2734375 ,  0.29296875, -0.20898438,  0.00144196,\n",
       "       -0.07714844, -0.08203125, -0.16210938,  0.24023438,  0.30664062,\n",
       "       -0.28710938, -0.25195312,  0.16894531, -0.12353516,  0.14941406,\n",
       "       -0.43164062, -0.04077148,  0.1640625 , -0.2734375 ,  0.15234375,\n",
       "       -0.37109375, -0.16015625, -0.3671875 , -0.12695312,  0.01989746,\n",
       "       -0.11669922, -0.19140625,  0.14257812,  0.20703125, -0.01342773,\n",
       "        0.33203125,  0.17382812,  0.08789062, -0.2109375 ,  0.26171875,\n",
       "        0.01086426,  0.16308594, -0.00320435,  0.17578125,  0.00323486,\n",
       "       -0.01794434,  0.24707031,  0.1875    ,  0.16796875,  0.30859375,\n",
       "        0.0456543 ,  0.09570312,  0.17382812,  0.10791016,  0.03686523,\n",
       "       -0.0456543 ,  0.17578125, -0.02929688, -0.16308594,  0.20898438,\n",
       "        0.1875    ,  0.12109375,  0.01599121,  0.05517578,  0.08691406,\n",
       "       -0.16503906, -0.20800781,  0.10009766,  0.20214844,  0.00662231,\n",
       "       -0.2578125 , -0.03613281, -0.12597656,  0.00335693, -0.2734375 ,\n",
       "       -0.09033203, -0.03149414,  0.07958984, -0.05664062,  0.25195312,\n",
       "       -0.07177734,  0.10546875, -0.0612793 , -0.11767578, -0.07958984,\n",
       "        0.12451172,  0.109375  , -0.02966309,  0.13867188, -0.27539062,\n",
       "        0.24414062,  0.33984375, -0.03662109,  0.15722656, -0.25195312,\n",
       "        0.23535156,  0.09814453, -0.27148438,  0.078125  ,  0.18652344,\n",
       "       -0.31835938,  0.15039062,  0.04077148,  0.27734375, -0.11083984,\n",
       "       -0.23535156,  0.06347656, -0.09130859, -0.20117188, -0.15429688,\n",
       "        0.10400391, -0.10107422,  0.19628906,  0.13964844, -0.13574219,\n",
       "        0.20605469, -0.4375    ,  0.1484375 ,  0.12597656, -0.00183868,\n",
       "        0.13964844, -0.08300781,  0.15039062,  0.0255127 ,  0.11962891,\n",
       "        0.16992188,  0.20898438, -0.0546875 , -0.05151367,  0.01257324,\n",
       "       -0.07519531, -0.10791016,  0.12988281,  0.16113281, -0.06103516,\n",
       "        0.23632812,  0.06787109,  0.19042969,  0.1640625 , -0.13867188,\n",
       "        0.09619141, -0.19433594, -0.18066406,  0.05004883, -0.01818848,\n",
       "        0.23144531, -0.09082031, -0.08642578, -0.02160645,  0.13085938,\n",
       "        0.09521484,  0.13574219, -0.13085938,  0.19628906,  0.00994873,\n",
       "        0.21289062,  0.12988281,  0.09765625,  0.24511719, -0.06005859,\n",
       "        0.16210938,  0.11767578, -0.00512695, -0.09228516,  0.12158203,\n",
       "        0.02746582, -0.03100586, -0.10644531,  0.03027344,  0.12792969,\n",
       "       -0.08447266, -0.01794434, -0.08154297,  0.24316406,  0.07910156,\n",
       "       -0.23339844,  0.38085938,  0.13769531, -0.06591797, -0.21679688,\n",
       "        0.30273438, -0.171875  ,  0.07275391, -0.39257812,  0.00946045,\n",
       "       -0.01373291, -0.06982422, -0.27148438,  0.23632812,  0.14453125,\n",
       "        0.13964844, -0.31054688, -0.16894531,  0.08447266, -0.00430298],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['Rome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T01:21:59.058732400Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v.most_similar('Rome', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar('Paris', topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Word Embeddings in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> To make them compact and fast, spaCy's small pipeline packages (all packages that end in `sm`) don't ship with word vectors, and only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens -- but the result won't be as good, and individual tokens won't have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:\n",
    "\n",
    "> `python -m spacy download en_core_web_lg`\n",
    "\n",
    "> Pipeline packages that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an __average of their token vectors__. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n",
    "\n",
    "> Each `Doc`, `Span`, `Token` and `Lexeme` comes with a `.similarity` method that lets you compare it with another object, and determine the similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.1. Accessing Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:23:58.012103700Z",
     "start_time": "2023-06-07T01:23:50.224860500Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: Rome\n",
      "vector dimension: 300\n",
      "spacy vector norm: 54.82853\n",
      "numpy vector norm: 54.82853\n",
      "numpy linalg norm: 54.82853\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "txt = 'Rome is the capital of Italy'\n",
    "doc = nlp(txt)\n",
    "\n",
    "tok = doc[0]  # let's take Rome\n",
    "\n",
    "print(\"string:\", tok.text)\n",
    "\n",
    "print(\"vector dimension:\", len(tok.vector))\n",
    "print(\"spacy vector norm:\", tok.vector_norm)\n",
    "print(\"numpy vector norm:\", np.sqrt(np.dot(tok.vector, tok.vector)))\n",
    "print(\"numpy linalg norm:\", np.linalg.norm(tok.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:03.096006600Z",
     "start_time": "2023-06-07T01:24:03.045601900Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "spacy CosSim(Rome, Paris): 0.6117807626724243\n",
      "scipy CosSim(Rome, Paris): 0.6117808222770691\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# let's get Paris & compare its vector to rome\n",
    "paris = nlp('Paris')[0]\n",
    "print(paris.text)\n",
    "\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, paris.text), tok.similarity(paris))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, paris.text), 1 - cosine(tok.vector, paris.vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Train your own Word Embeddings\n",
    "One way to train word embeddings is to use a language model. We have already seen language models in Lab 3, but now we are going to develop a language model using a neural architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Task definition\n",
    "To model the probaiblity distribution over a sequence, we are going to use the Chain Rule as we have seen in LAB 3:\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "However, at that time we have used ngram to trucate the previous context ($N-1$), in order to compute meaningfull probabilities. While using neural models, we will let the model to decide by itself how to manage the previous contex and thus which are the tokens relevant for the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 RNNs are the most suitable architacture\n",
    "One of most suitable neural architecture for the Language Model task is the Recurrent Neural Network. The architecture is composed of a RNN layer (vanilla, LSTM, GRU) and a softmax that outputs the probability over the dictionary. Indeed the size of the output vector is equal to the size of the dictionary, i.e. the model cannot predict tokens that are not present in vocabularly. <br>\n",
    "> LM task in RNN can be tackled as a sequence labelling task (i.e. len of input and output sequence are always the same) in which the input sequence is $ input = \\{w_1, w_2, w_{n-1}\\}$ and the output is $ output = \\{w_2, w_2, w_{n}\\}$\n",
    ">\n",
    "> **Example** our sentence is ***\"I go to Miami\"*** the input sequence would be ***\"I go to\"*** and the output is ***\"go to Miami\"***. \n",
    ">\n",
    "> Notice: \n",
    "> - To proper model the sequence probabilities we need add boundary markers \\<s\\> and \\</s\\>.\n",
    "> - However in LM RNN only the end of sentence token \\</s\\>  is usually used unless we need for some reason (e.g. in ASR) to compute the probability distribution of the first token of a sentence. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/zGH99MFY/rnn-lm.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "In the image below you can see a working example of a language model with RNN. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/fydQNrYP/LM-RNN.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:00:04.783245800Z",
     "start_time": "2023-06-07T01:00:01.708241400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# RNN Elman version\n",
    "# We are not going to use this since for efficienty purposes it's better to use the RNN layer provided by pytorch  \n",
    "\n",
    "class RNN_cell(nn.Module):\n",
    "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, prev_hidden, word):\n",
    "        input_emb = self.W(word)\n",
    "        prev_hidden_rep = self.U(prev_hidden)\n",
    "        # ht = σ(Wx + Uht-1 + b)\n",
    "        hidden_state = self.sigmoid(x + prev_hidden_rep)\n",
    "        # yt = σ(Vht + b)\n",
    "        output = self.output(hidden_state)\n",
    "        return hidden_state, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:13.249184400Z",
     "start_time": "2023-06-07T01:24:13.208152200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Data loading \n",
    "For sake of time we are going to see this part in detail in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:15.255158500Z",
     "start_time": "2023-06-07T01:24:15.249153200Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:17.008127900Z",
     "start_time": "2023-06-07T01:24:16.942749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:17.288906300Z",
     "start_time": "2023-06-07T01:24:17.174905400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set \n",
    "# However you can compute it for dev and test just for statistics about OOV \n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:17.604660Z",
     "start_time": "2023-06-07T01:24:17.593659600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:17.839662Z",
     "start_time": "2023-06-07T01:24:17.834661900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:18.170658400Z",
     "start_time": "2023-06-07T01:24:18.040660300Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:18.300658900Z",
     "start_time": "2023-06-07T01:24:18.274661400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:20.377660Z",
     "start_time": "2023-06-07T01:24:18.533660400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:20.389663400Z",
     "start_time": "2023-06-07T01:24:20.373662800Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Train and validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:20.825455500Z",
     "start_time": "2023-06-07T01:24:20.781457800Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:28.792894800Z",
     "start_time": "2023-06-07T01:24:21.672266200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 100\n",
    "emb_size = 150\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.1 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T00:11:15.373555600Z",
     "start_time": "2023-06-07T00:11:15.354551800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T00:45:33.681261900Z",
     "start_time": "2023-06-07T00:11:21.513055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 331.672248: 100%|██████████| 99/99 [34:10<00:00, 20.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  316.9268679431873\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "\n",
    "# Set max_split_size_mb to avoid fragmentation of the GPU memory\n",
    "torch.backends.cuda.max_split_size_mb = 512  # Adjust the value according to your needs\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.max_memory_allocated(device=device) / 1024 ** 2\n",
    "gc.collect()\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "#If the PPL is too high try to change the learning rate\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "                          \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print('Test ppl: ', final_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model makes you happy and you want to reuse it, you have [to save it and load it](https://pytorch.org/tutorials/beginner/saving_loading_models.html). \n",
    "In pytorch this is super straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:28.839894300Z",
     "start_time": "2023-06-07T01:24:28.795900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LM_RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive/MyDrive/NLU_LABs/model_bin/lab-9-prof-model.pt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# torch.save(model.state_dict(), path)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# To load the model you need to initialize it\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLM_RNN\u001B[49m(emb_size, hid_size, vocab_len, pad_index\u001B[38;5;241m=\u001B[39mlang\u001B[38;5;241m.\u001B[39mword2id[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<pad>\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Then you load it\u001B[39;00m\n\u001B[0;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(path))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'LM_RNN' is not defined"
     ]
    }
   ],
   "source": [
    "# # To save the model\n",
    "path = 'model_bin/model_name.pt'\n",
    "# torch.save(model.state_dict(), path)\n",
    "# To load the model you need to initialize it\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "# Then you load it\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7 Evaluation: Analogy Task\n",
    "In the word analogy task, we complete the sentence of the form\n",
    "\n",
    "\"$w_1$ is to $w_2$ as $w_3$ is to $w4$\", where $w_4$ is a blank. \n",
    "\n",
    "For instance:\n",
    "\n",
    "\"*man* is to *woman* as *king* is to **__**\", and our goal is to guess the missing word (*queen*)\n",
    "\n",
    "The task is approached using cosine similarity between vector differences: \n",
    "\n",
    "$$\\vec{w_2} - \\vec{w_1} \\approx \\vec{w_4} - \\vec{w_3}$$\n",
    "\n",
    "$$\\vec{w_4} \\approx = \\vec{w_3} + \\vec{w_2} - \\vec{w_1}$$\n",
    "\n",
    "$$w = \\arg\\max_{w \\in V}(\\vec{w} \\cdot (\\vec{w_3} + \\vec{w_2} - \\vec{w_1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w = \\arg\\max_{w \\in V}\\text{CosSim}(\\vec{w_2} - \\vec{w_1}, \\vec{w} - \\vec{w_3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Analogy using Most Similar\n",
    "> For each of the given vectors, find the `n` most similar entries to it by cosine. \n",
    "Queries are by vector. Results are returned as a (`keys`, `best_rows`, `scores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:26:58.266101100Z",
     "start_time": "2023-06-07T01:26:58.251996200Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_spacy(w1, w2, w3):\n",
    "    v1 = nlp.vocab[w1].vector\n",
    "    v2 = nlp.vocab[w2].vector\n",
    "    v3 = nlp.vocab[w3].vector\n",
    "    \n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "   \n",
    "    # n=1 & sorted by default\n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([rv]), n=10)\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0][0]):\n",
    "        print(nlp.vocab.strings[key], ms[2][0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:26:59.108702500Z",
     "start_time": "2023-06-07T01:26:59.099701200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:48:08.746475100Z",
     "start_time": "2023-06-07T01:48:08.335476300Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analogy_spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43manalogy_spacy\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwoman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mking\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'analogy_spacy' is not defined"
     ]
    }
   ],
   "source": [
    "print(analogy_spacy('man', 'woman', 'king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Write a function that computes the analogy with our RNN based model\n",
    "- Compare Spacy and our RNN based model (just try a couple of examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:39.216058300Z",
     "start_time": "2023-06-07T01:24:39.194015700Z"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_our_model(w1, w2, w3, model, lang):\n",
    "    model.eval().to('cpu')\n",
    "    \n",
    "    # Suggest: make use of torch.LongTensor and check if the word is in the vocab\n",
    "    # Get word ids\n",
    "    temp_w1 = lang.word2id[w1]\n",
    "    temp_w2 = lang.word2id[w2]\n",
    "    temp_w3 = lang.word2id[w3]\n",
    "\n",
    "    # Get word vectors\n",
    "    v1 = model.get_word_embedding(torch.LongTensor([temp_w1]))\n",
    "    v2 = model.get_word_embedding(torch.LongTensor([temp_w2]))\n",
    "    v3 = model.get_word_embedding(torch.LongTensor([temp_w3]))\n",
    "\n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "\n",
    "    # Get the most similar word\n",
    "    ms = model.get_most_similar(rv, top_k=10)\n",
    "\n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0]):\n",
    "        print(lang.id2word[key], ms[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:24:40.567961900Z",
     "start_time": "2023-06-07T01:24:40.463963200Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stakes 0.6451224\n",
      "argue 0.41147023\n",
      "has 0.30575985\n",
      "efforts 0.2905369\n",
      "novelist 0.27574766\n",
      "teeth 0.27525675\n",
      "hidden 0.26221195\n",
      "chips 0.25811046\n",
      "disadvantage 0.25252596\n",
      "motion 0.25185156\n"
     ]
    }
   ],
   "source": [
    "# Our model is trained on WSJ news queen and king should be OOV or very rare tokens\n",
    "# Try with different words\n",
    "analogy_our_model('man', 'woman', 'u.s.', model, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:26:48.311119300Z",
     "start_time": "2023-06-07T01:26:48.228117400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valley 0.6443743\n",
      "stakes 0.63430893\n",
      "woolworth 0.30361828\n",
      "shamir 0.28658432\n",
      "interested 0.26614782\n",
      "gillett 0.26429635\n",
      "agnelli 0.262242\n",
      "users 0.25788736\n",
      "basir 0.25620177\n",
      "breaker 0.24996667\n"
     ]
    }
   ],
   "source": [
    "# Our model is trained on WSJ news queen and king should be OOV or very rare tokens\n",
    "analogy_our_model('a', 'woman', 'queen', model, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1 (2 points)\n",
    "Modify the baseline LM_RNN (the idea is to add a set of improvements and see how these affect the performance). Furthremore, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration. Here are the links to the state-of-the-art papers which uses vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf). \n",
    "- Replace RNN with LSTM (output the PPL)\n",
    "- Add two dropout layers: (output the PPL)\n",
    "    - one on embeddings, \n",
    "    - one on the output\n",
    "- Replace SGD with AdamW (output the PPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:27.197878300Z",
     "start_time": "2023-06-08T21:12:23.248619400Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from numpy.linalg import norm\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:28.521183700Z",
     "start_time": "2023-06-08T21:12:28.492185300Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v, w) / (norm(v) * norm(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the RNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:29.944989Z",
     "start_time": "2023-06-08T21:12:29.855994Z"
    }
   },
   "outputs": [],
   "source": [
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "\n",
    "        # Token ids to vectors, we will better see this in the next lab\n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)  # Added Dropout layer after embedding\n",
    "        # Pytorch LSTM layer\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False)  # Replaced RNN with LSTM\n",
    "        self.out_dropout = nn.Dropout(out_dropout)  # Added Dropout layer before output\n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        emb = self.emb_dropout(emb)  # Applied Dropout after embedding\n",
    "        rnn_out, _ = self.rnn(emb)\n",
    "        rnn_out = self.out_dropout(rnn_out)  # Applied Dropout before output\n",
    "        output = self.output(rnn_out).permute(0, 2, 1)\n",
    "        return output\n",
    "\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens\n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]\n",
    "        top_scores = scores[indexes]\n",
    "\n",
    "        return (indexes, top_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:31.265995Z",
     "start_time": "2023-06-08T21:12:31.225992Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0\n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:31.862990Z",
     "start_time": "2023-06-08T21:12:31.815990600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:33.028673300Z",
     "start_time": "2023-06-08T21:12:32.883672200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set\n",
    "# However you can compute it for dev and test just for statistics about OOV\n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:33.426719600Z",
     "start_time": "2023-06-08T21:12:33.391717600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:34.098717900Z",
     "start_time": "2023-06-08T21:12:34.073717900Z"
    }
   },
   "outputs": [],
   "source": [
    "class PennTreeBank(data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "\n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "\n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "\n",
    "    # Auxiliary methods\n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:35.000716400Z",
     "start_time": "2023-06-08T21:12:34.977717600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        \"\"\"\n",
    "        :param corpus:\n",
    "        :param special_tokens:\n",
    "        \"\"\"\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "\n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        \"\"\"\n",
    "        description: function to get the vocabulary of the corpus\n",
    "        :param corpus:\n",
    "        :param special_tokens:\n",
    "        :return: dict of word to id\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        i = 0\n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:36.095978300Z",
     "start_time": "2023-06-08T21:12:35.994979Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:38.376978Z",
     "start_time": "2023-06-08T21:12:36.629978Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:39.767652600Z",
     "start_time": "2023-06-08T21:12:39.746652900Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len\n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "\n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:12:41.232724900Z",
     "start_time": "2023-06-08T21:12:41.195728600Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step() # Update the weights\n",
    "\n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "\n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T22:25:10.834815200Z",
     "start_time": "2023-06-08T22:25:10.774810600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes\n",
    "# A large model tends to overfit\n",
    "hid_size = 50\n",
    "emb_size = 100\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.1 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T22:25:12.874869800Z",
     "start_time": "2023-06-08T22:25:12.828801400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T22:54:08.173649500Z",
     "start_time": "2023-06-08T22:25:14.075522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 454.034543: 100%|██████████| 99/99 [28:53<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  437.30882551999696\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "\n",
    "# Set max_split_size_mb to avoid fragmentation of the GPU memory\n",
    "torch.backends.cuda.max_split_size_mb = 512  # Adjust the value according to your needs\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.max_memory_allocated(device=device) / 1024 ** 2\n",
    "gc.collect()\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "\n",
    "#If the PPL is too high try to change the learning rate\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n",
    "print('Test ppl: ', final_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model makes you happy and you want to reuse it, you have [to save it and load it](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "In pytorch this is super straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T22:22:39.591749900Z",
     "start_time": "2023-06-08T22:22:39.488714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # To save the model\n",
    "path = 'model_bin/lab-9_model_rnn_lstm-lr-0.1_hs-100_es-150.pt'\n",
    "torch.save(model.state_dict(), path)\n",
    "# To load the model you need to initialize it\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "# Then you load it\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (4 points)\n",
    "Add to best model of Exercise 1 the following regularizations described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ):\n",
    "- Weight Tying (PPL)\n",
    "- Variational Dropout (PPL)\n",
    "- Non-monotonically Triggered AvSGD (PPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.533288600Z",
     "start_time": "2023-07-17T16:21:03.122881900Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from numpy.linalg import norm\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.544289900Z",
     "start_time": "2023-07-17T16:21:06.534292100Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v, w) / (norm(v) * norm(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.588548400Z",
     "start_time": "2023-07-17T16:21:06.548289500Z"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, p=0.5, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "        m = x.data.new(x.size(0), 1, x.size(2) if self.batch_first else 1).bernoulli_(1 - self.p)\n",
    "        mask = m.div_(1 - self.p)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.589550200Z",
     "start_time": "2023-07-17T16:21:06.570548800Z"
    }
   },
   "outputs": [],
   "source": [
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1, tied_weights=False):\n",
    "        super(LM_RNN, self).__init__()\n",
    "\n",
    "        # Token ids to vectors, we will better see this in the next lab\n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # self.emb_dropout = nn.Dropout(emb_dropout)  # Added Dropout layer after embedding\n",
    "        # added variational dropout\n",
    "        self.emb_dropout = VariationalDropout(emb_dropout)\n",
    "        # Pytorch LSTM layer\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False)  # Replaced RNN with LSTM\n",
    "\n",
    "        if tied_weights:\n",
    "            # Linear layer to project the hidden layer to our output space\n",
    "            self.output = nn.Linear(hidden_size, output_size, bias=False)  # no bias if weights are tied\n",
    "            self.output.weight = self.embedding.weight  # tie weights\n",
    "        else:\n",
    "            # Linear layer to project the hidden layer to our output space\n",
    "            self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # self.out_dropout = nn.Dropout(out_dropout)  # Added Dropout layer before output\n",
    "        # added variational dropout to output\n",
    "        self.out_dropout = VariationalDropout(out_dropout)\n",
    "        self.pad_token = pad_index\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        emb = self.emb_dropout(emb)  # Applied Dropout after embedding\n",
    "        rnn_out, _ = self.rnn(emb)\n",
    "        rnn_out = self.out_dropout(rnn_out)  # Applied Dropout before output\n",
    "        output = self.output(rnn_out).permute(0, 2, 1)\n",
    "        return output\n",
    "\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        # Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens\n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]\n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.608549400Z",
     "start_time": "2023-07-17T16:21:06.597552700Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0\n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.650550400Z",
     "start_time": "2023-07-17T16:21:06.613551200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0\n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.650550400Z",
     "start_time": "2023-07-17T16:21:06.628554Z"
    }
   },
   "outputs": [],
   "source": [
    "class PennTreeBank(data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "\n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1])  # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:])  # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "\n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "\n",
    "    # Auxiliary methods\n",
    "    def mapping_seq(self, data, lang):  # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print(\n",
    "                        'You have to deal with that')  # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.661551900Z",
     "start_time": "2023-07-17T16:21:06.642550400Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len\n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths) == 0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences), max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq  # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "\n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.724549600Z",
     "start_time": "2023-07-17T16:21:06.655550500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6.5 Train and validate the model\n",
    "def train_loop(data, optimizer, criterion, model, average_model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad()  # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward()  # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosion gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "        # Update the average model\n",
    "        # for param, avg_param in zip(model.parameters(), average_model.parameters()):\n",
    "        #     avg_param.data.mul_(0.999).add_(0.001, param.data)\n",
    "\n",
    "        #  Update the average model.\n",
    "        for param, avg_param in zip(model.parameters(), average_model.parameters()):\n",
    "            avg_param.data.mul_(0.9).add_(param.data, alpha=0.1)\n",
    "\n",
    "    return sum(loss_array) / sum(number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.725549200Z",
     "start_time": "2023-07-17T16:21:06.686550200Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad():  # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "\n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:21:06.726549Z",
     "start_time": "2023-07-17T16:21:06.703551500Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0] // 4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx * mul:(idx + 1) * mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0] // 4\n",
    "                        torch.nn.init.orthogonal_(param[idx * mul:(idx + 1) * mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:23:57.867461500Z",
     "start_time": "2023-07-17T16:23:52.693631600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")\n",
    "\n",
    "# Create the vocabulary\n",
    "# Vocab is computed only on training set\n",
    "# However you can compute it for dev and test just for statistics about OOV\n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])\n",
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])\n",
    "\n",
    "# PennTreeBank dataset instantiation\n",
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=256,\n",
    "                          collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]), shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes\n",
    "# A large model tends to overfit\n",
    "hid_size = 100\n",
    "emb_size = 150\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.1  # This is definitely not good for SGD\n",
    "clip = 5  # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], tied_weights=True).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=lr)  # Replaced SGD with AdamW\n",
    "# Changing optimizer to ASGD, which is the basis for NT-ASGD (Non-monotonically Triggered AvSGD)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# make copy of model\n",
    "average_model = copy.deepcopy(model)\n",
    "check_interval = 5  # Set check interval for NT-AvSGD\n",
    "non_monotonic_trigger = 2  # Set trigger for NT-AvSGD\n",
    "last_losses = []  # Store losses of last check_interval epochs\n",
    "\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T16:24:04.354818600Z",
     "start_time": "2023-07-17T16:23:59.700463800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (19712x100 and 150x10001)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# If the PPL is too high try to change the learning rate\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[1;32m---> 22\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     25\u001B[0m         sampled_epochs\u001B[38;5;241m.\u001B[39mappend(epoch)\n",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(data, optimizer, criterion, model, average_model, clip)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[0;32m      8\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Zeroing the gradient\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msource\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(output, sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     11\u001B[0m     loss_array\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m sample[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumber_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1506\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1507\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1509\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1510\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1512\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[4], line 32\u001B[0m, in \u001B[0;36mLM_RNN.forward\u001B[1;34m(self, input_sequence)\u001B[0m\n\u001B[0;32m     30\u001B[0m rnn_out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(emb)\n\u001B[0;32m     31\u001B[0m rnn_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_dropout(rnn_out)  \u001B[38;5;66;03m# Applied Dropout before output\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrnn_out\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1506\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1507\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1509\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1510\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1512\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (19712x100 and 150x10001)"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Set max_split_size_mb to avoid fragmentation of the GPU memory\n",
    "torch.backends.cuda.max_split_size_mb = 512  # Adjust the value according to your needs\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.max_memory_allocated(device=device) / 1024 ** 2\n",
    "gc.collect()\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1, n_epochs))\n",
    "\n",
    "# LR scheduler that reduces LR when a metric has stopped improving (patience=3 means reducing LR after 3 epochs)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=patience, factor=0.1, verbose=True)\n",
    "\n",
    "# If the PPL is too high try to change the learning rate\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, average_model, clip)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if ppl_dev < best_ppl:  # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        # NT-AvSGD\n",
    "        # Add logic for non-monotonic triggering\n",
    "        last_losses.append(loss_dev)\n",
    "        if len(last_losses) > check_interval:\n",
    "            last_losses.pop(0)\n",
    "            # Check if the last check_interval losses are not monotonically decreasing (trigger)\n",
    "            # and switch to average model\n",
    "            if sum(x > y for x, y in zip(last_losses[1:], last_losses[:-1])) >= non_monotonic_trigger:\n",
    "                model.load_state_dict(average_model.state_dict())  # Switch to average model\n",
    "\n",
    "        if patience <= 0:  # Early stopping with patience\n",
    "            break  # Not nice but it keeps the code clean\n",
    "\n",
    "best_model.to(device)\n",
    "final_ppl, _ = eval_loop(test_loader, criterion_eval, best_model)\n",
    "print('Test ppl: ', final_ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T17:42:00.301706300Z",
     "start_time": "2023-06-09T17:42:00.203705800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # To save the model\n",
    "path = 'model_bin/ex2-model_name-2.pt'\n",
    "torch.save(model.state_dict(), path)\n",
    "# To load the model you need to initialize it\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "# Then you load it\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T17:42:08.550601800Z",
     "start_time": "2023-06-09T17:42:08.521601500Z"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_our_model(w1, w2, w3, model, lang):\n",
    "    model.eval().to('cpu')\n",
    "\n",
    "    # Suggest: make use of torch.LongTensor and check if the word is in the vocab\n",
    "    # Get word ids\n",
    "    temp_w1 = lang.word2id[w1]\n",
    "    temp_w2 = lang.word2id[w2]\n",
    "    temp_w3 = lang.word2id[w3]\n",
    "\n",
    "    # Get word vectors\n",
    "    v1 = model.get_word_embedding(torch.LongTensor([temp_w1]))\n",
    "    v2 = model.get_word_embedding(torch.LongTensor([temp_w2]))\n",
    "    v3 = model.get_word_embedding(torch.LongTensor([temp_w3]))\n",
    "\n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "\n",
    "    # Get the most similar word\n",
    "    ms = model.get_most_similar(rv, top_k=10)\n",
    "\n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0]):\n",
    "        print(lang.id2word[key], ms[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T17:42:11.975495100Z",
     "start_time": "2023-06-09T17:42:11.857498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stakes 0.60711277\n",
      "argue 0.5819302\n",
      "banks 0.3156696\n",
      "exercises 0.31024888\n",
      "entrepreneurial 0.27440998\n",
      "recover 0.27205944\n",
      "alternatively 0.26840913\n",
      "whiskey 0.26704118\n",
      "aware 0.26165247\n",
      "mile 0.25744873\n"
     ]
    }
   ],
   "source": [
    "# Our model is trained on WSJ news queen and king should be OOV or very rare tokens\n",
    "# Try with different words\n",
    "analogy_our_model('man', 'woman', 'u.s.', model, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T17:42:13.993500500Z",
     "start_time": "2023-06-09T17:42:13.841501300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stakes 0.5844376\n",
      "valley 0.5793578\n",
      "travel 0.30261374\n",
      "timing 0.2850653\n",
      "cocoa 0.2767574\n",
      "strip 0.27127886\n",
      "mills 0.26965898\n",
      "procedural 0.2546013\n",
      "locations 0.25051957\n",
      "volatile 0.24943632\n"
     ]
    }
   ],
   "source": [
    "# Our model is trained on WSJ news queen and king should be OOV or very rare tokens\n",
    "analogy_our_model('a', 'woman', 'queen', model, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
