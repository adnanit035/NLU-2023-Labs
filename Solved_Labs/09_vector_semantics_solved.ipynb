{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - different methods of representing words as vectors\n",
    "    - vectors and similarity between vectors\n",
    "    - evaluation of word embeddings\n",
    "    \n",
    "- Learning how to:\n",
    "    - train word embeddings with gensim\n",
    "    - use pre-trained word embeddings for similarity computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/)\n",
    "- tqdm\n",
    "- matplotlib\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "\n",
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/6.pdf) Chapter 6: Vector Semantics and Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Words as Vectors (Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), [**word embedding**](https://en.wikipedia.org/wiki/Word_embedding) is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings is the process by which words are transformed into vectors of (real) numbers.\n",
    "- Definition of meaning by distributional similarity / usage: similar words are close in \"space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. One-Hot Encoding\n",
    "- sparse vectors\n",
    "- most basic way to turn a token into a vector\n",
    "- method\n",
    "    - associate a unique integer index with every word in a vocabulary of size $V$\n",
    "    - turn this integer index $i$ into a binary vector of size $V$ (i.e. the size of the vocabulary)\n",
    "    - the vector has all values `0` except for the $i$th entry, which is `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Co-Occurence Matrices and Word as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Term-Document Matrix\n",
    "- could be used to represent words, where dimension are documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. TF-IDF\n",
    "- sparse vectors\n",
    "- generally used to represent documents, where dimensions are words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TF: Term Frequency\n",
    "$$\\text{tf}_{t,d} = \\text{count}(t,d)$$\n",
    "$$\\text{tf}_{t,d} = \\log_{10}(\\text{count}(t,d) + 1)$$\n",
    "\n",
    "`+1` is because log of 0 is undefined.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(t,d)), & \\text{if count}(t,d) > 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "$$\\text{idf}_t = \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "Usually in log space, like term frequency.\n",
    "\n",
    "$$\\text{idf}_t = \\log_{10}(\\frac{N}{\\text{df}_t})$$\n",
    "\n",
    "- $\\text{df}_t$ is the number of documents in which term $t$ occurs\n",
    "- $N$ is the total number of documents in the collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The __tf-idf__ weighted value $w_{t,d}$ for word $t$ in document $d$ is the combination of $\\text{tf}_{t,d}$ and $\\text{idf}_t$:\n",
    "\n",
    "$$w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Term-Term Matrix\n",
    "- a.k.a. \"word-word\" or \"word-context\" matrix\n",
    "- words are represented by a function of the counts of nearby words \n",
    "- size $|V| \\times |V|$, where $V$ is the vocabulary size\n",
    "    - usually context is taken to be a document or words in a window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. Pointwise Mutual Information (PMI) and Positive Pointwise Mutual Information (PPMI)\n",
    "- used for term-term matrices\n",
    "- \"the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.1. Pointwise Mutual Information (PMI)\n",
    "- a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$I(x, y) = \\log_2 \\frac{P(x, y)}{P(x)P(y)}$$\n",
    "\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.2. Positive Pointwise Mutual Information (PMI)\n",
    "- PMI values range from negative to positive infinity.\n",
    "- negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable\n",
    "- it is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.3. PPMI Matrix\n",
    "To get a PPMI matrix from a co-occurrence matrix $F$, where $W$ rows are words and $C$ columns are contexts, and $f_{ij}$ is the number of times word $w_i$ appears in context $c_j$ (i.e. value of the cell).\n",
    "\n",
    "$$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(w) = \\frac{\\sum_{j=1}^C f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(c) = \\frac{\\sum_{i=1}^W f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PMI has the problem of being biased toward infrequent events: very rare words tend to have very high PMI values.\n",
    "- Thus, $P(c)$ is computed as $P_{\\alpha}(c)$ that raises the probability of the context word to the power of $\\alpha$ (e.g. $0.75$)\n",
    "    - Alternative is Laplace smoothing\n",
    "\n",
    "$$\\text{PPMI}_{\\alpha}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P_{\\alpha}(c)}, 0)$$\n",
    "\n",
    "$$P_{\\alpha}(c) = \\frac{\\text{count}(c)^{\\alpha}}{\\sum_{c}\\text{count}(c)^{\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Training Word Embeddings with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.1. Word2Vec\n",
    "- dense vectors\n",
    "- representation is created by training a classifier to distinguish nearby and far-away words\n",
    "- Variants\n",
    "    - SKIP-GRAM\n",
    "        - given the target predict the context i.e. $P(w_0, w_1, w_{n-1}|w_n)$\n",
    "    - CBOW (Continuous Bag of Words)\n",
    "        - it's the opposite of skip-gram, given the context predict the target i.e. $P(w_n | w_0, w_1, w_{n-1})$\n",
    "- Refer to [documentation](https://radimrehurek.com/gensim/models/word2vec.html) for details\n",
    "- [Tutorial](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:35:01.403666800Z",
     "start_time": "2023-06-05T09:34:52.684668200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: Levenshtein==0.21.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from python-Levenshtein) (0.21.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from Levenshtein==0.21.0->python-Levenshtein) (3.1.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\adnan\\anaconda3\\envs\\nlu-labs-venv\\lib\\site-packages (from gensim) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:35:03.354667300Z",
     "start_time": "2023-06-05T09:35:01.006667200Z"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from gensim.models import Word2Vec\n",
    "data = ['Iceland is faraway from Padova', 'Rome is the capital of Italy', 'Paris is a big city']\n",
    "model = Word2Vec(sentences=[d.split() for d in data], vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:35:03.360669800Z",
     "start_time": "2023-06-05T09:35:03.343670600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=14, vector_size=10, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:35:03.448669300Z",
     "start_time": "2023-06-05T09:35:03.360669800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01631476  0.00189917  0.03473637  0.00217777  0.09618826  0.05060603\n",
      " -0.0891739  -0.0704156   0.00901456  0.06392534]\n",
      "[('faraway', 0.5111488103866577), ('Italy', 0.2914133667945862), ('Iceland', 0.07346687465906143)]\n"
     ]
    }
   ],
   "source": [
    "# getting word vectors\n",
    "print(model.wv['Rome'])\n",
    "# getting most similar\n",
    "print(model.wv.most_similar('Rome', topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:35:03.464666600Z",
     "start_time": "2023-06-05T09:35:03.390667400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Vector Similarity\n",
    "- two words are similar in meaning if their context __vectors__ are similar\n",
    "- __Cosine similarity__ measures the similarity between two vectors of an __inner product space__. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Dot Product\n",
    "\n",
    "- dot product (inner product)\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum^N_{i=1}v_i w_i = v_1 w_1 + v_2 w_2 + ... + v_N w_N$$\n",
    "\n",
    "- vector length (L2 norm $||v||_2$)\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{\\sum^N_{i=1} v_i^2}$$ \n",
    "\n",
    "$$ |\\vec{v}| = \\sqrt{\\vec{v}\\cdot\\vec{v}} = \\sqrt{\\sum^N_{i=1} v_i v_i} = \\sqrt{\\sum^N_{i=1} v_1 v_1 + v_2 v_2 + ... + v_N v_N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Cosine Similarity\n",
    "\n",
    "- L2 normalized dot product of 2 vectors\n",
    "    - $\\theta$ is the angle between $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = |\\vec{v}||\\vec{w}|\\cos\\theta$$\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$$\n",
    "\n",
    "$$\\text{CosSim}(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum^N_{i=1}v_i w_i}{\\sqrt{\\sum^N_{i=1} v_i^2} \\sqrt{\\sum^N_{i=1} w_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cosine Distance\n",
    "$$\\text{Cosine Distance}(\\vec{v}, \\vec{w}) = 1 - \\text{Cosine Similarity}(\\vec{v}, \\vec{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n",
    "- Implement a function to compute __cosine similarity__ using `numpy` methods\n",
    "    - `np.dot`\n",
    "    - `norm`\n",
    "- Using the defined functions\n",
    "    - compute cosine similarity between two word embeddings for instance `Rome` and `city` or `Paris` and `Rome`\n",
    "    - compare similarity values to the cosine similarity using the output of (`scipy.spatial.distance.cosine`)\n",
    "        - i.e. use *distance* to compute *similarity*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:35:09.924668300Z",
     "start_time": "2023-06-05T09:35:09.883667600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04265024\n",
      "0.9573497511446476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v, w) / (norm(v) * norm(w))\n",
    "\n",
    "rome = model.wv['Rome']\n",
    "paris = model.wv['Paris']\n",
    "print(cosine_similarity(rome, paris))\n",
    "# print cosine similarity using distance\n",
    "print(cosine(rome, paris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Pre-Trained Embeddings\n",
    "- Training embeddings is computationally expensive\n",
    "- Many pre-trained models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:36:28.232330100Z",
     "start_time": "2023-06-05T09:35:10.277667800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# Download the 'word2vec-google-news-300' embeddings\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.23535156,  0.18652344, -0.0390625 ,  0.31445312, -0.01019287,\n        0.09375   , -0.3203125 , -0.01635742, -0.06347656,  0.22167969,\n       -0.17382812,  0.04492188,  0.10595703,  0.06298828, -0.08300781,\n       -0.03808594, -0.06982422, -0.05395508, -0.00891113,  0.14160156,\n        0.08984375,  0.0703125 ,  0.2890625 , -0.06079102,  0.3515625 ,\n        0.01855469,  0.03833008,  0.34375   , -0.24511719, -0.00958252,\n        0.12060547, -0.04248047, -0.31445312,  0.109375  , -0.15039062,\n       -0.31054688, -0.01452637,  0.16015625, -0.04711914,  0.14453125,\n        0.13183594,  0.05541992,  0.34570312,  0.19921875,  0.12695312,\n        0.0378418 ,  0.07519531,  0.38085938, -0.0135498 ,  0.24414062,\n        0.01635742,  0.22851562, -0.04638672, -0.1953125 , -0.22949219,\n        0.18554688, -0.16601562, -0.11914062, -0.19726562, -0.04199219,\n        0.0859375 ,  0.09765625,  0.02624512, -0.07226562, -0.01055908,\n       -0.10839844, -0.24804688, -0.03808594,  0.15722656, -0.17382812,\n       -0.19042969, -0.12304688, -0.17871094,  0.171875  , -0.06933594,\n       -0.04174805,  0.18164062,  0.07617188, -0.09277344,  0.11962891,\n       -0.08544922,  0.04956055,  0.12011719, -0.16113281, -0.00216675,\n       -0.17578125, -0.04980469, -0.23730469,  0.10058594,  0.06982422,\n       -0.01153564, -0.12207031, -0.3515625 , -0.171875  , -0.06542969,\n       -0.27539062, -0.02709961, -0.23144531, -0.31640625, -0.15527344,\n       -0.43164062,  0.16699219,  0.11669922,  0.00146484,  0.00224304,\n       -0.02124023, -0.18554688, -0.19140625, -0.20019531, -0.10742188,\n        0.05224609,  0.2734375 ,  0.29296875, -0.20898438,  0.00144196,\n       -0.07714844, -0.08203125, -0.16210938,  0.24023438,  0.30664062,\n       -0.28710938, -0.25195312,  0.16894531, -0.12353516,  0.14941406,\n       -0.43164062, -0.04077148,  0.1640625 , -0.2734375 ,  0.15234375,\n       -0.37109375, -0.16015625, -0.3671875 , -0.12695312,  0.01989746,\n       -0.11669922, -0.19140625,  0.14257812,  0.20703125, -0.01342773,\n        0.33203125,  0.17382812,  0.08789062, -0.2109375 ,  0.26171875,\n        0.01086426,  0.16308594, -0.00320435,  0.17578125,  0.00323486,\n       -0.01794434,  0.24707031,  0.1875    ,  0.16796875,  0.30859375,\n        0.0456543 ,  0.09570312,  0.17382812,  0.10791016,  0.03686523,\n       -0.0456543 ,  0.17578125, -0.02929688, -0.16308594,  0.20898438,\n        0.1875    ,  0.12109375,  0.01599121,  0.05517578,  0.08691406,\n       -0.16503906, -0.20800781,  0.10009766,  0.20214844,  0.00662231,\n       -0.2578125 , -0.03613281, -0.12597656,  0.00335693, -0.2734375 ,\n       -0.09033203, -0.03149414,  0.07958984, -0.05664062,  0.25195312,\n       -0.07177734,  0.10546875, -0.0612793 , -0.11767578, -0.07958984,\n        0.12451172,  0.109375  , -0.02966309,  0.13867188, -0.27539062,\n        0.24414062,  0.33984375, -0.03662109,  0.15722656, -0.25195312,\n        0.23535156,  0.09814453, -0.27148438,  0.078125  ,  0.18652344,\n       -0.31835938,  0.15039062,  0.04077148,  0.27734375, -0.11083984,\n       -0.23535156,  0.06347656, -0.09130859, -0.20117188, -0.15429688,\n        0.10400391, -0.10107422,  0.19628906,  0.13964844, -0.13574219,\n        0.20605469, -0.4375    ,  0.1484375 ,  0.12597656, -0.00183868,\n        0.13964844, -0.08300781,  0.15039062,  0.0255127 ,  0.11962891,\n        0.16992188,  0.20898438, -0.0546875 , -0.05151367,  0.01257324,\n       -0.07519531, -0.10791016,  0.12988281,  0.16113281, -0.06103516,\n        0.23632812,  0.06787109,  0.19042969,  0.1640625 , -0.13867188,\n        0.09619141, -0.19433594, -0.18066406,  0.05004883, -0.01818848,\n        0.23144531, -0.09082031, -0.08642578, -0.02160645,  0.13085938,\n        0.09521484,  0.13574219, -0.13085938,  0.19628906,  0.00994873,\n        0.21289062,  0.12988281,  0.09765625,  0.24511719, -0.06005859,\n        0.16210938,  0.11767578, -0.00512695, -0.09228516,  0.12158203,\n        0.02746582, -0.03100586, -0.10644531,  0.03027344,  0.12792969,\n       -0.08447266, -0.01794434, -0.08154297,  0.24316406,  0.07910156,\n       -0.23339844,  0.38085938,  0.13769531, -0.06591797, -0.21679688,\n        0.30273438, -0.171875  ,  0.07275391, -0.39257812,  0.00946045,\n       -0.01373291, -0.06982422, -0.27148438,  0.23632812,  0.14453125,\n        0.13964844, -0.31054688, -0.16894531,  0.08447266, -0.00430298],\n      dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['Rome']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:36:28.410332200Z",
     "start_time": "2023-06-05T09:36:28.245331500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[('based_Isae_Institute', 0.6246561408042908),\n ('Isae_institute_confidence', 0.6089050769805908),\n ('Giulia_Segreti', 0.6059289574623108)]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('Rome', topn=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:37:07.649332600Z",
     "start_time": "2023-06-05T09:36:28.413335200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Parisian', 0.6789354085922241),\n ('Hopital_Europeen_Georges_Pompidou', 0.653655469417572),\n ('Spyker_D##_Peking', 0.6336592435836792)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('Paris', topn=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:37:07.977334Z",
     "start_time": "2023-06-05T09:37:07.652336100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Word Embeddings in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> To make them compact and fast, spaCy's small pipeline packages (all packages that end in `sm`) don't ship with word vectors, and only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens -- but the result won't be as good, and individual tokens won't have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:\n",
    "\n",
    "> `python -m spacy download en_core_web_lg`\n",
    "\n",
    "> Pipeline packages that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an __average of their token vectors__. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n",
    "\n",
    "> Each `Doc`, `Span`, `Token` and `Lexeme` comes with a `.similarity` method that lets you compare it with another object, and determine the similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:41.469369400Z",
     "start_time": "2023-06-05T09:37:07.980333500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.1. Accessing Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.479618200Z",
     "start_time": "2023-06-05T09:47:41.472369200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: Rome\n",
      "vector dimension: 300\n",
      "spacy vector norm: 54.82853\n",
      "numpy vector norm: 54.82853\n",
      "numpy linalg norm: 54.82853\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "txt = 'Rome is the capital of Italy'\n",
    "doc = nlp(txt)\n",
    "\n",
    "tok = doc[0]  # let's take Rome\n",
    "\n",
    "print(\"string:\", tok.text)\n",
    "\n",
    "print(\"vector dimension:\", len(tok.vector))\n",
    "print(\"spacy vector norm:\", tok.vector_norm)\n",
    "print(\"numpy vector norm:\", np.sqrt(np.dot(tok.vector, tok.vector)))\n",
    "print(\"numpy linalg norm:\", np.linalg.norm(tok.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.531658100Z",
     "start_time": "2023-06-05T09:47:44.483618800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "spacy CosSim(Rome, Paris): 0.6117807626724243\n",
      "scipy CosSim(Rome, Paris): 0.6117808222770691\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# let's get Paris & compare its vector to rome\n",
    "paris = nlp('Paris')[0]\n",
    "print(paris.text)\n",
    "\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, paris.text), tok.similarity(paris))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, paris.text), 1 - cosine(tok.vector, paris.vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Train your own Word Embeddings\n",
    "One way to train word embeddings is to use a language model. We have already seen language models in Lab 3, but now we are going to develop a language model using a neural architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Task definition\n",
    "To model the probaiblity distribution over a sequence, we are going to use the Chain Rule as we have seen in LAB 3:\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "However, at that time we have used ngram to trucate the previous context ($N-1$), in order to compute meaningfull probabilities. While using neural models, we will let the model to decide by itself how to manage the previous contex and thus which are the tokens relevant for the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 RNNs are the most suitable architacture\n",
    "One of most suitable neural architecture for the Language Model task is the Recurrent Neural Network. The architecture is composed of a RNN layer (vanilla, LSTM, GRU) and a softmax that outputs the probability over the dictionary. Indeed the size of the output vector is equal to the size of the dictionary, i.e. the model cannot predict tokens that are not present in vocabularly. <br>\n",
    "> LM task in RNN can be tackled as a sequence labelling task (i.e. len of input and output sequence are always the same) in which the input sequence is $ input = \\{w_1, w_2, w_{n-1}\\}$ and the output is $ output = \\{w_2, w_2, w_{n}\\}$\n",
    ">\n",
    "> **Example** our sentence is ***\"I go to Miami\"*** the input sequence would be ***\"I go to\"*** and the output is ***\"go to Miami\"***. \n",
    ">\n",
    "> Notice: \n",
    "> - To proper model the sequence probabilities we need add boundary markers \\<s\\> and \\</s\\>.\n",
    "> - However in LM RNN only the end of sentence token \\</s\\>  is usually used unless we need for some reason (e.g. in ASR) to compute the probability distribution of the first token of a sentence. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/zGH99MFY/rnn-lm.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "In the image below you can see a working example of a language model with RNN. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/fydQNrYP/LM-RNN.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# RNN Elman version\n",
    "# We are not going to use this since for efficienty purposes it's better to use the RNN layer provided by pytorch  \n",
    "\n",
    "class RNN_cell(nn.Module):\n",
    "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, prev_hidden, word):\n",
    "        input_emb = self.W(word)\n",
    "        prev_hidden_rep = self.U(prev_hidden)\n",
    "        # ht = σ(Wx + Uht-1 + b)\n",
    "        hidden_state = self.sigmoid(x + prev_hidden_rep)\n",
    "        # yt = σ(Vht + b)\n",
    "        output = self.output(hidden_state)\n",
    "        return hidden_state, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.610621100Z",
     "start_time": "2023-06-05T09:47:44.547618200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Data loading \n",
    "For sake of time we are going to see this part in detail in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.644616900Z",
     "start_time": "2023-06-05T09:47:44.619623100Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.698618100Z",
     "start_time": "2023-06-05T09:47:44.641617700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.822616300Z",
     "start_time": "2023-06-05T09:47:44.701618900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set \n",
    "# However you can compute it for dev and test just for statistics about OOV \n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.837619500Z",
     "start_time": "2023-06-05T09:47:44.824619500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "10001"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.867617100Z",
     "start_time": "2023-06-05T09:47:44.840618Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.979616400Z",
     "start_time": "2023-06-05T09:47:44.855619200Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:44.993617800Z",
     "start_time": "2023-06-05T09:47:44.983618100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:46.148618700Z",
     "start_time": "2023-06-05T09:47:44.995618600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:46.163622900Z",
     "start_time": "2023-06-05T09:47:46.111620300Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Train and validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T09:47:46.164622500Z",
     "start_time": "2023-06-05T09:47:46.130619800Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T10:02:34.717773500Z",
     "start_time": "2023-06-05T10:02:34.610769500Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 16\u001B[0m\n\u001B[0;32m     12\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:1\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     14\u001B[0m vocab_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(lang\u001B[38;5;241m.\u001B[39mword2id)\n\u001B[1;32m---> 16\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLM_RNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43memb_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhid_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlang\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword2id\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<pad>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39mapply(init_weights)\n\u001B[0;32m     19\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mSGD(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1149\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1146\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:801\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    799\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    800\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 801\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    803\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    804\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    805\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    806\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    811\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    812\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:824\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    820\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    821\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    822\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 824\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    825\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1147\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1146\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1147\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLU-LABs-venv\\lib\\site-packages\\torch\\cuda\\__init__.py:258\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    254\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    255\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    256\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 258\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.1 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:1'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "#If the PPL is too high try to change the learning rate\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "                          \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print('Test ppl: ', final_ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model makes you happy and you want to reuse it, you have [to save it and load it](https://pytorch.org/tutorials/beginner/saving_loading_models.html). \n",
    "In pytorch this is super straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "# path = 'model_bin/model_name.pt'\n",
    "# torch.save(model.state_dict(), path)\n",
    "# To load the model you need to initialize it\n",
    "# model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "# Then you load it\n",
    "# model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7 Evaluation: Analogy Task\n",
    "In the word analogy task, we complete the sentence of the form\n",
    "\n",
    "\"$w_1$ is to $w_2$ as $w_3$ is to $w4$\", where $w_4$ is a blank. \n",
    "\n",
    "For instance:\n",
    "\n",
    "\"*man* is to *woman* as *king* is to **__**\", and our goal is to guess the missing word (*queen*)\n",
    "\n",
    "The task is approached using cosine similarity between vector differences: \n",
    "\n",
    "$$\\vec{w_2} - \\vec{w_1} \\approx \\vec{w_4} - \\vec{w_3}$$\n",
    "\n",
    "$$\\vec{w_4} \\approx = \\vec{w_3} + \\vec{w_2} - \\vec{w_1}$$\n",
    "\n",
    "$$w = \\arg\\max_{w \\in V}(\\vec{w} \\cdot (\\vec{w_3} + \\vec{w_2} - \\vec{w_1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w = \\arg\\max_{w \\in V}\\text{CosSim}(\\vec{w_2} - \\vec{w_1}, \\vec{w} - \\vec{w_3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Analogy using Most Similar\n",
    "> For each of the given vectors, find the `n` most similar entries to it by cosine. \n",
    "Queries are by vector. Results are returned as a (`keys`, `best_rows`, `scores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_spacy(w1, w2, w3):\n",
    "    v1 = nlp.vocab[w1].vector\n",
    "    v2 = nlp.vocab[w2].vector\n",
    "    v3 = nlp.vocab[w3].vector\n",
    "    \n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "   \n",
    "    # n=1 & sorted by default\n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([rv]), n=10)\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0][0]):\n",
    "        print(nlp.vocab.strings[key], ms[2][0][i])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(analogy_spacy('man', 'woman', 'king'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Write a function that computes the analogy with our RNN based model\n",
    "- Compare Spacy and our RNN based model (just try a couple of examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_our_model(w1, w2, w3, model, lang):\n",
    "    model.eval().to('cpu')\n",
    "    \n",
    "    # Suggest: make use of torch.LongTensor and check if the word is in the vocab\n",
    "    # Get word ids\n",
    "    tmp_w1 = \n",
    "    tmp_w2 = \n",
    "    tmp_w3 = \n",
    "    # Get word emebddings from our model\n",
    "    v1 = \n",
    "    v2 = \n",
    "    v3 = \n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "\n",
    "    # n=1 & sorted by default\n",
    "    ms = model.get_most_similar(rv, top_k=10)\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0]):\n",
    "        print(lang.id2word[key], ms[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Our model is trained on WSJ news queen and king should be OOV or very rare tokens\n",
    "# Try with different words\n",
    "analogy_our_model('man', 'woman', 'u.s.', model, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1 (2 points)\n",
    "Modify the baseline LM_RNN (the idea is to add a set of improvements and see how these affect the performance). Furthremore, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration. Here are the links to the state-of-the-art papers which uses vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf). \n",
    "- Replace RNN with LSTM (output the PPL)\n",
    "- Add two dropout layers: (output the PPL)\n",
    "    - one on embeddings, \n",
    "    - one on the output\n",
    "- Replace SGD with AdamW (output the PPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (4 points)\n",
    "Add to best model of Exercise 1 the following regularizations described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ):\n",
    "- Weight Tying (PPL)\n",
    "- Variational Dropout (PPL)\n",
    "- Non-monotonically Triggered AvSGD (PPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
